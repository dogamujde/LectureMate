Okay.
We have the red light to go.
So it's Friday.
Your numbers have decreased further.
Maybe this is unsurprising for a Friday.
, so yesterday we looked at eigenvalues and eigenvectors.
While we looked at the definition of an eigenvalue and
an eigenvector.
Today is all about actually finding eigenvalues and eigenvectors.
So at the end of yesterday we just reached the
point where we saw.
To calculate an eigenvalue you essentially have to solve this
equation.
So lambda times the identity minus whatever matrix you
want.
And you find the determine to that you want to
make it equal to zero.
So in order to get going today.
Here is a first question to warm us all up.
Get our brains working for Friday lunchtime.
There is a matrix.
What are the eigenvalues of the matrix?
, I will give you the hint that they are
all between the values of one 0 to 5.
, what do I want to do?
That's what I want to do.
They are all between 0 and 5.
They shouldn't.
Actually, this is quite a nice matrix to find the
determinant of.
So hopefully shouldn't be too bad.
So I'm not going to give you too long but
just to warm up.
What are the eigenvalues for that.
Is that a hand up.
Know something at the corner of my eye okay.
Feel free to have a chat with your friend.
We essentially want to find this determinant.
Let's think about what that means.
Hopefully this isn't too bad.
Oh, we've reached a third of voters.
Then we seem to have stalled.
I'm going to give you an extra.
I'm going to give you an extra 15 seconds.
We seem quite happy there, actually.
Lots of people voting.
I'm going to give you another 10s.
Five seconds.
Two, one.
And magically, it's all going to appear.
There we go.
Okay, so we all seem fairly happy that it is
one, 2 or 5.
, yeah.
Well done everyone.
It's one, two and five.
Yay!
We have a brain even though it's a Friday.
Well done.
So hopefully we all kind of got to something
this.
And then we think about the term to this matrix.
We can expand along the first column.
And then so it's lambda minus one times the determinant
of this two by two which relatively clearly hopefully should
be lambda minus two lambda minus five.
And that all equals zero.
So our three roots are one, two and five.
Are we all fairly happy with that.
Yay happy people.
I'm going to bounce a bit because I feel
we're all asleep, so if I'm energetic, maybe that will
pass on.
The downside of this is I will crash halfway through
the lecture.
, okay, so we found eigenvalues.
Yay!
Hopefully not too terrible.
Do we all agree?
Good.
So how do we actually find eigenvalues?
It turns out you find eigenvalues by casting your mind
back to the last time I was here on a
Friday when we were solving systems of equations.
Yeah, everyone's favourite thing to do.
Well, that's what we're doing again.
Yay!
So wonderful calculation.
To kick off our Friday, let us find the eigen
vectors for our matrix A.
There we go.
So each eigenvalue is going to have a different eigenvector
because that kind of follows from the definition.
So we need to do to find the eigenvectors of
this matrix.
That's the word we're going to have to do three
times.
Yay.
Who's really going to talk on a Friday for a
somewhat long calculation.
so let us start.
We're going to look at our first eigenvalue lambda equals
one okay.
So we want to consider the matrix one.
There's a secret one there that we don't write because
it's one minus a.
And if we think about what that matrix is.
yell at me when I make a sign error, but
I'm just going to write this down and hopefully you'll
all agree we should end up with that.
Do we all somewhat agree?
Yeah, I'm agreeing with myself.
Good.
Okay, so so we've got to that matrix and we
are trying to find the, the the solving that homogeneous
system thing to bear in mind with an eigenvector.
Let's think about what we talked about yesterday.
These are the directions the lines that get mapped to
themselves the directions fixed lines.
That is a line.
Do you remember when way back when we were actually
solving homogeneous systems, when we knew the answer was geometrically
a line?
What did we kind of expect to come out of
our system?
What do we expect the rank to be, for example?
I had someone whisper not three.
Yes, we do not expect the rank to be three.
Or do we expect the rank to be?
If we're expecting two, wonderful.
We expect at least one free verbal.
If we have a line, we expect a free verbal.
And so far we're looking good.
We've got a column of zeros.
So we're definitely not rank three.
So yeah, we're looking good okay.
We're now going to do a bit of a reduction.
Yay.
does anyone have a first step they think we
should do before actually we do anything.
Multiplied by negative.
Multiplied by negative one.
I that one.
Yeah.
, so we've discussed this briefly yesterday.
Sometimes you have lambda times the identity minus a, sometimes
you have a minus lambda times the identity.
If you do it this way, the identity minus A,
you end up with a lot of negative signs generally.
So it's worth just let's times everything by minus one.
, cool.
Okay.
We've changed everything by minus one.
That's good.
.
Let's then do a bit of row reduction.
So I'm going to start by swapping.
Which two shall I swap.
First and second.
Yeah I that okay.
First and second.
So we've now got 010023 and 044.
Okay.
what do we do next?
Subtract two times the third one from the rotate.
Yay!
So who's going to go to row two?
Minus two.
Lots of row 10100.
I know this does get a bit dull.
You've done this a lot now.
And what should I do for the last row?
No one can be good.
Still worth practising because you will do this a lot.
And if you can do it in your sleep, you
can do it in the exam.
Okay, so there is the matrix.
Anything else I should do?
One last step maybe.
2 or 3.
Yeah.
Let's do that.
001.
And we could we'll leave it on trust that you
can get rid of that fall.
, okay, so we are in reduced row echelon form,
actually, because everything's zeros and ones.
, and we have no leading one at the bar.
There we go.
Thank you.
, , we have no leading one in the first
column.
So what does that mean?
What is our free variable going to be?
The first one.
Yeah.
Good.
Okay.
So if we're thinking about general solutions we're going to
set X to be a free variable.
Then what's y going to be.
Good zero.
Well done.
There was someone put A0A fist with zero fingers up.
I'm taking that to be zero.
What is that going to be.
Yes I the the yes okay.
Zero.
Wonderful.
Okay.
So we have to zero zero at general solution looks
x, y z looks T00.
So I can factor out that t and we get
a we get a nice vector there.
Wonderful.
What's our eigenvector.
100.
Wonderful.
Actually I disagree with you.
The eigenvectors 200.
Who's right?
Who's wrong?
Yes, yes.
We're all right.
Wonderful.
It's a Friday.
Everyone can be correct today.
Yeah, it doesn't matter, frankly, because it's a line.
I care a lot about the direction.
I don't really care the magnitude of this line.
So yes 100 is an eigenvector but so is 200.
So is -2020 400.
So is pick your favourite number zero zero.
They're all eigenvectors.
So we're expecting a line of solutions.
This is just saying go off in the x direction
and don't do anything in y and Z Okay.
Are we all kind of happy with how we did
that?
Good.
Generally, you probably would write 100.
You want to be as nice as possible.
You probably want to reduce it to the lowest common
factors.
Okay.
So that was our first one.
Let us do our second one.
So our second eigenvalue is two.
I'm not going to do all three.
But I am going to do two because something special
happens or two.
so we've now lost the matrix A let's bring
the matrix A there you go.
There's a matrix A so if I think about what
lambda times the identity.
Well, two lots of the identity minus that matrix is
again telling me when I make a mistake, but I
make it to be minus one, minus two, minus three
0000, minus four.
What's the last number?
Minus three.
Good.
Cool.
Are we all happy with how we got there?
I think the first one is.
Yeah, the first one is a one.
Good.
See.
Everyone has a Friday brain.
Be careful with minus.
Sounds good.
Okay, so we have one minus two minus c.
We've got two zeros.
Again with quite obviously rank two.
This is looking good.
We're going to have a free variable okay.
So let's reduce again I'm going to do this a
bit quickly because hopefully we're all good with row reduction.
so I'm going to move that row to the
bottom.
Everyone happy I might make a mistake.
So do follow along and yell at me when I
make a mistake.
, and then I'm actually just going to divide by
four.
Actually, this ones.
I don't think I have made a mistake.
.
Oh, I have made a mistake.
Yeah, actually, I can keep those signs.
Yeah, that's what I get.
Do we all somewhat agree?
I swapped the rows.
I times the first row by nothing at times.
The second row I divided by minus four.
Are we all happy?
Yes.
Okay.
Don't do maths quickly.
, so what's off?
We verbal in this case going to be Z.
Good Z equals t.
, what should I take y to be?
Minus three.
T yes.
Minus three quarters d.
Good okay.
And x.
Plus two.
One.
Yeah.
So it's two y plus three z.
And if you stick that in terms of teas, you're
going to get minus three halves of tea plus three
tea, which is three halves of tea.
Cool.
Okay.
Are we all happy people?
Wonderful.
So the eigenvector I can factor out the tea and
I'm going to have, , three halves minus three quarters
one.
Okay.
We could do that.
Equally we would probably most people probably.
Right.
This eigenvector as you go on and do stuff with
eigenvectors.
We don't care about the size.
So what might we want to do with that eigenvector
in particular multiplied by four.
Yeah.
Let's get rid of the.
Whole.
Let's learn to multiply by four.
there we go.
Yeah.
Let's get rid of the fractions where possible.
Okay.
Are we.
Still.
All good?
Right.
I'm not going to do the last one.
I'm going to leave that one to you.
I will give you the answer and let you double
check this.
But lambda equals five is the last one.
, and you should end up with your third eigenvalue
to be minus three zero four, I hope, , go
forth and check that at some point.
But there you go.
Are we all happy with how to find eigenvectors?
Good.
It's slightly long winded, but not actually too hot.
Well, it's the same method you've been using for five
weeks now, , in various different guises.
So hopefully.
Okay.
Right.
Okay, good.
So that is how to find eigenvalues and eigenvectors.
So that's all good.
what if though.
So that was quite nice.
I had two I had three eigenvectors and I had
three eigenvalues.
What if when we find our characteristic polynomial, though, we
end up with a double root?
What do we think could happen then?
So let's think about another matrix.
I'm going to call this one B just to change
stuff up a bit.
Let's think about this matrix.
.
What are the eigenvalues there.
If we just yell at me.
One zero and one.
Good.
But if we think about the characteristic polynomial, it's actually
going to be lambda lambda minus one squared.
So we've only got two roots but we've kind of
got three eigenvalues because lambda is a repeated eigenvalue.
Okay.
So what's happening here.
What do people think's going to happen when we find
eigenvectors one.
Has two parameter two three.
Yeah.
Good.
So let's just consider I mean finding the eigenvectors for
zero, I will leave you to do.
Let's consider the eigenvectors for one.
If we do the identity minus this matrix, we're going
to end up with minus one, minus seven plus 17
minus eight 000000.
What ranks that matrix one.
So how many free free variables do we have.
Two.
Wonderful.
So we could times everything by minus one to make
this nice and quickly solve it.
And we're going to get y equals one free variable
z equals another free variable.
And x is going to be 17.
Lots of t minus eight lots of s.
And if we solve this I appreciate I'm doing this
relatively quickly.
We're going to have 17 one zero plus zero.
Yeah.
Minus eight zero one okay.
.
Good.
So we now have two basic solutions, and both of
them are eigenvectors.
So in this case we actually have two eigenvectors.
Okay.
So this is quite funky.
We so we did the algebra.
We found two roots.
This is known as the.
So yeah every eigenvalue has a corresponding eigenvector.
And if you find a double root when you're doing
your determinant that is known as the algebraic multiplicity.
If you find a triple root it's got multiplicity three.
So it's how much when you do the all the
algebra how often does your root appear.
That's known as the algebraic multiplicity.
Okay.
But let's start thinking geometrically.
We thinking geometrically.
What is happening geometrically when I have got two eigenvalues?
Do you get it?
I get a plane.
Yeah.
So instead of having a fixed line, instead of a
line being mapped to itself, I've now got a whole
plane that's getting mapped to itself.
So you can think about it as a reflection in
3D.
That mirror gets reflected to itself.
So the eigenvalues of that mirror, well they're going to
have eigenvector one, but it's going to have two parameters.
So that mirror gets mapped entirely to itself.
And we get a fixed plane.
And that is the geometric multiplicity.
So how many eigenvectors you get for your algebraic multiplicity
is your geometric multiplicity.
Does that make sense.
So you have the algebra that gives you how many
times an eigenvalue appears.
And then for every eigenvalue you have.
Different.
Number of eigenvectors that can appear.
And that's your geometric multiplicity.
So far, so happy still?
Yes.
So the transformation.
Applied by matrix B it's reflecting it out.
So this one yeah actually this one, this one's
not a reflection because we've got eigenvalue zero.
So if it was a reflection you'd only have eigenvalues
one and minus one.
That's what we saw yesterday.
But here we've got a plane.
So the plane spanned by these two vectors a plane.
Anything that can be written this, is fixed
by this matrix.
And then everything that's not on that plane is kind
of where it's got eigenvalue zero.
So that's getting what happens if you have eigenvalue zero
actually.
Let's go back to the definition.
What happens if your eigenvalue zero you will get mapped
to the origin.
Yeah.
So you get pushed down to everything.
So actually if we think about this a bit, this
matrix here, it's got a plane that's being fixed and
everything else is being mapped to the origin.
So it's kind of collapsing everything just down to the
plane.
Yeah.
So for each.
Line that was presented by those two classic solutions, we
think of the map.
To itself and very linear combination which is represented by
the plane as well.
So it's going to also be mapped to itself.
Yeah.
Because even if I have say if I have three
lots of this vector plus two lots of this vector,
well three lots of that vector is getting mapped to
three lots of that vector.
And two lots of that one is getting mapped to
that one.
And we know linear transformation is respect addition.
So it's still going to be those three.
Lots of that plus two lots of that.
So we get exactly the plane gets exactly mapped to
itself.
Okay.
Any other questions or Queries or comments.
But actually that's quite good if we think about what
we just did there geometrically, just by looking at the
eigenvalues and eigenvectors, we could tell what that matrix did
geometrically.
I can't look at that matrix off the top of
my head and go, that is pushing everything is observing
the plane and taking everything else down to the origin.
But we can do that with the eigenvalues and eigenvectors.
That is, the power of eigenvalues and eigenvectors is.
They allow us to do the geometry without doing any
geometry, which is nice because in higher dimensions I can't
draw things.
Okay.
So we know if the eigenvalue has an eigenvector.
So how does the algebraic multiplicity and the geometric multiplicity
relate to each other?
Which one's got to be bigger than the other one.
One's got to be bigger than the other.
Which one.
Yeah, yeah.
The outbreaks got to be bigger because for every algebraic
one, you can't have more eigenvectors than you can eigenvalues.
No.
Hang on.
I said that completely backwards and all that.
You can't have more eigenvectors than you do.
The number of eigenvalues, each eigenvalue even repeated, only gives
you at most one eigenvector.
So we know that the algebraic multiplicity is bigger than
the geometric multiplicity.
Good.
So we actually have this inequality.
We also know both of them are bigger than one.
Because if you have one eigenvalue, you have a eigenvector
by definition.
If you do not have an eigenvector, then you're not
an eigenvalue.
So they come in a pest.
You have at least one eigenvector, but you don't have
to have all of the.
So this we will see it in a bit a
case where we have algebraic multiplicity but not geometric multiplicity.
So we don't have two eigenvectors.
We only have one eigenvector.
Even though the eigenvalue appears twice.
Cool.
I feel we kind of went boring calculation boring
calculation calculation hard.
But the theory we're about to go back to born
calculation.
But does anyone have questions about the spiking theory?
Yes.
Let me get back to finish.
I think I.
Got.
It.
Yeah.
So we got two eigenvectors because our eigenvalue appeared twice,
but it's not always the case.
Even if your eigenvalue appears multiple times, that doesn't guarantees
you one eigenvector, but it doesn't guarantee you two eigenvectors
for example.
So we'll see that in a in a bit.
Okay.
So we're actually going to go back to somewhat boolean
calculation now.
yay.
Hopefully or not.
Not yay.
, hopefully by now you've realised we diagonal matrices.
Can anyone give me a reason why we diagonal
matrix?
What do they make life really easy for?
The determinants.
Easy, yeah.
What else?
Multiplications.
Easy.
What else did I have?
Finding the inverse.
Why is finding the inverse of a diagonal matrix really
easy?
Yeah it's basically the reciprocal.
More or less you find the reciprocal of everything on
the diagonal.
why is it easier to interpret transformation.
What is the diagonal transformation doing?
It's a combination of matrices.
So it's really easy to think of a combination of
those.
Yeah exactly.
And we know geometrically what the elementary matrices are doing.
So actually it's quite easy to interpret.
It's easy to calculate with we diagonal matrices.
If we have a matrix that, though, that's less
nice.
I mean, that one's got a load of zeros in.
That's not too bad.
I'm picking ones of loads of zeros in deliberately.
But, , if you had a big matrix they're not
with not zeros in they're less nice.
So you have to think about inverses and determinants and
things.
So we actually want things to be diagonal.
We want we prefer diagonal matrices.
So we're going to diagonalise our matrix.
this is a very typical mass thing.
We have a diagonal matrix.
We want to make something diagonal.
So we make diagonal a verb and call it diagonalise.
, so we are diagonalising our matrix.
there we go.
So when are we diagonalisable?
When can we make a matrix diagonal?
It's when we've got some matrix P where Papp holds
p inverse AP equals our diagonal matrix D for some
matrix P.
Later on we'll see.
This is known as a similar relationship, but don't worry
about that too much at the moment.
, but we need to know that such a P
exists.
You cannot always diagonalise a matrix, but you can sometimes.
And when you can, that's what we mean.
We mean that p inverse a p equals a diagonal
matrix D.
So let us think about why this preserves all our
nice diagonal things.
so one thing that we said was nice was
we could, , multiply diagonal matrices together nicely.
Let's actually just think about taking a power.
Taking a power of a diagonal matrix isn't bad.
, we know if this is the same as d
times d.
If we're diagonalisable, then we have p inverse a p
times p inverse a p.
How does that simplify?
Brilliant.
P inverse a squared p.
Good.
So say, if I actually wanted to find a squared.
How can I find a squared?
How can I rearrange this?
Brilliant.
There we go.
I can now find a squared.
That's a lot easier than doing matrix multiplication with the
three by three, which doesn't have zeros in.
Particularly when we see the finding p is actually quite
easy.
So suddenly it's easy to square matrices.
, what do we know about the determinant of of
this.
So when we have a p inverse AP equals d
what do we know about the determinant.
This is the product of that.
Yeah.
So the weak easily find the determinant of D.
And we know the determinant of p and p inverse
are reciprocal.
Determinants are just numbers.
So actually we can quite easily see the determinant of
A is just the determinant of d.
And I can do that one.
That's an easy calculation.
So determinants are nice to and equally finding inverses
is good.
And we'll talk more about interpreting transformations when we talk
about similar matrices more generally okay.
So diagonal matrices this does retain the properties we want
to retain.
It's really nice.
So how do we find P.
I've said it's kind of easy.
so we're diagonalisable.
If our eigenvectors can form a matrix that which
is invertible.
So what I mean there is you take your first
eigenvector as your first column.
Your second eigenvector is the second column.
Yes.
Third eigenvectors.
Your third column.
Keep going, keep going, keep going.
So all you've got to do is find your eigenvectors
and then bung them together in a matrix.
You don't actually have to do anything more with that.
, and then if we have it our diagonal matrix
is really nice.
It's just the eigenvalues again eigenvalues and eigenvectors.
Making life really easy.
Okay.
So we know we're diagonalisable.
If that matrix is invertible, is there a shortcut to
working out for that invertible.
Because that's also a bit of a tricky calculation.
Yep.
, no eigenvector is good.
So even if your eigenvector is zero, you can still
that means your matrix is not invertible, but that we're
not trying to invert A we're trying to invert P,
which is our new matrix.
Yeah.
At the back.
So the determinant of the matrix A or the determinant
of P.
Yeah.
Yeah.
We want to determine a p not to be zero.
But again working out the determinant of a disease and
its p is going to allow us to prove that
A is easy.
But P is a different matter altogether.
And actually we have a result to do this.
So that theorem just gave us those two results.
Turns out we're diagonalisable.
If every eigenvalue of some algebraic multiplicity has m corresponding
basic eigenvectors, or to put it in the language we
just said, your algebraic multiplicity always equals your geometric multiplicity.
So in this case we are diagonalisable because yes we
had algebraic multiplicity two, but we also had geometric multiplicity
two.
So for every eigenvalue including repeats we have a different
eigenvector.
If that happens then we're all good and happy.
And we're diagonalisable.
Are people happy so far?
Good.
Okay, so, did I have anything else I wanted to
say here?
There we go.
Yeah.
Kind of corollary, if you've not got any repeated eigenvector
values at all, then you would definitely diagonalisable.
Because of that inequality, we know every eigenvalue has to
have an eigenvector.
So if you've got and we know that there are
going to be n eigenvalues for an n by n
matrix.
So if you've got n distinct eigenvalues you've got to
have n distinct eigenvectors.
And you've not got to go any further.
Okay.
So let us do a bit.
Of.
a bit of practice here.
How am I going to get this all up on
the board.
Oh I'm going to do something I've not done before,
and I'm going to put both of these up.
This is exciting.
Okay.
So let us go back to.
Is that working?
Yes.
No.
Why are you both showing the same side?
Yes.
Brilliant.
Okay, so let us go back to this matrix A.
What would the matrix P be for this matrix A?
What is the first column going to be.
One zero is zero good.
What's the second column going to be.
Minus three.
Therefore I'll take that.
Yeah.
And what's the last column going to be.
Yeah.
Good.
Okay.
And then what's the corresponding vector D.
What's the diagonal.
I don't even have to do a matrix multiplication.
This is nice.
What's d going to be.
152 on the diagonal here.
And zeros everywhere else.
Done.
That's easy.
Hopefully you just copied out what you've already worked out.
You've not got to do anything more.
Are we all happy with how to find a diagonal
matrix?
Yes.
Does it matter?
Not at all.
So provided that the eigenvector corresponds to the eigenvalue in
your diagonal matrix, that's fine.
So we could have written these 1006 minus three four
minus three zero four and swapped the two and the
five.
But this also.
Works.
So there's not one answer.
There were several answers.
However many ways you want to order your eigenvectors, all
of them are going to give you a diagonal matrix.
Any other questions or queries or thoughts?
In which case, let's do a question.
, so there we go.
And Eigen.
A matrix has double eigenvalue one with that corresponding eigenvector,
and eigenvalue three with that corresponding eigenvector is a diagonalisable.
I will give you.
Oh hang on, you can't see this at all, can
you?
Oh.
There we go.
And magically appeared.
.
I will give you 30 odd seconds to think about
this.
Feel free to have a chat with your neighbour.
Okay, I'm going to give you another few.
What should we have?
Let's have another 20s.
10s.
Let's have a late rush of votes, five seconds or
however long it takes me to walk back over to
the screen.
Four.
Three.
Two.
One.
Okay, so we seem tossed up.
Okay.
We all seem quite happy that it's that it's not
too good.
Okay, then it's a toss up between 1 and 3.
So three seems the most popular.
What are we saying?
There is not invertible and not n by n.
, number of eigenvalues equals the number of eigenvectors.
, well what we've got two eigenvalues and two eigenvectors.
So that looks nice.
, columns are identical and P is therefore not invertible.
The algebraic multiplicity of eigenvalue one does not cause one
to its geometric multiplicity.
What does?
What does that mean?
What someone saying there.
Is only two vectors.
Yeah.
Yeah, exactly.
So this this eigenvalue, we're counting twice.
It's got one and one.
So we've actually got three eigenvalues one, one and three
and two eigenvectors.
So we are not diagonalisable.
Actually this one I can can see why you
think that this is, you know, this is a classic,
classic thing people do.
They go, well, it's got eigenvalue one.
There you go.
But you might as one of those comments said, this
has got a repeated column.
So it's not invertible.
, this is invertible.
I can't find the inverse.
Good.
Okay.
So , let's do one more that.
So here's another matrix with eigenvalue one and a double
eigenvalue.
Is this one diagonalisable.
And if so what is p and d?
I'll give you another 30s to think about this one.
That's a very similar question.
So I'm not going to give you too much longer.
Give you ten more seconds.
Five more seconds.
Three more.
Seconds.
Two, one.
Okay, good.
So lots of people saying one.
What are the comments?
Both the correct.
Both are correct.
I have no idea.
You made a good guess though.
And sometimes that is good enough.
, yeah.
Good.
So we seem quite happy that it is this one.
, I've taken my eigenvectors.
I've smushed them together into a matrix, and I've labelled
the corresponding columns with the eigenvector value.
The eigenvalue.
Okay.
, lots of people said too though.
And some of those comments were saying both were correct.
What what's going on there?
Yeah.
Yes.
No.
You you've spoken less today you know.
Yeah.
Good.
So all I've done in this one is swap to
these two eigenvectors.
There's nothing special about the order of an eigenvector at
all.
So actually yay!
Both of them are correct.
Yay.
Okay.
Does anyone have any questions?
Thoughts?
Feelings?
Complaints?
Queries?
Comments?
Yes.
So the first question we have just so one but
two proponents of double eigenvalue.
Yeah I think we split it up into two.
Okay.
Linear combination of tobacco here vectors up there.
So actually if we so if we think about what's
going on in this case, we are preserving a plane.
We've got two we've got two eigenvalues.
We're preserving the plane.
, so geometrically, anything that plane is mapped to, ,
I double by two is mapped to double itself.
But that plane is kind of, , preserved as a
play.
In the second case, though.
Oops.
That's the wrong question.
, in this case, , we've got eigenvalue one is
occurring twice, but it's only preserving the line.
You can kind of think of it as preserving
the line twice.
, and so it's a doubly preserved line, but you've
not got that plane, so you've only got a line
that's fixed by one, and then you've got this other
line fixed by three.
, there are reasons later on that we will come
to about what you need to preserve the whole space
in some degree or another.
So you need three directions that are kind of, ,
either fixed or mapped to zero.
And at the moment you've not got that, you've got
one line that's fixed, another line that's fixed, and then
you've kind of got a third dimension that's doing something,
but it's not being a linear, it's not being invertible.
, so we can't diagonalise it later on.
We when we do kind of Jordan normal form and
other forms of diagonalization, we'll see what actually is happening
in that case.
, so, so it might kind of the crux of
my answer is C later linear bits of linear algebra,
which I appreciate is disappointing.
, but yeah, essentially this is only fixing a line
was when you've got eigenvalue two eigenvectors, you're fixing a
plane.
And we kind of want to fix the same dimension
space as we do eigenvalues.
Cool.
Is that a question?
Yeah.
How is it possible?
Even though the region values repeat, that we still have
only one practice.
I saw what you said before.
When it's repeated, it means that there are going to.
Be playing and therefore.
So when it's repeated algebraically, that doesn't guarantee that a
plane is fixed.
It just means .
So for example you can have that.
In this case you can have the line is fixed
but kind of twice I am glossing over some geometry
here.
I'm sorry.
, but you've kind of got the line fixed twice
as opposed to a plane of solutions being fixed.
So yes, the eigenvalues counted twice algebraically, but geometrically that's
not matching up.
Those two values.
They're not d and they're just yeah.
There's just t.
Yeah.
Actually, if we think about if we think about that
matrix and we think about what happens with one, we're
going to end up with three minus two, minus three,
two, zero, minus two, zero minus two, and then minus
one, minus two minus one.
Which if you think about these two rows, actually if
you think about that that's a degenerate matrix altogether.
, so that's what's going on there.
, and why it's not working.
Okay.
, so one other thing I just want to say
kind of before the end, that's more or less I
do need a bit of time at the end.
, two things I want to say.
Firstly, sometimes you have to check this.
It's always worth checking your answer, partly because it's a
good check to make sure you actually work at your
eigenvectors correctly.
So you might be tempted to do that.
Matrix multiplication do p inverse AP equals D.
Can anyone think of an easier way of doing this
calculation?
What's the hard bit of doing that calculation?
Yeah I don't want to find p inverse but I'm
quite happy to do that.
That's a lot easier.
So if you're doing a check try not to check
that.
Check that you should do a check.
Checks a nice.
That's a much easier check than than the other one.
, the other thing I want to say is to
kind of go back to where this all started.
, so I've kind of said linear transformations have eigenvectors
and eigenvalues.
, it turns out every matrix has an eigenvalue at
least one and at least one eigenvector.
Therefore, , but yesterday we said that rotations don't fix
lines.
That's the one.
That's the one.
, rotations don't.
Fix.
Lines.
What does a rotation fix?
The plane distance of.
Actually, that's a good point.
If we're preserving distance, then we're preserving the circle.
So actually I can go back to this picture.
, I have preserved I have preserved a line.
This is not a straight line.
It's a curved line.
, turns out that in a rotation, you do have
eigenvalues and eigenvectors.
, but your eigenvalue is complex.
How many people have seen complex numbers?
Okay.
Decent fortune for you.
Good.
, I'm not going to go into this right now,
but I am going to in the reading video.
In the reading quiz, I'm going to put an extra
video at the end that U is absolutely optional and
goes into a down a weird rabbit hole of maths
into conic sections and stuff.
Feel absolutely free to skip it.
It will mainly be me getting excited over conic sections
and explaining why these are imaginary eigenvalues.
But that's what's happening there.
Okay, before you go.
One last thing though.
So that is everything on this week's reading.
, I will warn you now, next week's reading, you
again.
Have me, for better or for worse.
Next week's reading is not different.
Most of you should be somewhat familiar with most of
it.
It's not.
It's basically basic vector theory.
, so it's somewhat okay content wise, hopefully, but there's
quite a lot of it.
So I'm going to try and release the reading a
slightly earlier.
, just so you have a chance to maybe flick
through it.
It's basically all of chapter four.
It's quite long, but it should not be too taxing.
, and then the last thing I want to say
is we've still got five minutes.
Don't pack up yet.
, I do not know how many of you have
seen this, but, , if you go to the stack
page.
We have the mid-semester feedback form.
Some of you have filled this in.
Some of you have not.
Please.
Now I am giving you a couple of minutes to
sit and fill in this form.
Please do fill in this form.
Let us know how we are getting on, how the
tutors are getting on, and we will get back to
you about your feedback.
So you will have, at some point, someone standing here
and explaining how we are going to be better for
you.
So it does have an impact.
So yeah, have a couple of minutes to do that
and then please leave through the front again as we
did yesterday.
I am here for any questions.
It's optional.
You don't have to do it.
But please do do it.
It's nice to have some feedback.
Otherwise I'll see you all next week.
Have a nice weekend.