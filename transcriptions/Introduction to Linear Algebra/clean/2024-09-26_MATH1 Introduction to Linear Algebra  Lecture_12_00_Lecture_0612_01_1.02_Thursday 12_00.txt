Right.
Okay.
, you know the drill.
The lights.
Come on.
.
Which means that we are starting the lecture.
So, , this is the first lecture.
Proper lecture you'll get from me.
, Cordelia.
Arrogant me will alternate during the lecture.
, so you'll have me for this week, next week.
, and then after that, Erik, , will do his
first lecture.
, just before we get into it, remind you that
on Thursdays, we're always doing the attendance poll.
, I put the QR code there.
The poll should be live.
You can lock in at any point during the lecture.
, in case I take this down in a second.
, I've written down the code here.
, if you can see it.
, if you not managed to get that.
, and I have a different code, a different poll
pole that I use for my lecture polls.
, I've written that down here.
I will put up a QR code for that one.
, once we get to the first of those polls.
Okay.
, apart from that, this is a little bit on
the dark side here, I think.
Let's see.
See whether we can do it somewhere in between.
, try this one here.
Right.
Okay.
, the other thing is, if you've had a look
on learn recently, you will see I've made an announcement.
The first of a hand in assessment of the four
that we have, , has gone live this morning.
, so you've got a week to work on it.
The hand and date for that is next Wednesday at
5:00.
, that will cover the material from last week.
, and some of this week what we're doing here.
, okay.
So, , without further ado, , let's get into what
we're doing or take this off here.
I have this here at the moment.
Right.
Okay.
In case you're doing the lecture poll.
So last week with Cordelia, you've done Gaussian elimination, and
I thought I'd tell you just a little bit my
personal anecdote about Gauss.
So that there is Gauss.
And as I had seen on the old German ten
mark note before the year was introduced, this was the
last series.
And if you look closely on that one, behind him,
you see some buildings there.
, there is a picture from a town where Gauss
was active, the town called Göttingen.
, and my personal thing is that this is actually
my home town.
This is where I am from now.
It goes a little bit further.
, there in Göttingen, it's the grave of Gauss.
, and you can see that this is a little
bit of a parkland.
And actually where I grew up as a kid was
about 200m away from this park, and we'd just hang
about there, and we literally used to play football right
next by that, that grave there.
So my, my thing is that I always say I
used to play football as a kid on the grave
of gout.
That's what gets you into mathematics then later.
But okay, so that's just the sort of, , little
thing there.
, let's get into, , what we're doing today.
And I think at this point I'm going to take
this one down, , and go back to the other
ones.
So, , lecture today is about what we call matrix
algebra.
, so algebra sort of how you work with expressions.
So this is really how do you calculate with matrices
and by extension with vectors.
, so there's a few things of just basically calculation
rules.
How they out.
How do you multiply matrix by scalar this transposition, which
it cannot do for numbers.
And then the sort of main thing is going to
be multiplication of a matrix and a vector, and a
bit of where this is leading to.
, I don't think I'm going to mention managed multiplication
of two matrices either.
I think this will come tomorrow.
, and also the concept of a matrix inverse, ,
is what we're going to do, , tomorrow.
So, , let's set up some light-hearted, , warm up
and I'm going, going to give you the other QR
code.
, so this here is the other QR code, and
I haven't.
Now I'm just I was hoping to do that on
my laptop and I am not sure.
So I think I can leave this open.
I can give me a second.
I need to note I don't want to reset.
I want to go back to my events.
There we go.
, polls.
, I want to have a timer.
, right.
Okay.
I think that should be all you need to.
You need to have my answers back.
, we've lost the timer.
Right.
Okay.
So, , I'm just trying to wanting to ask you,
what is your exposure to linear algebra matrices?
, and vectors?
.
Probably while you're doing that, , say that one way
of looking at what we're doing in this course is
that you may have seen matrices before expecting most of
you will see it in some way or form.
film.
But typically the question is what really are matrices?
Now you can do stuff with them, but you don't
really know what really are they.
And I think this is what this course Introduction to
Linear Algebra is aiming to answer.
But after that you will have an answer to that
question.
What really are matrices?
.
Right now, , I forgot my timer.
You cannot vote anymore.
The timer is out.
Okay, sorry for that.
I didn't see the time.
I couldn't calm it out.
Okay.
Goodness me.
Sorry.
I think I'll try to do this the other way
round.
I'm sorry.
I was meaning to do the polls on my laptop
here, but that won't connect to the internet at the
moment.
, so I've got to try to do this differently
now.
Okay.
, so never study most of your studies.
Some vectors and matrices.
Some of you said that you've seen eigenvalues.
Eigenvectors.
This is something we'll do in quite a bit of
detail towards the end of this course.
And a few said that they've even seen abstract vector
spaces.
Okay.
This is very good.
So for some of you this will be new.
For others there will be quite a bit of repeat.
But do be aware that we are doing things at
university quite differently to what you've done in school.
We're looking at different things, looking at different aspects much
more, looking at how will we generalise these things later.
So even if you think I've seen some of this
before, , please, please do pay attention to what we're
doing here.
, right now I'm on the wrong one there.
I need to do this one here.
So.
Okay, let's get into, , what we wanted to do.
Proper.
And I haven't set this up.
I set this up in a second.
Okay.
Let's start with matrices.
So assuming we've got this situation we've got given a
matrix.
And I'm telling you the matrix has elements by J
where the I jth element can be calculated by taking
I minus j.
So if we're doing this which of these matrices am
I referring to.
And .
I believe I can just do that.
Yes.
Somebody has voted.
So this seems to work.
So again I'm giving you one minute.
I'm not sure why.
We've got 2300 people on this course.
That is an exaggeration, right?
A couple of seconds.
We haven't voted yet.
All right.
Good, good, good.
Few have come in at the end.
So let's have a look at the result.
Right okay.
So this is more or less what I was expecting.
I think it's relatively obvious that A and D aren't
the ones.
but then the question is is it B or
C.
They're sort of just alternating.
So it's the question of is I and what do
they what do they stand for.
So this this really is a little bit arbitrary.
, so I think I'm going to, , just spend
a couple of minutes on that.
, so if I've got.
So I'm going to swap these rounds, , so that
I can see, see what I'm writing here.
Excellent.
, so if I have a matrix B.
Yeah.
, and I want to sort of write down the
elements of this matrix.
Right.
Okay.
You don't see this where there's a matrix P, there's
the matrix B, right.
Write down the elements of it.
I mean, I have to admit I usually have to
make a, make a diagram.
So you're going to start with element B1 one up
here.
and then it's element B1 two.
Then it's B1 three.
And then the second Rhodes.
Be two one.
Be two two.
Be two three and then be three one.
Be three two and be three three.
So I think this is reasonably, , sort of, ,
intuitive, , that you say, because we're reading from left
to right, the first row.
Certainly.
First of all that we're saying, well, first of all,
B1 one B1 two, , we are going to go
we've got to first of all start increasing the second
index.
And then if we go into a new row, ,
we start increasing the first index.
, another way to think about it is that it's
always with these indices.
It's always rows before columns.
And that's a useful thing to keep in mind.
So the first index here will tell you which row
you're in.
And the second index will tell you which column you're
in.
Yeah.
So if I now go and say, I've got this
rule that I want to say b I j ,
is calculated as I minus j.
And if I then go and want to say work
out the B1 two element here.
So that's the one up here.
B1 two is then one minus two.
so this is minus one.
, and if I now look at b1 2 or
-1 there, that is matrix C up there.
Yeah.
but as I say there's something arbitrary about it.
Either get into the habit of drawing the diagram or
remember row rows B4 columns.
, that should help to get you there.
, okay.
So, , matrices got the wrong one.
, okay.
So just recap here.
, rows before columns.
.
Yeah.
Also option C we have we have that right okay.
So if I talk about an M times n matrix
this is a matrix with m rows and columns.
Again we're giving the rows the number of rows before
the number of columns.
Yeah.
The jth entry there corresponds to the entry in the
ith row column again rows before columns.
And we have this notation that if I have elements
given I j by a double index, then the matrix
made of all those elements I'm going to write in
sort square brackets with a symbol I j in it.
So just saying this are the indices.
This are the elements that make up the matrix.
Oh and rows before columns.
, in case I haven't mentioned that yet.
, okay.
So let's go.
We've got matrices and so far matrix is really just
a table of numbers.
There's nothing more about it.
So what can we do with matrices?
The first thing is we can define rules how to
calculate.
So easiest thing we can do is trying to add
two matrices together.
And I've given you a few matrices here.
And I want to know from you which matrices which
pairs of matrices can you add together so that I
can add every pair of these together, or only C
plus D or others, or actually I can't do anything
with these.
I cannot add these matrices together.
So again give me a second.
I'm going to reset the poll here.
, and we're going to do the same thing again.
Again I'm going to give you a one minute, ,
for that poll.
Now I see that.
And you see the timer as well.
So about ten more seconds.
Okay.
150.
Good.
Good results.
Right.
Very good, very good indeed.
So now these all have a different shape and you
cannot add them together.
You know, you can only add matrices together if they
have the same shape.
Now very few I was actually expecting more have answered
B that we can add C and D together and
you might think that is okay.
I mean, if I ask you to add those together,
you know what you do.
You know, at first element of one vector with first
element of the other vector.
But this is a row vector and a column vector.
And they really are different things.
And it's really important to keep, keep in mind that
the difference that a row vector is not the same
as a column vector.
Although I have to say sometimes out of laziness or
space restrictions or so, people will write a row vector
when they really mean a column vector.
But we should try not to do that and keep
that in mind.
So a few things , to say so we can
only add matrices of the same size or more, ,
precise of the same shape.
, if I add to m times n matrices, I'm
not changing the shape.
It's still m times n.
You must check that these two matrices are the same
size before you're trying to add them.
Especially important if you've got sort of long actually your,
, your, , , homework, , has some matrix expressions
that I'm asking you to simplify or work with.
, although I think in that case all matrices happen
to be the same size.
Anyhow, , remember row vector is not a column vector,
, and vectors and to some extent are really just
a special case of matrices here.
Yeah.
They're just , n times 1 or 1 times m
matrices.
, depends a little bit on the interpretation.
reputation, but for the time being, you can think of
these of just special matrices, really.
, if I'm talking just about a vector, , we
usually mean by this a column vector.
So unless it's clear for some reason that I want
specifically a row vector unqualified, if I say something is
a vector, it's always a column vector.
, okay.
So next thing is we can multiply a matrix by
a scalar.
, and we're doing that.
If I say k times a, I'm just multiplying every
element of my matrix by that scalar.
, now first of all, what is a scalar?
, magnitude.
Sorry.
Only magnitude.
Right.
Okay.
Are you a physicist?
I think no, sorry.
I'm asking.
I think it's correct.
I'm just thinking, I think the sort of magnitude direction
thing is, I know from the physicist that they do
distinguish them.
I know that's right.
There's another answer there a number, a number.
Yeah, a number in this case actually will be a
number.
, in general by scalar I mean it's sort of
a one dimensional object.
Yeah.
For what we're doing in this course, we will always
work with numbers, probably always with real numbers.
It could quite easily be a complex number.
, those of you who are doing mathematics, who will
take a more abstract way into things later, , it
doesn't have to be a number.
It could be anything.
It is something that your vector space, your matrices, are
defined over some very, very abstract objects.
, as long as these things are one dimensional.
Yeah, it's a sort of one dimensional object, whatever that
means in the situation that you find yourself in.
For us, this really is just the number at the
moment.
And just for completeness, , I'm just going to go
do this here, , on the visualiser.
, give me a second.
, there is what I had.
So, for example, if I want to do something
k times, , the matrix one, two, three, four.
So, , I really am just going to multiply every
element of the matrix.
So this is going to be k times one.
, this is going to be k times two.
Or I can just write this as three k and
4k here.
So that's the matrix k times.
the matrix there.
Okay.
So I'm going slightly slightly fast through this because I
think at the moment I believe this is relatively straightforward.
transposed.
So if I've got an M times n matrix not
necessarily square.
, the transpose of the matrix, , is the n
times m Matrix.
So now swapping number of rows and number of columns
whose rows are just the columns of the original matrix
in the same order.
, I've written there a you usually denote this with
an A and an uppercase T.
, some people are a bit particular and say this
isn't really a T, there should be some geometrical symbol
there.
, but I think a T will do.
, and again, I believe I've got a poll through
that.
So here we've got a matrix, not square.
, a little bit more fun.
, which of those?
There is the transpose of my matrix.
And again, reset the poll and start it.
Right.
And I'll show you what we've got.
Yeah.
Right.
Just about finished.
Okay.
Yeah.
Okay.
Okay.
Very, very good.
, pretty much everybody's got the right answer to see.
I just noticed that, , seems to be always see
the right answer.
Or, , should we agree the pose?
I find sometimes you need to be a little bit
careful of which actually is the first row, which is
the first column.
, especially if it's a non square matrix, but it's
more a matter of, of concentrating rather than not understanding
typically.
Okay.
All fine so far.
Now, now we're getting to something which I think is
a little bit more elaborate , and more of possible
confusion.
So I don't want that.
I do want that.
I do want that.
I do want that.
Sorry sorry, sorry.
Confusion.
So, , multiplication of a vector times a matrix.
, and I suspect that the majority of you will
have seen something that.
Multiplying matrices is vectors at school.
, there may be not quite in this manner here.
So Nicholson says the following in its definition.
, he says in order to multiply a matrix of
the vector, we are taking the matrix M times n
matrix.
So we know that means m rows and columns.
And we're going to write this down as its as
its columns.
So we're going to note them.
So we're writing down this matrix really just as a
collection of column vectors here.
And that's a relatively standard trick if you're working with
matrices that occasionally, , it's useful to view a matrix
as a collection of columns or as a collection of
rows, and then introduce notation to do that.
So the first column here we're going to call a1
and then a2 and then a n and just two
because we've got an m times n matrix to n
columns.
It means you've got n vectors here.
And every one of those vectors, because we've got
Em Rose is now a vector with Em components.
Yeah.
So this is what we have for the matrix.
, and then we also.
We also got to have a vector and we need
a vector with n components.
That's important.
If you do a matrix vector multiplication that the dimensions
match.
So we have a vector with n components.
And then we say multiplying those we do the following
that we take the first component of our vector
here multiplied with the first column of our matrix.
And then add to that second component of the vector,
multiply it by the second column, and so on.
, so number of components in the vector has got
to be the same as the number of columns.
And we add that.
And that is the multiplication of a vector by a
matrix.
, and I think it's useful after doing this sort
of generally it's useful to .
Sorry.
Wrong one.
I want to see what I'm doing with you.
To just do an example.
, so, , matrix, I do we see that?
Yeah.
Matrix vector multiplication.
So you say we've got the following matrix here.
We've got a matrix one, two, three, four.
, and I'm going to do that.
Multiply this by a vector which I'll just call x
y.
, the only reason I use letters here are the
numbers that you sort of see where, where different items
come from in the end.
So , and then I've got these two columns here.
So I'm going to call this here column A1.
And that here is column A2.
and then I say what this is, is the
first component of my vector times A1 plus the second
component of the vector times A2, right?
, some people, I think, again, physicists probably to
underline all the vectors.
, mathematicians tend to sort of forget about that and
all the same thing anyhow.
Or in other words, we always have to keep track
of what certain objects are, , vectors on special from
others.
So you just have to do things by context.
So this is, , you have space there.
, x times one three plus y times two, four.
, and then I can go and probably need more
space here.
, and say, so this really is just a vector.
And I can now, , do it for every component
of the vector.
So this is the vector.
Which component?
The first one is x times one.
So this is just x plus two y in the
first component.
And then three x plus four y in the second
component.
So this is our how we do matrix vector multiplication.
Or at least how Nicholson says that we should do
matrix vector multiplication.
Yeah.
, so this as I say, this might be this
way of doing matrix vector multiplication might be different to
what you've learned in school.
, can I just ask for who have learned a
different way of doing this in school?
, so quite a few, , who has learned it
this way?
Also, slightly less suspect.
The others haven't done it at all.
, right.
Okay.
, I say a little bit more about that, but
remember, this is this is doing matrix vector multiplication is
one possible way of doing that.
I've done the example.
So now we know what matrix vector multiplication Implication is
we need to think a little bit about its properties.
, so first thing I'm asking you.
So say you're given a matrix and a vector and
you can multiply them together.
Dimensions work.
Otherwise I'm not saying anything about that.
We know that A times x, the matrix vector product
is zero.
And we know the vector x is not zero.
, what does that mean for the matrix A?
Do we know that A is zero or not?
, and , let me just quickly go back.
Right.
Oh.
Oh.
All right, so this is a bit more split now.
.
Some people reckon they've got a counterexample.
So I think I'm going to do what I said
I was going to do.
I'm going to do the poll again.
You've seen the results here and I'm going to do
the whole thing again.
You discuss with your neighbours.
, and I'll see.
, see whether this makes a difference.
Just to warn you, if you say, just be if
you say a false and a half a counterexample, I'm
going to ask you for your counter example.
Okay.
10s.
Okay.
That's right.
Nobody's changed their mind.
Did I do something wrong here?
Okay, okay, so, , majority thinks this is true.
And indeed, you would think so.
You would think so.
I mean, after all, if we're doing numbers, if I
do, .
Right.
Okay, which one do.
I'm not going to do this.
I don't want that one.
, let's keep that up there.
The results.
So if I'm doing numbers and say axe equal to
zero, then I know that I have a equal to
zero or x equal to zero.
Yeah.
And this is relatively fundamental.
I mean, if you're doing things , , Factorising polynomials
for example, and setting that to zero, you know, one
of the factors have to be zero.
This is something the argument comes at a lot of
the time.
And you've internalised that argument.
, now the important thing to keep in mind is
we're not working with numbers anymore here.
Yeah.
We've now introduced a, a whole new concept of vectors
and matrices.
And we introduced rules of calculating.
And now we have to think about what things that
we used to actually hold in this case and what
don't anymore.
So this is the first thing that we're thinking does
this still hold.
Now people have said they have no counterexample.
, and so let me just go back to that
here.
That we know what we're talking about actually.
So who has a counterexample?
I think you're.
Sort of the matrix is 0001.
Zero.
Right okay.
Do I.
Yeah.
You can see it here 0001X0.
X or anything.
Zero.
Yeah.
Yeah.
.
All right.
Okay.
So I can do something that instead of the
X here.
Doesn't have to be an X can be any number.
Yeah.
So I could for example have a five here.
And then if I work out what is a x.
okay.
First, first.
, so five Five times zero zero.
Second element is zero times zero, one and five.
Times zero zero will give me a zero and zero
times zero.
One will give me zero everything zero.
So it doesn't work.
Or in this case, , this isn't true.
I can't have x not equal to zero, A not
equal to zero.
And still their product is zero.
So as I said be aware that we're talking about
and we keep doing this in mathematics introducing new concepts,
introducing rules how to work with them.
But any properties that we think these things now have,
we'll have to be really careful and have to start
proving them and have to be convinced ourself that this,
that this does work well.
And there's probably one of the main takeaways from this
first math lecture, , for you, that you'll have to
get into the habit , of always checking that.
Okay, , keep doing this on the wrong one.
So let's go on to, , still matrix vector multiplication,
actually, but the other way.
So, , for the other way of doing this, we
need the concept of a dot product or a scalar
product.
, and that is the following, , that you say
I've got given two vectors of the same dimension, say
n dimensional, , and I can now do the dot
product of these by saying by basically sort of zipping
them up, , first element of the first vector times,
first element of the second vector plus, second times, second
plus third times, third and so on.
And again, , I think, , just out of completeness,
I'm going to do an example.
, and I do this partially because I reckon, ,
the dot product is probably in this sort of operations,
what you can do with matrices and vectors.
This is probably the most used one.
You keep doing this.
This keeps popping up everywhere, and you keep having to
do dot products of two matrices of two vectors.
So , if you're doing this this is one times
five.
Here the two first element multiply together plus the two
second elements three times minus two plus seven times one.
and if I do this so I've got five
, five plus six is 11.
this is no minus six five minus six is
minus one plus seven is six.
So the dot product here is six.
Yeah okay.
, from the example be aware of the notations here
and be aware in two ways.
First of all, people use different notations for the dot
product.
I use this dot in the middle, and I think
Nicholson by sort of implication, does something that.
, I've seen, at least in some Scottish schools or
some Scottish school books, that they use a dot at
the bottom of the line.
, other people use this bracket notation U comma v
, I believe, especially if you're doing stuff in sort
of more abstract vector space at some point, , or
the physicists tend to think it's quantum physics or something
that.
They're half notation that are inspired by that.
, but these are all the same thing.
These are all the dot products and actually u transpose
V.
So taking the vector u transpose a row vector times
v as the matrix vector product turns out to be
the same thing.
, and some people prefer to use that notation, ,
for the dot product.
On the other hand, be aware that there's other notations
u dot v, u cross v and simply saying
UV, which all mean different things.
So you dot v is the dot product that we've
done.
You cross V is the cross product.
I think we're doing that later.
You find the cross product is useful in three dimensions.
And it's something that I think those of you doing
computer science, computer graphics or physics that are quite often
working in three dimensions will use the cross product quite
often.
Mathematicians tend to not because we tend to work in
n dimensions rather than three dimensions, and there's no cross
product there.
You've just written that.
I would interpret this as a column vector times a
column vector as matrix vector multiplication, and then it just
doesn't work.
, the dimensions don't match, actually.
, so you can't do that, but be aware of
these different notations.
, so.
But if we know the dot product, Nicholson now has
a different way of defining what a matrix vector multiplication
is.
, and it says the following.
Each entry of the vector axe is the dot product
of the corresponding row of A with x.
, and again, I think it's useful to have a
look at an example here.
So , if I say I want to do so
this is again matrix vector product.
and it's the row approach.
Right.
Okay.
So say I want to do the following.
I've got one, two three, four And I want to
multiply this with x y.
So this is the same thing that I've done a
little bit above.
So it says in order to get from the result,
in order to get the first entry here, I should
take the dot product of the corresponding row of a.
So the corresponding row is this.
Here one two.
Again we're taking the matrix apart, this time by rows
and taking the dot product with xy with a vector.
So I to sort of if I mentally visualise
that I take this vector then and sort of flip
it and put it over here.
So I mentally think there's an x here, there's the
y here.
And then I sort of sum that up.
So this will give me a one times X1X plus
A2Y for the first component.
And then we do the same thing with the second
component.
Flipping this over.
And I've got A3X plus for?
Why?
Yeah.
Same result that we had the last time, right?
, should say 1 or 2 remarks on that.
, we've got these two equivalent approaches of doing the
dot product.
, it tends of doing the matrix vector product.
What I've done here.
, I find, , you might find it differently, but
I find this is easier to remember and easier and
quicker to do for hand calculations if I'm actually asked
to multiply matrix vector by hand.
Now, the other approach that we did earlier, , doing
it by columns, tends to be more useful for theoretical
things if you're trying to derive, if trying to prove
a theorem or doing some theory tends to be more
useful there, but both are equivalent.
, now Nicholson decided to take the the other approach.
The column approach asked his definition of the matrix vector
multiplication, and then prove as a theorem that the dot
product way is equivalent.
That is just arbitrary.
Yeah, you could have done it just the other way
round.
You could have defined this dot product approach as being
what the matrix vector product is.
And proof the other thing as a theorem.
It's I don't know why Nicholson has done it this
way.
Personal preference.
But again, it's a concept that we see quite often
in mathematics that we've got a concept that are various
different ways how we could define them.
We define it in one manner and then prove that
all the others are equivalent.
So you will see that way of working quite often.
, okay, so a couple more things.
, for the end of the lecture.
, one is identity matrix.
, so this comes up quite often.
, we have an identity matrix which is denoted by
an eye.
Or, if you want to be particular, I tend to
indicate this is the identity matrix in n dimensions, which
is the matrix which has once on the main diagonal
and zeros everywhere, everywhere else.
So what does this mean?
, so say I want i3.
, I am working in three dimensions here.
, so the main diagonal is this one here.
, I have ones there.
So I've got two, one, one and another one.
If I'm in three dimensions and everything else is zero.
Yeah.
, and the, , reason for doing that, , is
really to or the importance is really the following there
that you will find.
If I multiply the identity matrix with a vector, I
will get the vector back.
And again , let's just do that.
So if I say if I want a more deploy
this here I three , times x, so 100010001.
, yeah.
I've got the times x, y z.
, then well, let's do the column approach again.
I have x times 100 plus y times 010 plus
z times 001.
.
And that if I sum this up, this is an
X or the others are zeros.
This is a y and that is a z.
So I will, , get my.
Oops.
I'm sorry.
, I will.
Get my vector back.
, and we will see that this is quite often
with other things as well.
The identity, , will not change something.
, note I just want to make you aware.
I've written my matrix, my vector X here as a
row vector here, and to be exact, I actually need
to put a transpose on this to say I want
this flipped.
What my matrix, what the vector x is should really
be a column vector.
It's got to be a column vector in order that
I can do a identity times x here with a
row vector, I wouldn't know what to do.
Dimensions don't match.
Yeah.
So that a transpose up there is important.
, okay.
I did say if I, I n if you really
make the dimension explicit, otherwise I will do.
.
Okay.
So where are we with time?
Five minutes.
Good.
Okay.
There should be just what I need.
So I just want to say a little bit there.
So Nicholson has this theorem in there, , where he
comes with all these calculation rules.
Huh?
, actually, what are they called?
, can somebody tell me that second rule there?
What is that called?
Associativity.
Yes.
Yeah.
, the others are examples of a distributive law.
, it's useful to get into the habit of sort
of knowing the names for these things if you're talking
to other people, again, especially as you're getting into more
abstract things, as properties of these objects you're working with
become more important than the actual values.
Yeah.
, again, these are rules, you might think if you
see something that.
Well, why do we why do we do this?
Yeah.
I mean, these are stuff that you've seen 100 times,
probably when working with numbers.
, you sort of know the names, but it's relatively
obvious.
It goes back to the same thing that we had
before.
We don't know.
In this case, we've just defined completely new things to
work with vectors and matrices.
So we now need to think about what properties do
these things have.
Well, , so we have that if I do a
times in brackets x plus y I can bracket this
out a times x plus a times y.
That is by no means self-evident.
Now we we actually happened to have defined the matrix
vector multiplication in such a way that this works well.
But we need to check that that is the case.
And the same thing with things .
So I've now got a multiply scalar with a matrix
with a vector.
Or I can multiply a scalar by a vector here
or I can here.
This is that I multiply the scalar with a matrix
first, and then multiply that by a vector.
Three different types of objects are certainly not obvious at
all that something that should hold.
So be aware that if we're working with these things,
always check that what you're thinking works.
You can actually do that.
You know, get into the habit of that need to
prove everything.
All right okay.
And I think the final thing before we stop it
here, , now that we have a matrix vector multiplication
that sort of links it now up to what you've
done last week, , this gives us a way to
represent linear systems of equations.
, so, , we've got the following there.
We've got the system of equations.
, and I think let me just take what I
think.
I really didn't want to do anything more than to
point this out.
.
Yeah.
So I've got the system of equations that I can
now write this, , as a matrix vector multiplication.
, so I could now say that I take a
matrix of coefficients, and in this case just the coefficients,
not the right hand side.
For Gaussian elimination, you've copied the right hand side as
well.
So I just take the coefficients.
So this would be three, two, three, four, six, seven
and eight.
and I can multiply this now by a vector
of unknowns x1, x2, x3 and say this is equal
to the vector of right hand side.
So this is five times nine, five and nine.
And if I write down something that and I
now do the matrix vector multiplication, what I've written down
here, you will realise that this equation that I've written
down here, actually it's the same thing as the system
of linear equations that I've got over there.
Yeah.
, and this is really it's a little bit at
the moment, notational convenience because I could do something.
I could say, I'm going to call this matrix here
a yeah, I'm going to call this vector here x.
And I'm going to call this right hand side here.
For some reason right hand sides are always called b.
and then what I've got here is I can
write the whole linear system of equations as axe equals
to be.
So I can just write this as one single equation
which because this vectors and matrix is actually means it's
a whole system of equations.
But it does make this a, this is used quite
often and it does make things convenient of working with
that.
Right.
Okay.
We're just about at the end of the lecture for
today.
, so I'm going to carry on tomorrow with matrix
matrix multiplication and then a bit more talking about that.
, see you then.