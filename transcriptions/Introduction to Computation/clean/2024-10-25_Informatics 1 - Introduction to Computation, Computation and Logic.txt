Okay.
Good afternoon.
So yesterday or logically speaking, last week we talked about
putting formulae into a nice normal form DNF or CNF.
We did that by Cono maps humanly, and we did
it by messing around with pushing formulae with De Morgan's.
Or at least I hope you did, because yesterday I
asked you to go away and do that.
, did anybody write a program to turn a formula
into DNF?
Nobody really.
I'm sure somebody did.
, so what I haven't done is tell you What
CNF and DNF are good for.
Why do we want to turn formally into this particular
nice, simple form?
And what we're going to do now is take a
break from turning formula into CNF or DNF and explore
what it is we can do with them once we've
got them into that form.
So what are they good for?
So the next topic of this week, meaning for today
and Thursday since we're running a little ahead, is finding
satisfying assignments.
So what's a satisfying assignment?
Well, the satisfying assignment is a way of making a
formula.
True formula mentions an assortment of variables propositions a, b,
c, d, p, q, r, whatever.
And given a formula, you might ask, can we make
the formula true?
So when we were doing proofs, we found a way
to work that out so we could take a formula.
We could try to prove it.
We might find that it was a tautology, in which
case it's always true.
We might find it was a contradiction, in which case
it was always false, or we might find that it
was neither, in which case doing a proof, a sequent
calculus proof gave us a set of assumptions which were
sufficient to make it true.
Sequent calculus proof is not a particularly fast way of
doing things, and in applications of logic, one wants quite
quickly to find out whether a formula can be satisfied
and if it can be satisfied, what the values of
the variables are.
So there are countless examples of things that can be
turned into.
Here's a formula.
Find settings of the variables to true or false that
satisfy it, but make it true.
One example is timetabling.
So timetabling your exams.
Obviously timetabling the exams for this course is nice and
easy, but timetabling all your exams is a very hard
problem, and it can be turned into a problem of
finding a satisfying assignment for a Boolean formula, a large
Boolean formula, but nonetheless a Boolean formula and many, many
other problems can be.
In fact, it can be proven that any problem can
be turned into, well, any reasonable problem can be turned
into finding a satisfying assignment.
There are problems that are so hard we can't even
do that.
But many practically important problems can be expressed as finding
settings of a variable that make a formula true.
So what I'm going to do today and Thursday is
introduce some formalism and introduce a little bit of programming
just to show you how we would program it.
And then I'm going to talk about a very, very
famous algorithm for finding the satisfying assignments of a formula.
Now, I'll tell you in a little bit about why
it's famous and why it deserves to be famous.
But I thought I might just say a little bit
about the people attached to it.
So it's called DPL.
That stands for Davis, Putnam, Laghman and Loveland.
And notice only one of them is still alive.
And Martin Davis sadly died last year.
And it's a very interesting set of authors, because these
two people were very well known to me before I
started teaching this course, and I'd never heard of these
two.
So despite the fact that this is a core piece
of computer science.
Who are these people?
Martin Davis is not, by any stretch of the amount,
was not by any stretch of the imagination, a computer
scientist.
He was a set theorist.
, unless you are doing maths and you already know
that you're going to do set theory, you probably know
very little about modern set theory.
Modern set theory is fantastically complicated.
, it's one of the most abstruse areas of pure
mathematics.
, just to give you an idea, I used to
do a bit of set theory because it has applications
in my computer science research.
A decade or two ago, I subscribed to the Journal
of Symbolic Logic, which is the main publication, and in
a typical journal there were about between 2 and 5%
of the journal was something I had some hope of
understanding without.
Well, actually no, no qualifications.
Something I had some hope of understanding.
Set theory is an old and very complicated subject, and
has nothing, apparently to do with finding satisfying assignments.
I still don't actually know the story of how Martin
Davis came to get involved in this.
Hilary Putnam, as you can see, he's another was another
older person.
He's also famous.
He's mainly famous as a more philosophical logician, but also
with some logical technical achievements of note.
But he was not a computer scientist by any means.
So we have a set theorist and a logician, and
these two people whom I hadn't heard of are in
fact computer scientists were and are computer scientists.
They just happened to work in areas of computer science
that I don't.
So you can go and look them up and find
out what they have done.
Loveland is still alive, but not working, which in America
is unusual because they tend to go on forever.
But what they came up with is an algorithm that
is remarkable for its simplicity, its and its effectiveness, which
is more or less the definition of beautiful mathematics.
So before we get on to talking about DHFL, let
me talk about CNCF a little more formally and think
about how we might represent this in Haskell.
So this is the kind of thing that you should
have been doing yesterday after the lecture, when I said,
go away and work out how to do it.
So we looked at corner maps, and you have also
looked at representing formal languages of various kinds in Haskell.
So you've represented Boolean propositions using the or and the
and with two colons as constructors.
And what I'm going to do now is illustrate how
we might represent Boolean propositions specifically in CNF, and use
them in Haskell to solve problems.
So let me refresh the terminology.
This is the terminology from last week.
And the technical terms as usual are in purple.
So we're going to need to have those embedded.
And the terms I haven't used last week except accidentally,
but these are fairly standard and one has to remember
what they are.
So remember CNF is a formula that is an and
of ORS.
So it's a big and an.
Each of the things that is being ended is an
or of simple formulae.
So that means a formula.
So a formula is formula CNF formula is a conjunction
of things.
Shall I write one?
So there's a CNF formula.
It's a conjunction of things.
The things that it's a conjunction of will be called
clauses.
, so this is a fairly standard logical use of
clause which is vaguely related to its grammatical use, but
not very.
And each clause here is a disjunction of things.
And what's it's a disjunction of either atomic propositions or
negated atomic propositions.
And those things we call literals.
, why are they called literals?
It's a historical piece of terminology.
It's because they literally say what they mean, I guess.
So the term literal means either an atomic proposition or
a negated atomic proposition.
And atomic means just the the things that are part
of our language, part of our universe, part of our
model.
, the terminology I'm going to use all the time.
So it's important that you absorb it.
Any questions about the terminology?
So the only thing to remember is that clauses go
inside formally.
, here's a piece of Haskell for you.
By now, this should be easy to read.
So let's go through it and check that all of
it is Haskell.
You know, I think it is all Haskell, you know?
So I'm defining.
A data type literal which takes an argument type atom.
So given a type of atoms, a literal of an
atom is either a positive atom or a negative atom.
So I'm using p and n for positive and negative
notice by the way that this is kind of adding
notation.
So I'm effectively.
Adding plus signs or something into the notation to say
this literal is positive rather than just using the atom
by itself.
The reason we do that is so the types work.
I want literals to be different from atoms.
So a literal has to be either the atom occurring
positively or the atom occurring negatively.
Is everybody okay with that as a piece of.
As a sensible piece of Haskell?
I see some slightly half hearted.
So anybody have a worry about it or question about
why I'm doing it?
So really, the only question is why do I have
positive atoms?
Why do I distinguish literals from atoms?
It's a general principle of Haskell programming.
Have as many types as you need.
So never feel shy of introducing another data type because
they let you separate things out and do pattern matching.
And if you ever find you have too many, you
can always convert between them.
So having defined a type of literal things I'm then
going to define a clause.
And this is a slightly confusing piece of Haskell.
But why not?
What is a clause of atoms?
A clause of atoms is defined this, right?
Can you all now read that Haskell definition?
You'll notice that I haven't put the types in there,
so I'm expecting you to be able to work out
what the types are.
.
What is the type of this capital o r thing?
Nope.
Sorry.
What's wrong with it?
So it takes a list of literals, because you can
see I'm applying it to a list of literals.
But what's it returning?
A clause, more specifically, a clause of atoms So this
is actually your standard Haskell algebraic data type definition.
, Don told me that algebraic data types are where
Haskell starts getting difficult for most people, which is unfortunate
because it's the core of what Haskell is.
Remember, when I write a data type this and
I'm using this thing that helpfully colours things for you?
So this red or is a data constructor?
It's a thing that we use to build up a
data type.
The brackets are lists.
So what we have in here is a literal atom.
This is a list of literal atoms.
And then I apply the or data constructor to it
in order to produce something that is a clause of
atoms.
So a clause is essentially a list of atoms joined
by Also, but it's important to note that this thing
is simply a data constructor.
It has no meaning whatsoever.
So all I'm doing here is taking a list of
clauses, for example.
I can't draw it because I have to draw it
in Haskell.
I'm taking a list of clauses and I'm tagging it
with the Or data constructor.
And that gives me a thing that is a clause.
, why am I doing that?
I could also say that a clause.
I could do this.
So a clause is basically just a list of the
atoms in it.
Why have I made it a datatype by adding this
constructor?
I remember the fact that this says or has no
meaning.
It's not actually doing any kind of or it's just
the tag.
Why am I doing that?
Rather than simply defining the clause type to be just
a list of atoms, a list of literals.
Any clauses, a list of literals, and any list of
literals makes a clause.
So why don't I just define them to be the
same thing?
It's easy to understand.
Yeah.
What a reason.
There are many reasons, but so one is understandability.
Yeah sorry.
Readability.
So that's it.
So if it's a different type that helps me read
it, I can see that it's supposed to be an
error.
So although this all has no actual meaning, obviously as
a human I can see that it's the English word
or and that reminds me what I'm doing, that a
clause is something that is an or of literals.
What's another reason?
A more practical reason.
Yeah.
It makes it easier to.
, I don't have to put in AWS myself.
Well, I do, because the author has no meaning.
Okay.
So if I want to evaluate it, for example, I'm
going to have to write a function that takes this
capital or thing strips off the capital o r and
applies little or to the list.
So it doesn't make it easier.
In fact, it makes it possibly harder.
To.
Differentiate between CNF and DNF.
So that's a a good reason.
I might have clauses for DNF and CNF.
I'm not doing that at the moment, but I could
, which is kind of leading on yeah, pattern matching.
So I can now pattern match against the Or.
And I know that I'm dealing with the clause which
leads on to which is leading even closer to one
of the things that modern programming languages and Haskell in
particular, do.
A lot of that makes your life much easier.
What's the thing they do that makes your life much
easier?
Type checking.
If I say that a clause is a list, then
Haskell can't tell the difference between something.
I mean to be a clause and something.
I mean to be a list.
If I do this, then it's a different type.
And so Haskell will tell me if I'm randomly taking
a list of things that happen to be literals and
using them where I didn't intend to use them as
a clause, or vice versa.
You might think that's not very likely to happen, but
you'd be surprised what you can come up with because
you use lists everywhere in Haskell programming.
And this is what I meant when I said a
couple of minutes ago, introduce new types freely because they
give you type checking.
Sometimes you find that the type checking then gets in
the way, and you have to write functions to convert
between types.
But that's safer than saying, oh, I don't care about
type checking.
, so this is the official line, and I believe
it when I'm lecturing about Haskell.
As I may have mentioned before, the language I do
most of my ordinary everyday systems programming is Perl, which
is notable for having about three types, and it freely
converts between them whenever it thinks it's a good idea.
Which makes for some very hard debugging problems sometime, but
makes it a good intellectual exercise.
Okay, so that's a clause.
Having defined a clause, I'm going on to define the
type of form for formula.
I don't know why I was too lazy to write
formula just because I'm inherently lazy.
And what is a form?
A formula is just an and of a list of
clauses.
So I'm doing exactly the same thing.
And again here the and has absolutely no meaning.
It's just the data constructor, a tag attached to this
list of clauses to say that I mean it to
be a formula.
So.
So far I have talked about literals and clauses and
formulae.
So now we've got that far through.
Any questions about what I'm doing there?
Have you all mastered polymorphism?
No.
Okay.
What is this atom?
What is atom in this definition?
Yeah.
Sorry.
, well, quite likely I'm going to intend it to
be a string, but that's not what it is in
the piece of Haskell in front of you.
Yeah.
A type placeholder, a type variable, I think we say.
So what I'm saying here is that I can take
any type.
Plug it in and get a type of literals.
So for example.
If I have a type string, then I can define
literal of string to be positive strings and negative strings.
So what I'm doing here is saying I want to
give myself the freedom to use whatever is convenient for
the atomic propositions.
Strings are an obvious example, but as you see down
here, that's not actually what I'm doing in this one.
I'm doing something much more primitive and arguably rather stupid.
But it's easy and it makes for short formulae.
So polymorphism, this ability to define types that depend on
other types is another critical thing about functional programming languages,
and it's now spread into almost all other modern languages
as well.
We call it polymorphism because the type literal can have
many shapes.
Polymorphism means many shapes, and it can have a shape
with a string, the shape with a number, the shape
with a boolean, the shape with or whatever you
for atom.
And in this particular definition I'm going to define a
type A data type which is uppercase atom.
So this is an actual type.
And I'm just going to say oh I'm just going
to have abcd w x y z.
So this is a very simple finite predetermined set of
atomic propositions, which is enough for playing around with things
as was suggested.
Anybody with any sense would probably make atom B string
so that you can have whatever atomic propositions you ,
but for reasons I now can't remember, possibly because I
inherited the code.
This is how we're doing it here , notice there's
the usual bit of magic here.
Has this been explained to you officially yet?
I explained it early on, unofficially, sort of.
Maybe a bit.
, so to remind you what it means, it says
tells Haskell to do the obvious thing with this.
It tells it that A is equal to A, and
that if it's going to, if it's asked to print
the value of A, it should print a.
, it's not really to do with programming.
It's a bit of magic.
And in practice I'm also going to put deriving it
show into everything else as well, because it makes life
a lot easier.
So that's a page of Haskell data type.
We'll start using it in a moment.
But before I go and start using it, any final
questions about the data type definition?
I think this illustrates everything you're supposed to know so
far in Haskell data type definitions.
, well, what can we do with it?
Here are some little pieces of code which do simple
jobs that we're going to need.
So for example, if I have a literal what's a
literal?
It's a positive atom or an negated atom, and I
want to negate it.
How do I write the function?
So the negative of a positive atom is a negative
atom, and the negation of a negative atom is a
positive atom.
What's the sequent calculus equivalent of this function?
Yeah.
Yeah.
No, not quite contrary.
Well, no double negation.
So remember, we assume that double negation behaves in the
way that we think it does logically.
And this is effectively implementing double negation.
Because if you negate a negative you get a positive.
here is just the definition of a little example
formula.
So this is.
If I write this formula out normally it's, .
Where am I?
So assuming I've written that correctly, then this formula e
g is the thing I've just written on the board.
Have I written it correctly?
Can you all see whether I've written it correctly or
not?
Yep.
So hopefully this explains slightly more clearly the relationship between
the data type and the actual formulae we're thinking of.
Of course, here I've written it out with an symbols
in between.
I can make it more Haskell ish by.
Making this a list.
I'm putting a big hand on the outside or something,
but who cares?
So that's a formula.
But in order to work out whether a formula is
true or not, I need to know something about the
universe that it's describing.
And when we've talked about universes, we've said that they
have an assignment of truth values to the propositional variables,
Or we know which sets of things a particular propositional
variable is true of.
So, for example, A might be true in the universe,
C might be false or whatever.
And that raises the question of if we're looking for
universes in which the formula is true.
How are we going to represent.
A valuation what's a valuation?
The valuation is the thing that tells you whether each
variable is true or false.
Now don't look at what I've done here.
If you don't look at that, what do you think
is the natural way to code in Haskell evaluation?
It might be a bit tricky because you might say,
is there a natural way here?
Okay.
So that's.
.
What is evaluation?
It's something that takes an atom and tells you whether
it's true or false.
So the obvious thing for evaluation to be is a
function from atoms to booleans, because that's what it is.
But that's not a very nice data type to be
typing in and working with.
Because how do we define the function.
Okay.
So we're going to have to define the function in
terms of something more primitive.
You might write it out case by case.
So for example we could just say.
Is it lowercase true.
In Haskell?
I can never remember capital.
So we could write out a function that.
That would be one way of doing it.
.
There's a reason for not doing that, which is that
functions are rather hard to break down into their component
parts, because we can't do pattern matching on a function,
we can't look at the code of a function.
A function gets compiled into code.
So it's not something that we can analyse except by
just trying it out on things.
So what we tend to do when we're trying to
represent evaluation is say, what we want is some nice,
simple data type that Contains the information that would be
in the function.
And there there are lots and lots of ways of
doing it.
So one way of doing it might be.
To have a list of pairs where each pair is
an atom and its value.
So that's a very simple.
Simple is good.
So never, never underestimate the benefits of a simple approach.
If I did that, what kind of problems might there
be?
Yeah.
Could have multiple values.
So if I'm.
Either deliberately or unintentionally.
Do that.
What do I mean by it?
So if you do that, you have to have some
idea of how to deal with creating things with multiple
values.
There are many ways in which one would do that.
So actually what you would do is you wouldn't use
lists, you would use sets because that gets well.
No, even sets isn't enough.
You would use some kind of array structure keyed by
the atom name.
Or you could use lists that.
But then you would have to write functions to manipulate
it and make sure that you never introduce multiple values.
So there's a lot of work that has to be
done there.
And so what I'm going to do here is use
something that is even simpler minded than that.
And has the benefit that it accidentally allows us to
think about valuations that we don't yet know about properly.
So I'm not going to give you valuations that have
the form.
Here's a value.
Here's an atom.
This is its value.
I'm just going to give you a list of which
literals are true.
So valuation over a particular type of atoms is the
val constructor.
And again this is a data type and structure.
It's meaningless.
And then it's just a list of literals nothing more.
What I mean by that.
Is that this list of literals is the ones that
I take to be true in the formula, as I'm
evaluating it.
Does that share some of the problems we just talked
about with lists?
Yes it does.
Right.
Because if it's a list of literals, there is nothing
stopping me writing.
Nothing stopping me writing.
The so-called valuation that.
Says that both A and not A are true literals.
, what am I going to do about that?
What I'm going to do about that is say we
will try very hard not to do that.
So.
Yeah.
Why not?
, why not make it a list of atoms?
because we're going to be manipulating literals.
And at some points we might want to say we
don't know whether which way this literal is.
So it'll turn out that it's convenient for the algorithm
that works.
Yeah, making it a list of atoms is the obvious
thing, but actually it turns out that this way we
can, by the back door, represent literal atoms that we
know to be positive, atoms that we know to be
negative, and those that we don't yet know anything about.
So it's a bit sneaky.
, which from a Haskell point of view, I think
is very bad programming style.
There is a Haskell way of doing it more nicely,
but that would require more effort, and you're welcome to
think about that.
Okay.
So if I've got a formula and evaluation, can I
write down a little Haskell function that tells me the
value of the formula.
True or false, given the valuation.
Well, of course I can.
And because Haskell is so wonderful, you can do it
all with a one line doubly nested list comprehension.
Which I have to stare at for a little while
before I understand it.
So you probably also have to stare at it a
little while, even after six weeks of this stuff.
, what's the function doing?
So this is the valuation.
And remember, this is just pattern matching against the tag
which says it's a valuation.
So this is a list of literals.
This is the formula.
And remember a formula is just an and tagged list
of clauses.
So CS here is a list of clauses.
And then I'm saying well I expect an end of
a set a list of clauses to evaluate to the
end of the ORS of all the things in it.
So what does this say?
This is doing the and over.
All the things that are in the list of clauses.
Now each clause is itself an all of stuff because
that's the way I defined it.
So I've got some implicit some pattern matching going on
in here.
So each all of see each all of the clause
is a member of the list of clauses.
And then given the list of literals here I'm saying
take each literal from the clause and determine whether
it's in the valuation list or not.
And then all all the result of that.
So given a complete valuation that tells you whether things
are true or false.
This arguably does it.
So if you want to try it out, I will.
Is the code already up on the open course page?
Yep.
So you can try this.
You can try this code and play with it.
If I try it if I look at the empty
valuation list.
So this is the formula which has no clauses.
And is.
So sorry this is an empty empty valuation.
So nothing is true.
Not even negative literals are true.
What is.
So any formula is going to turn out false.
Here is a valuation which says C is false, a
is true and D is false.
And if I apply that to the example formula E
e.g. which is shown down here.
Then you can see that it will indeed come out
true.
, and then as we said, the valuation is strange.
When I did this, this is an empty valuation.
So it's going to make everything false.
In particular, it's going to make a false and it's
going to make not a false, which is not very
meaningful.
, there are in fact some logics which are useful
in many circumstances.
Well, some circumstances in which this kind of thing is
actually allowed.
You can talk about a situation where A is not
true and not A is not true.
And you can even talk about situations where both A
and not a, a true, which is normally a contradiction.
And once you have a contradiction, then the whole mathematical
world collapses.
But actually it's useful in some circumstances to be able
to reason about contradictory things.
Because, amongst other things, people are very good at holding
contradictory beliefs.
So if you want to reason about the way people
think, it's useful to be able to handle contradiction.
Another application is database updates.
, but this is an aside.
What we're doing here is defining a kind of partial
valuation thing, tells us what we know to be false
and tells us what we know to be true.
And then given such a valuation, this very simple function
will tell you whether a formula is true or false
in it.
And if we don't know anything about a variable, it
defaults to being just false, both negated and positively.
, so you should have a look at the code
and play with it and check that it does what
I say it does.
Before we go on, any questions about this slide and
in particular this piece of code.
Yeah.
This is the one that we saw in the previous
slides.
the one you see I said.
.
So this is, this is the same thing with just
different variable names because this is actually extracted from the
code instead of being written lazily by me on the
slide.
Yeah.
So just l instead of literal.
I should fix that, shouldn't I?
Because it's unnecessarily confusing.
.
However, what I'm trying to do is not simply evaluate
a formula.
What I want to know is whether a formula can
be made to be true by setting variables to particular
values.
So how do we do that?
Is there a simple way to do it?
So if we have an arbitrary formula, it's got lots
of variables in it.
And we want to find out whether there's a way
to make the formula true.
Is there a simple way to do it?
I give you a formula.
It's got ten variables in it.
I say can you choose values of true and false
that make the formula true?
Yeah.
I heard that there is a kind of entity problem.
And that's true.
But we're not getting there yet because I asked if
there's a simple way to do it.
Yeah.
It's true that what happens when you want to set.
Some of the variables might be negated.
Yeah, but I mean, this isn't simple.
We're trying to find out.
Is there a way.
Is there any.
When I say simple, I mean absolutely, totally, almost totally
braindead.
You don't need to think.
It's just the simplest thing you could possibly do.
Bearing in mind that computers are good at doing things
fast and they don't get bored.
So again, try every possible evaluation of the variables.
Okay, so that's easy to program.
It's by now you should be able to write a
Haskell function that generates all possible valuations.
, if I've got n propositional atoms in my formula,
how many possible valuations are there?
Two to the n, which is an unfortunately large number.
So it's easy in the sense that it's trivial to
write the code to list all possible valuations, but it
takes quite a long time.
And this is a bit sad because these problems get
quite large quite quickly.
For example, the timetabling problem for your exams.
How many students are there in the university by now?
20,015 thousand and 20,000.
There are.
At least a thousand different courses.
There are in a three week exam diet.
There are what, I don't know, 2050 different exam timetable
slots.
There are maybe a couple of hundred rooms in the
university.
That's an awful lot of variables.
And two to that is not something that we're going
to solve just by doing a brute force search.
Can we do better?
So who thinks we can definitely do better than that?
Okay, who thinks we definitely can't do better than that?
At least not all the time.
Okay.
So the sad answer is, well, actually, the answer is
we don't know whether we can do better.
, but to the best of our belief, there is
no way of avoiding the exponential blow up all the
time.
Somebody can always come up with a really contorted example,
which is going to trigger the exponential blow up.
However, in practice we can do better a lot of
the time in the kind of examples that we actually
meet in practice.
So for example, your exams do actually get timetabled.
And so we are not hitting the exponential blow up
with your exam timetabling data.
, and.
Well, I have to be a bit careful here because
although often it works, it is actually also quite easy
to run into bad cases in some of the problems.
Why is this?
It's the one of the deepest results in deepest non
results in computer science because we simply don't know the
answer.
So if you've heard of p equals NP this is
the problem.
Can we find a satisfying assignment to a formula in
less than exponential time.
And strictly speaking, it's not quite that problem because NP
is not quite exponential time, but it's roughly that problem.
, and if you can find an algorithm works fast
always, then you will win what is now not an
enormous amount of money, but is still a handy amount
of money to have.
Just buy you a decent flat in Edinburgh.
And much more importantly, you will be assured of fame
for as long as civilisation survives.
Which is what, about 30 years at current rates of
progress, but a long time anyway.
So P equals NP if you haven't come across it
before we will say a little bit about it.
Maybe not in this course, but it will come up
again in second year, and then you can have the
opportunity to study it in detail in third year.
It's really deeply embarrassing because it's a very easy problem
to state, and we've been looking at it for almost
50 years and we don't know the answer.
We know an awful lot about why we don't know
the answer, but that stuff is outside my competence.
But if you do fourth year courses, you can find
us a bit about that.
So despite the fact that we can't do it tomorrow,
we will do it with something that works most of
the time pretty well.