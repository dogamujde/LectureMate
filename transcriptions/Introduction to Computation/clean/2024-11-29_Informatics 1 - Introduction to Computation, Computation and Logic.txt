Yeah.
I think.
Okay, so to infinity and beyond.
Thank you for coming.
Personally, I think this talk is more fun than yesterday,
so I'm a bit surprised that there are fewer people.
, it's getting on for 30 years since Toy Story
was released.
Do your generation watch it?
Yes.
Yep.
It's still a pretty good film.
, if you remember it.
Well, you might remember that.
Well, at least in my understanding of it, which was
quite a long time ago, because obviously it's a two
new for me, and it was too young for my
son, but I think he watched it anyway.
So therefore I did, , Buzz Lightyear phrase catch phrase
to infinity and beyond.
It's one of the things that makes him look just
slightly stupid.
There are lots of things in the film that make
him look just slightly stupid.
And part of the point of this talk is that
it's not a stupid statement at all.
It's a perfectly reasonable statement.
, that, of course, is Buzz Lightyear.
This is a large array of copyright, trademark and so
on.
Symbols.
In a desperate attempt to stop Pixar suing the university
if these slides ever escape into the world.
And if you think I'm joking about all this copyright
stuff, I'm not.
One of my colleagues gave her inaugural lecture as professor,
and she took a nice picture of a field of
flowers off a picture site that advertised that it had
these things available.
The slides from the inaugural lecture were duly put on
the web, and a couple of months later, somebody said
the site didn't have permission to make these available for
public performance.
That'll be £500, please, after they were beaten down from
£5,000.
So you should worry about these things.
So what am I talking about today?
Obviously infinity, infinity and eternity are closely related concepts.
One we think of as space and the other we
think of as time.
But it's all just space time.
So they're the same thing in everyday language.
So language of normal people rather than people me.
There are idioms which express various forms of eternity and
infinity.
So in English language fairy stories, things often end forever
and ever.
And why?
Forever and ever.
That's just.
I mean, language says things twice to make them more
emphatic.
That's all there is.
Other languages are a bit more elegant.
So the German equivalent is imitative, which means basically the
same, but has some elegant variation in it.
, there's a remarkable number of idioms in English which
were coined not by normal people, but by Shakespeare.
So there's a phrase forever and a day, which is
actually one of Shakespeare's but became part of the language.
, how many of you grew up in the Christian
tradition with Latin liturgy?
Anybody?
Just a few.
Okay, so, you know, nuns separate in secular secularism at
the end of the doxology.
Now and always and in centuries of centuries.
In English, we say, as it was, as the beginning
is now and ever shall be, world without end.
, this isn't a Latin idiom.
It isn't even a Greek idiom.
It was probably once an Aramaic idiom, but we don't
actually know.
And then Infinity and eternity pop up in various fairy
stories and folktales.
This is from the an extract from one of the
stories in the tales collected by the Grimm Brothers.
And it goes, the king said.
The third question is how many seconds of time are
there in eternity?
Then, said the shepherd boy, in Lower Pomerania is the
Diamond Mountain, which is a league high, a league wide,
and a league in depth.
Every 100 years, a little bird comes in, sharpens its
beak on it, and when the whole mountain is worn
away by this, then the first second of eternity will
be over.
How long does that make?
A second of eternity?
Well, it depends on your assumptions, doesn't it?
Also, what do you mean by quadrillion, which these days
is a bit tricky?
So I get a longer answer.
, how many of you know what a league is?
, I mean, a league is a wildly variable unit
of measurement depending on when and where you were.
I think the more or less standard English definition is
three miles, but it varies from 2 to 7, I
think somewhere even 14 miles.
, so making a few assumptions a league is
three miles and the bird manages to move one atom
each time, then that makes a second of eternity somewhere
around ten to the 40 years, which is quite a
long time longer than you said.
So I must be making different assumptions, but hey, who
cares?
.
Folktales.
Who reads folktales?
Children read folktales for children, get folk tales told to
them.
And then I'm going to give you a little quotation
from another wonderful book, which can keep you happy for
a very long time.
It's called the Handbook of Mathematical Logic.
It's a big yellow book about that thick weighs a
lot.
, I understand probably about 10% of it.
By the time I die.
I might understand 15% of it.
And this says mathematicians and other children often play the
following game.
We take turns naming numbers and see who can name
the largest one.
This is a game, the psychological rather than the formal
sense, since I might always just add one to your
number.
But my goal is to try to completely demolish your
ego by transcending your number via some completely new principle.
And this is essentially a summary of what set theory
has been doing for the last century.
Kenneth Coonan is I think he still is.
I haven't checked recently.
, one of the leading set theorists of the 20th
century and is responsible not just for some of the
biggest numbers, but also for the demonstration that there is
a limit to how big you can keep inventing numbers.
There's a thing called the Coonan inconsistency, which is where
your postulation of ever more absurd infinity becomes so absurd
that you actually get a contradiction?
, so this talk is basically about postulating large numbers.
I'm going to start by postulating by talking about infinities.
And these are actually very small infinities.
But they will probably look quite large to you.
And then I'll connect them to some very large actual
numbers, because big numbers are fun.
And well, that'll be more or less it.
And there could be another talk about seriously big infinities,
but not in this course.
So let's go back a century and a half.
To Georg Cantor.
.
Interesting career.
He's now one of the.
He's considered the father of set theory.
Modern set theory.
He had a very difficult time as an academic because
he had crazy ideas which important people didn't , and
they ensured that he didn't get the positions he wanted
because they didn't approve of his ideas.
, nowadays we think he was extremely right.
So this is a quotation from David Hilbert in about
1901 saying that nobody shall drive us, meaning mathematicians in
general, and set theorists in particular, from the Paradise that
Cantor has created for us by inventing set theory in
its modern form.
So the first breakthrough in terms of infinity was actually
the one I'm not going to talk about today.
You may have heard of it.
That's the fact that there are more real numbers than
there are natural numbers, then that real numbers are uncountable.
Everything we're going to do is today is actually going
to be countable.
And then a bit later, he invented the theory of
ordinal or discovered the theory of ordinal numbers, which is
what I will talk about today.
And these are a lot of the more humanly accessible
kinds of large numbers and infinity than the stuff that
he thought about earlier.
So I want to talk about ordinals and cardinals.
What do they mean?
, as I've said so often, you may possibly have
studied the grammar of some language in an old fashioned
terminology.
And if you had a sufficiently old fashioned grammar textbook,
it will have taught you about cardinal numbers in Latin
and ordinal numbers in Latin or Sanskrit or Middle Chinese
or whatever.
It was ancient language.
It was you had to study, and possibly even in
whatever modern languages you studied.
What do we mean in language?
By cardinal numbers, we mean things that count how many
of something.
So in English we say there are two dogs over
there.
, and then we have ordinal, ordinal numbers which put
things into a sequence and say which one?
The first dog is black and the second dog is
brown.
, different languages take different attitudes to how they distinguish
these.
English is awkward.
For the first couple of numbers we go one, two,
three, first, second, third.
It's quite bizarre, actually, that we borrowed a Latin word
for such a fundamental word as second, but they are
.
So in mathematics, we mean basically the same.
It turns out that there are two rather different kinds
of ways to count in mathematics, just as there are
in language.
So you can take a set and this is just
a set in the naive sense that you know it
and you can say how many things are there in
it?
How many antecedents are there in the left of, ,
sequence in the sequence calculus?
And we call these zero, one, and two, because all
modern mathematicians count from zero, except the ones who don't
and they don't count.
And the other thing we can do is say we
don't have just a set.
We've got a set of things laid out in order.
So that's a list in Haskell terms.
And then we can say where in the sequence is
this.
So in programming language it's also an array.
And again in all sensible languages we start counting arrays
from zero.
So we talk about the zeroth element of the array
and the first element and so on.
Alternatively we can say how long is this list.
It's one two 3 or 0 elements long.
So as an example the distinction if we talk about
this sequence ABBA, that's a sequence that is three letters
long.
But if we count the number of distinct letters in
it then there are just two.
Okay, I've also said before, at some point in this
course, remember back to probably before you can remember.
Do you remember when you first learned to count?
Probably not, because we tend to do it a bit
before the age of five, and then we tend to
forget almost everything that happened before the age of five.
, you've probably got something that at some point,
but right at the very beginning, probably the absolute first
thing you did with counting was to lay down sticks
in a row.
, in due course, you may find yourself doing this.
, so the simplest way of counting is to say
that's one that's 210 sticks in a row.
That's ten.
This is what we call unary counting in computer science
notation, not binary.
Because we don't have digits.
We just lay down sticks in order.
So this lets us count lets us count rather inefficiently,
but it lets us count in a long way.
, and then we can just count forever.
I can just lay down sticks all the way out
to the end of the universe.
And where is the universe finite?
Is the universe infinite?
I mean, we believe the universe is finite, but expanding.
I mean, it's expanding faster than I can lay down
sticks, so maybe I'll never get to the end.
, can we count forever and a day?
So let's keep counting.
Laid on an infinite row of sticks.
And then after that, start another row and put down
a new stick.
So clearly that's what Shakespeare meant.
He was a set theorist before his time.
This is now getting a bit tedious to write down
because I'm writing lots and lots of sticks.
So let me introduce a bit of a notation.
Let me use the symbol omega for the length of
the infinite sequence.
Have you all learnt your Greek alphabet yet?
, even if you haven't, you should know Omega.
If you've had any exposure to the Christian tradition.
Again, because I am the Alpha and the Omega occurs
somewhere I can't remember where.
So Omega is the last letter of the Greek alphabet,
so it tends to get used for things at the
end.
And Omega stands for forever.
The infinite number of sticks, which comes after all the
other numbers.
And what's a reasonable way to write forever and a
day?
Well, it's obviously it's omega sticks plus one stick because
I've laid down one stick after laying down omega sticks.
Makes perfect sense, doesn't it?
, I want to do some more stick diagrams, but
I'm running out of space, so I'm going to do
a little trick.
So imagine I've got my row of sticks going out
all the way to infinity over there, and then I'm
just going to rotate it so that it's behind me.
And now you can see it all at once
that.
Okay.
Perspective is a wonderful thing.
So now I have a symbolic representation of Omega.
Now I can start thinking about adding things further.
There's omega plus one.
There's omega plus three.
So I can do forever and ever as well.
Lay down a row of sticks and then lay down
another infinite row of sticks.
Why not?
.
What happens if I do one plus omega?
I lay down a stick, and then I lay down
an infinite row of sticks.
Yeah.
You can't tell it apart from just omega.
I mean, I've drawn it here because perspective lets you
see that it's slightly different, but we don't have perspective
in numbers.
So we have the slightly strange situation.
That addition here doesn't mean what we think it's used
to mean.
Because if I go one plus omega, I get omega.
But we are mathematicians and so Humpty Dumpty.
Words mean whatever we choose them to mean.
And we choose addition to mean this.
, if we can do addition, then we can do
multiplication, right?
Do you remember learning multiplication?
Yeah, maybe.
Probably right at the very beginning, when you learned multiplication,
it was just repeated addition.
If I do two times three, that means add three
copies of two together, which is, of course, the definition
of multiplication.
, so just as a matter of convention, in order
to distinguish ordinals from numbers, I'm going to use the
dot multiplication symbol for ordinals rather than the times one.
Also to avoid confusion between times and x.
So let me write x dot y to mean y
copies of X all added together.
So two times three is three copies of two, which
is six.
What's omega times three?
Well, it's three copies of ever.
So it's forever and ever and ever.
And it looks that.
What about two times Omega?
So that's omega copies of two.
Well, if I take pairs of sticks and lay them
down forever, that's indistinguishable from taking one stick and laying
it down forever.
So again, we have this situation where two times omega
is the same as Omega.
.
Are you getting any hint about a general rule about
addition and multiplication?
Yeah.
, if I multiply on the left, or add on
the left by something small and multiply on the right
by omega, it's all just omega.
So there's a kind of general rule hitting there, which
is something small time.
Something big is just something big.
So it's not quite that simple, but roughly.
Yeah.
, if I can do two times omega, I can
do three times omega and 47 times omega.
And I must be able to do omega times omega
two.
Okay, so what does Omega look ?
Well, omega times omega is obviously in secular cycle.
Autumn through ages of ages.
So we have omega many a Omega, many copies of
Omega.
And again we can do our perspective trick and we
can lay out an infinite number of rows going that
way, lay them up that way, head up in another
dimension somewhere, and look at them this.
Only only a third dimension.
That's not too much to ask.
So there is a graphical representation of Omega Times Omega,
which we might as well call omega squared.
Guess what's going to happen next?
Well, I'm going to take a little interlude just to
explain why these things are useful before we continue doing
absurd things.
What's the point of introducing numbers this, which are
infinite but seem to be a sort of well behaved
kind of infinity?
Because we can see how to lay them down and
build them.
What's actually important with these numbers is that they are
what's called well-founded.
So if I take any of those numbers I've written
down, let's take that one and I go somewhere inside
it and I jump to somewhere smaller.
So that means either jumping to the left or maybe
jumping down to a smaller to a closer thing to
us if I keep jumping.
So when I jump down, I can jump as far
to the right as I , because everything here is
smaller than here.
So even though I might do something starting there.
Jump to their.
Jump jump jump jump jump to their jump jump jump
jump.
I'm always going to hit the beginning.
I'm always going to reach zero.
So that's a property of all these numbers.
If I were to define them properly.
If you keep going down, then after a finite number
of steps you must hit zero.
And that's a maybe a slightly counterintuitive property because these
are big infinite numbers.
And I'm multiplying infinities by each other.
But now I'm telling you that they have a nice
property, that as long as if you jump or at
least one step at a time, then you will eventually
hit zero and it won't take you forever to do
so.
, essentially, because there's no biggest number.
So if you're at Omega Plus one and you want
to jump, well, you can go to Omega.
But where can you jump from?
From Omega, you have to jump to some finite actual
number 10,000,705.
You can't.
So then you have 10 million steps.
Steps and you have to hit zero.
Why is that important?
Because that means they give us a general notion of
proof by induction.
So who have you think you know?
Proof by induction.
Okay, I suspect more of you probably should, but maybe
not.
So proof by induction as you learn it in school
if you do, or in the first year of courses
here.
Basically says if I can prove that if something holds
n, then it holds for n plus one, then it
holds for all numbers.
We have to be slightly more careful with ordinals, because
we have to worry about things omega, and there's
no number immediately below omega.
So the way we formulate it is to say if
we have some nice property P and it's true for
zero, and if it's also the case that if we
can show that it holds for if it holds for
every number smaller than alpha alpha, then it holds for
alpha.
So that's the equivalent of saying if it holds for
all numbers less than n, then it holds for n.
If we have these two things, then we can conclude
that the property holds for all of these ordinal numbers
we're generating.
So that's a slightly abstract one.
But I'll give you a well I'll give you a
couple of demonstrations of this.
So we'll start with a relatively easy one.
, this is a fairly condensed talk.
So on the whole I'm not stopping for questions, but
if there's a clarification point you want, then shout, okay.
So this is recreational.
You're not being examined on it.
But nonetheless it's a if there's something that would help
understanding then do shout.
So here's an example.
This is a function that you will meet in courses
later on.
It's called the Ackermann function.
It was very hard to find a picture of Ackermann
and that was the best I could find.
, it's a function of two arguments which are non-negative
integers.
So when I say integer, I mean non-negative integer, and
it's defined this.
So you can write this down in Haskell.
Ackermann of zero comma y is just y plus one.
So the successor function Ackermann of x comma zero is
Ackermann of x minus one and one.
So you decrease the first argument by one and you
increase the second argument by one.
What happens when both arguments are positive?
You do a recursive a nested recursive call.
So first of all you compute Ackermann of x comma
y minus one and then you use the result of
that as the second argument to Ackermann of x minus
one.
, is it immediately obvious that this function is always
going to terminate and give a result if you write
it in Haskell?
So some people, it's immediately obvious.
To some people it's not quite immediately obvious.
, if it's not immediate, obvious, immediately obvious, and you
start testing it, you might get quite worried.
So if you do four of four, you start expanding
it.
And if you were to keep doing it by hand
without thinking, you would go on for a seriously long
time, as we'll see shortly.
But there is something you can observe about it.
Every time I do a recursive call, either x or
y get smaller.
So why is that?
Because when I do this recursion x gets smaller.
When I do this recursion.
X stays the same and y gets smaller.
So every time you expand the recursive call, something gets
smaller and the other thing stays the same.
So essentially you're doing a nested recursion on the x
and the y.
And when you do a recursive call, maybe x gets
smaller or maybe x stays the same and y gets
smaller smaller.
So that is effectively doing an inductive proof on omega
cross omega times omega the natural numbers times the natural
numbers.
If you draw out the x and the y and
see where it goes, it goes heads down to the
origin.
So you will ultimately hit Ackerman of zero comma y.
Nonetheless, if you actually do evaluate this function, and obviously
it's trivial to write this down in Haskell so you
can do it enthusiastic among you can do it whilst
I'm speaking.
, it grows quite fast.
We will come back to see how fast it grows.
So that was a little diversion to explain why ordinals
are useful, to show that things finish.
I now want to go back to building even more
ridiculously large numbers.
When you learn multiplication, you learned it by repeating additions.
When you learned exponentiation, which was probably some years later,
you probably learned it as repeated multiplication, right?
Because that's the definition of exponentiation, at least until you
have to start worrying about what e to the I
theta means.
So when we write x to the y, that means
y copies of x multiplied together.
Well, that is not too hard.
We can just continue thinking about omega squared omega times
omega.
We've seen yet again two to the omega.
Well, that's infinitely many copies of two multiplied together, which
is basically indistinguishable from infinity.
Omega Omega cubed, on the other hand, is omega times,
omega times, omega or omega copies of Omega Squared, and
we can kind of visualise it that.
, how far am I going to go?
Well, at some point my ability to draw diagrams runs
out and my ability to draw diagrams runs out about
there.
So this is an obviously inaccurate representation of Omega to
the Omega, but it looks something that.
And incidentally, these diagrams are generated not even in pure
LaTeX.
They're generated in pure text.
So no graphics packages here.
, but you can see and you can also see
how the kind of, well, handedness works, because if I'm
somewhere in here and I jump to the left or
down and left or down, ultimately I'm going to hit
the bottom left.
So what are we doing for time, as always?
Not enough.
Okay.
I'll take a quick pause.
Any quick questions at this point?
Yeah.
So.
We're basically being 100% right.
So we're coming to this system.
And this number can get to zero by subtracting, you
know, this one a finite number of times.
Which doesn't make sense.
Ah.
But that yes.
So that's a very good point.
And you're now kind of spoiling the next ten minutes.
But that's okay.
So the question is what happens if you do some
subtraction and you get Omega say how do you subtract
one from Omega.
There is no number immediately below omega.
So that's what we're about to see.
So so you should be able to solve this little
puzzle.
So this is a slightly strange concept, but it kind
of makes sense.
You know what it means to write numbers in base
seven.
Or at least you know what it means to write
numbers in base two, and you should know what it
means to write numbers in base seven.
This is a kind of expanded base notation, where we
think about writing a number as the sum of powers
of two.
So writing it in binary basically.
But then we look at the exponents of the powers
of two, and we write those in binary as well.
So we end up saying that a number 1030
can be written as two to the power of two,
to the power of two plus one plus two plus
two to the power, two plus two.
So basically we're not allowed to use any digit larger
than two.
And we have to write out the number as a
sum of powers of powers or powers.
If I were to do it with three then I
would end up writing something this.
So you can take my word for it that these
are correct.
I think I checked them.
That's what The number 1030 looks in this so-called
hereditary base three.
And then the puzzle goes this.
Think of a number, any number n write it in
hereditary base two.
Now wherever you see the digit two, replace it by
three.
Evaluate the result.
The number's going to get much bigger.
Subtract one from that number.
Okay.
Write it in hereditary base three.
Again.
Everywhere you see three, replace it by four.
Evaluate it.
It's going to get much bigger again.
Subtract one.
Keep on going until you hit zero.
If you hit zero.
, so the Goodstein function g of n is how
many steps of this process it takes to hit zero.
And the question is, is that a sensible number or
is it infinity?
.
Any bets?
Yeah.
Finite.
Anybody going for infinite?
Okay.
Right.
This is a deliberately deceptive way of writing it.
Okay, so now let's think about it.
But let's also, I mean, when you get a puzzle
this, it helps to try a few examples.
What is the value of the Goodstein function?
For example, how many steps does it take.
So is your question going to anticipate something or not?
I was going to say that when.
, each time you subtract one, you're eventually going to
remove a digit from this.
Yes.
So we'll see that in action now.
But let's think about what the numbers are.
So here is let's think about the good steam function
applied to the value three.
Now back sometime in the 80s or 90s, I forget
when there was some conservative politician who was cutting public
spending in ways that are familiar to us.
And they said, there is no magic money tree.
So here's a magic money tree.
Why is it magic?
It has wonderful little gold sovereigns as leaves.
And each day you can pluck one of the gold
sovereigns.
And then something happens to the tree.
So the tree has dead nodes and live nodes.
And what happens is each day, each live node grows
another leaf with a nice gold coin on the end.
And then you pluck a sovereign, and if you're sensible,
you pluck it from one of the dead nodes.
Okay, so we get that.
What happens on the next day the tree grows another
leaf there with a gold coin, another fruit, gold fruit.
And now you have to pluck from there.
But when you pluck from a live node, you kill
it and it can't go anymore.
So when you keep plucking your gold coins.
You're going to.
After six days, you'll go back to the tree and
find there's nothing more to pluck.
So in terms of this hereditary base B, which I'm
not sure is actually point, well, it's the point that
was just made here.
If I look at the expansion here, three is two
plus one.
So I replace two by three and write it out
again.
Well that's really just subtracting one.
So I get three plus one and then three and
then four.
And then four has to be replaced.
I have to subtract one from four which gives me
three.
And then I no longer have the magic base digit
in my number.
And now it just dies off.
So that's the reason why you can see that actually
this always finishes, but you might wonder how long it
takes to finish.
So starting with the number three here, the magic money
tree that started with three fruits on it took six
days to run out of money.
If I start with four.
What do you think happens?
How long do you think it takes to run out
of money?
Much, much longer.
Much longer.
How much is much longer, I wonder?
Well, let's have a quick look at it.
You'll notice that I've written live nodes in two sizes.
That's because these are super fertile live nodes.
They don't just expand the fruits, they do something more
complicated.
So when I take money off this one, I'm going
to pluck a live node which is going to kill
a node.
But on that day, not only did these nodes grow
new fruits and this node grew a new fruit, but
this node grew a whole new level of new fruits.
So the tree actually expanded to that.
That was the original root.
And just before it died, it managed to spout off
a new level of live nodes.
These are now ordinary life nodes.
And then what happens?
Well, I'm going to pluck a thing from up here,
but when I do that, everything else is ground going
to grow.
Lots and lots more.
So you can see it might go on for quite
a long time.
And my latex skills got a bit.
I got just got bored with later seeing these diagrams.
So then we come to the question of, well, we've
we've been led into thinking that g of for the
number of days it takes for that tree to die
is not probably infinity.
It's probably some large number.
And somebody said a large number.
Anybody want to guess what kind of large number it
might be?
A Google and who can remember what a googol is.
Yeah.
Ten to the 100.
Anybody going to go for a Googleplex which is ten
to the power of the Google.
Well let's see, Google is actually a pretty good guess.
, but I computed this number for you.
So.
That is not G of four.
That's the first few digits of g of four.
Now, I don't know how long you're willing to sit
here, but I imagine you're not willing for me to
skip through 135,000 slides.
And all of those slides were computed.
They were not typeset because LaTeX cannot typeset 135,000 slides.
And yes, I did try just to see whether it
could.
, and those are the last few digits.
, so no, actually ten to the Google was not
a good guess.
Why did I say, well, it depends on what you
mean by big small and what's a good guess.
So it's actually something around ten to the 121 million,
which is a lot bigger than a Google, but a
lot smaller than a Google Plex.
So somewhere in between, , that that's the exact value
of G of four.
, would you care to hazard a guess as to
what G of five is?
I mean, it's going to be a big number, right?
So I'm going to have to write some quite large
power of tens, tower of tens to talk about it.
, I can tell you how how high this Tower
of exponents is.
Unfortunately, in order to do so, I first have to
write down another stack of tens.
But I can tell you how high this tower of
tens is, provided you let me write down another stack
of powers of tens.
But I can tell you how high this one is.
Okay, so this is by sort of any normal understanding,
quite a big number, although mathematicians do work with much
larger numbers.
, and you can go on, you can try to
work out G of six.
It gets quite tedious trying to do.
This was all done with pen and paper after G
of four.
, why does the function always terminate?
Well, we've had the explanation during the questions and conversation
anyway, and essentially it's because this magic hereditary base B
why did I call it B it would actually be
quite nice to call it Omega.
, if I were to call it Omega, then four
is two to the power, two which I write as
omega to the omega, and then subtracting one from that
basically means expanding out omega for the current value of
the base as much as necessary, until I can actually
do some subtraction.
So here I've switched the base from 2 to 3,
and now I have to subtract one from omega to
the omega.
I can't do that.
So I say oh well Omega is three at the
moment.
So let's lets them make that omega cubed.
I can't subtract one from omega cubed, so I just
expand it to be omega squared plus omega squared plus
omega squared and so on and so on and so
on.
And eventually I end up with omega squared plus omega
squared plus omega plus omega plus two as the result
of subtracting one from omega cubed.
If I'm pretending that omega is three and now I
stop pretending that omega is three, and then it becomes
four, five, six.
And so whilst this number is getting bigger and bigger,
this fictional ordinal here is getting smaller and smaller.
And ultimately we hit the point where we've reduced to
this to Omega and then expands to the current value
of the base, and then you're just subtracting one for
the rest of it.
So you spend half the time going up and half
the time going down.
Roughly speaking.
Exactly.
Speaking this side note is something that only occurred to
me last year and it still boggles me.
So think about that.
So the Goodstein function is something that clearly grows quite
fast.
Year four is huge.
Year five is incomprehensible.
The Ackermann function grows quite fast as well.
So this is what the Ackermann function looks as
a table of x and y.
, as the first argument gets bigger, the numbers get
bigger.
Similarly with y.
Once x hits four, then it starts starts getting big
quite quickly as you.
When x is four, basically you get a new level
of power of two every time you increase y.
, so which is growing faster?
Ackermann or Goodstein?
So Ackermann is a two argument function.
So let's turn it into a one argument function by
taking the diagonal that.
Okay, so here's a table of the first few values
of these competing functions.
And you can see that although Goodstein is going pretty
fast, Ackermann is a long way ahead of it at
four.
, now to think about what happens any further, I
have to go back to building big numbers again.
Exponentiation is iterated multiplication.
Multiplication is iterated addition.
We can iterate exponentiation again.
We did these tiles of ten.
Why don't we have a notation for talking about towers
of 10 or 2?
Because we're computer scientists.
, it's called titration because it's the fourth level of
exponential iterating stuff, sometimes written that, sometimes written
that, and lots of other notations.
It's not useful enough to have a completely standard notation.
And now I'm going to introduce a handy little bit
of terminology which will let me avoid writing.
Trying to write down extremely big numbers.
Let's say that a number is small if it's small
in some humanly comprehensible sense, so I don't really care
what you mean by small.
Maybe you mean five, maybe you mean a few hundred.
Maybe you have a big brain and you mean a
million.
It doesn't matter.
And I'm going to call it big at level 1
or 1 big if it's two to the power of
something small.
Okay.
And I'm going to call it two big if it
is two titrated to the power of something small.
So that means express.
It's expressible as a stack of powers of two.
But the height of the stack is small for whatever
your favourite value of small is.
And so on and so on.
So three big is something that is two penetrated to
the towers small.
So you have a small number of iterated stacks of
I was doing for G of five.
So if we extend the comparison table, it now looks
this G of four was a two big function.
It was a power of two's that was small in
height.
G of five was a three big thing.
We had to have an extended expanded row of brackets
describing towers of two, and it turns out the same
happens with Ackermann at level five.
When X is five, you also get something that is
roughly three big.
, but this is where Goodstein starts pulling ahead.
So you are right.
So at six it pulls ahead.
We now have four big numbers.
But Goodstein has gone to level five of bigness seven
five big and seven big.
and this is where things get a little bit
absurd, because at level eight, the good steam function is
now G of four big.
, which is kind of silly.
What does that look ?
Sorry.
I mean, one can't write it down, right?
You can only write this in mathematical notation.
, I mean, I'll well, actually, I'll.
And you can keep doing this forever.
So again, some more informal terminology.
I can call a number huge at level one if
it's big at level one big and I can call
it two huge if it's big at level one huge
and I can so on and I can call it
one humongous if it's one huge huge.
Now these of course are not real definitions, but they
are in fact just an informal version of very real
definitions in large cardinal theory, which is something about genuine,
seriously big infinities to which all these things are tiny
and you just keep going.
And when this happens, depends on whether you're a set
theorist or not.
If you are a set theorist, happens at the Koonin
inconsistency when your universe explodes.
, I said do it to infinity.
But of course we're actually doing it beyond infinity.
, we can keep doing this exponentiation trick so I
can go Omega omega to the omega omega to the
omega to the omega.
Then I can define this thing called, which we call
epsilon zero, which is omega titrated to the omega.
So it's an omega high stack of powers of omega.
It's actually a useful number.
, it's an interesting fact that if you take this
thing and do omega to the epsilon nought, then you
get the same thing again, just as if we took
omega of the Omega and multiplied it by Omega, it
was still Omega to the Omega.
So what that means is that epsilon zero is what
we call the fixed point of the function.
The omega exponential exponentiation function.
And this is this is the recipe for going on
forever.
So now you can say well if there's one fixed
point there's probably a second fixed point.
And we can actually compute that by adding one to
the first and then exponentiating it forever and ever.
You can keep on going.
Then you can talk about epsilon sub epsilon sub epsilon,
which is the first fixed point of the function that
counts the epsilon numbers.
And you just well keep going.
So you're now counting functions and then you're computing fixed
points of functions and counting the fixed points and counting
the fixed points of the functions that count them, and
so on.
This gives you a thing called the Veblen hierarchy.
And the n process of that is some enormous ordinal
which is actually used inside in proof theory.
It's called gamma nought.
After that, it starts getting complicated and it is way
beyond my level of understanding.
So my understanding stops here and I don't have that
understanding all the time.
Only when I revise it.
, is this all just a game for mathematicians and
other children?
Does it have any use?
Well, actually it does.
So the working theoretical computer scientists.
By which I mean people me, we use ordinals
to do inductions to prove that some function terminates,
the Ackermann function or some function that we actually write
down.
I have had to prove that a function terminates.
Generally speaking, we don't need big ordinals to do this
up to omega.
To the omega is as far as one tends to
go in the kind of functions that we write in
everyday life.
But it turns out that proof theorists and proof theorists
are people who work on things the sequent calculus
and the basic theory of that, and a lot of
them are in computer science departments, because there's more money
in computer science departments than there is in math departments.
And computer scientists have a more favourable view of logic
than other mathematicians do.
, they need much bigger ordinals, because it turns out
that in a remarkably beautiful way that if you have
a proof, a system of proof, you want to know
how much can I prove with it?
The perfect proof system would be able to prove every
true mathematical theorem.
We know that's impossible, but you can take your favourite
theory and ask how much can it prove?
And it turns out that these ordinals give you a
very precise and nice measure of how much a theory
can prove.
, so if you've ever come across a thing called
piano arithmetic, it's not actually strong enough to prove that
the Goodstein function always terminates.
Why do we care about proof theory?
Well, we do formal verification of systems.
We do that by building computer tools that work in
a proof theory, and do the proofs for us to
make sure that they're correct.
And it's quite nice to know what the strength of
the system we're working in is.
And it's also quite nice to know how long it
might take to prove something.
And sometimes the fact that a proof has a large
ordinal associated with it also means that the proofs can
get very long, in the same way that the Goodstein
function can get very big.
So if you're unlucky, there are things that if you
try to prove it, you might end up with a
proof.
In the sequent calculus that is expanding the Goodstein
function expands.
And that would be bad news because we don't have
computers that big.
, okay, so for the first time in recorded history,
I've finished in an hour exactly from when I started.
So as a final comment, these things are all very
small.
These are all countable infinities.
They're all things that you can count natural numbers, whereas
if you do it in a funny order.
So the actual Cantor in Revolution was about the real
numbers, and I haven't gone there at all.
That's for another talk given by somebody else in some
other place.
So thank you for coming to the course.
I've had fun for the last 11 weeks, I hope.
Well, the fact that you're here means you probably have.
I hope the people who aren't here have as well.
, enjoy your lack of exams for this course.
Good luck in the exams for your other course, and
I will see you around over the next 3 to
4 years.
Somewhere, sometime.
After you are.