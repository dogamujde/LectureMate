Okay, let's get started.
So if you haven't already, please scan the QR code.
We'll again have some interactive.
Exercises on Wall Club.
So today we're going to talk about language again.
So building on the the Friday where we talked about
rules and words.
Now we're going to talk about what this means in
cognitive terms.
And in particular I'd to talk about language acquisition.
So language acquisition is the process.
Maybe this isn't loud enough.
Language acquisition is the process of acquiring a language.
Learning a language in particular in children.
So cognitive scientists, when they talk about language acquisition, they're
mostly interested in child language acquisition.
I mean, a few people also study language acquisition in
adults, but it's generally thought to be quite a different
process from first language acquisition.
So here we'll only talk about first language acquisition.
Okay.
This is really interesting.
So this is the game plan.
We're going to talk about the past tense.
Because the past tense has been used as a as
a case study, as sort of a and an exemplar
that people have used to study language acquisition.
And we'll see why in a moment.
And then we'll look at what kind of evidence we
normally rely on.
And then we'll try to sort of zoom out a
little bit and talk about knowledge acquisition more generally, and
different views of how that happens in cognitive science in
particular will come back to a neatness.
If you remember, lecture two where we talk about talked
about cultural evolution and innate knowledge versus learned knowledge.
So this sort of debate also looms large in terms
of language acquisition or knowledge acquisition more generally.
Then there's a preview, just two slides talking about neural
networks, which will start in earnest on Wednesday.
Sorry.
On Thursday and on Friday.
Okay.
Just to recap basically what we said on Friday.
, so we have argued that human language involves two
types of mental tissue, two types of mechanisms and two
types of knowledge.
, there's a lexicon of words.
These are stored.
They're retrieved from memory.
, it's a finite list.
, precomputed, readily available, presumably also very fast to access.
And if you've attended the tutorials, , will we have
seen quite a lot of examples there of stuff that
is precomputed in other languages as well.
Not just in English.
If not, well, go to one of the tutorials this
week to find out.
And in addition to that, there's a set of rules
of computational procedures that act upon those stored lexical items,
for example, to to construct sentences, but also to construct
words and more complicated words from simpler words, for example,
by affectation.
So you , it would be a fixation process,
or the compounding train and conductor becomes train conductor and
so on.
So the rules can act upon words as well as
on phrases and sentences.
We've talked about this notation here, which is called context
for grammar notation, where the words are written as lexical
items this determiner goes to the where there is
is an article or a determiner, and n goes to
dog.
So this is the part of speech for dog.
And then we can have rules that construct phrases.
So a noun phrase is made up of a determinant
a noun a verb phrase is made up of a
verb and a noun phrase.
And then if we have a sufficiently large quantity of
of words and rules, then we can use that to
parse sentences, to turn them into syntax trees, to turn
them into structures.
These structures are useful a for computing whether a sentence
is part of the language is grammatical or not.
And b they can also be used in deriving the
meaning for the sentence.
And again the tutorial talks about that a bit more.
So at this point this is just a hypothesis, right?
This is something that linguists have dreamed up.
And how do we know whether speakers actually do this?
They actually store lexical items in the in their brains,
and they actually apply rules On those lexical items to
compute sentences.
How would we even test this?
There's various ways of getting evidence.
People have looked at large amounts of speech or text,
so-called corpora of data, and try to find patterns, patterns
that repeat.
And, , it seemed to be rule governed, seemed to
be, , evidence for certain structures reoccurring all the time.
, certain things are possible, certain things are impossible.
And if you just look at enough data and use,
, statistical algorithms on those data, then you can, ,
you can try to test whether there's rules, whether there's,
, whether there's regularities of the type that linguists have
assumed.
And, of course, you know, you've all played with large
language models.
These large language models that are trained on a lot
of data.
Much more data than any human can consume in a
lifetime.
So billions, trillions of words.
And they don't use any rules, right?
They don't even have lexical items in the same way
that we have postulated here.
So that's already a little bit of evidence that maybe
a grammar is not necessary.
It doesn't mean that humans do it in the same
way.
It just means it's possible for a sufficiently large model
and sufficiently complicated learning algorithm to acquire behaviour that looks
as if it's governed by rules.
And this is something we'll come back to.
Another thing people have done, they've studied human language processing
in real time, because this rule and word distinction makes
claims about processing.
It would, for example, entail that things that are stored
in the lexicon, they're really quick to access.
Right.
You just have to retrieve them then then they're ready.
Whereas if you apply those rules, then maybe this takes
more time, maybe more complicated rules.
Write rules that have low frequency items that have a
lot of elements on the right hand side.
Maybe these take longer if you apply a lot of
rules, maybe that takes takes more time.
So a phrase that is made up of lots of
different rules would take more time than a phrase that's
made up of just a single rule and so on.
So this is something you can you can test.
You can, for example, do brain imaging.
You can do reaction time studies.
You can do eye tracking studies.
These are all, , ways of getting evidence for the
real time process that happens while we are listening or
reading or while we're speaking or writing.
And another way of looking at this is to look
at the sort of errors that people make.
And these are often systematic.
Certain errors occur frequently.
Others occur not at all.
And then looking at the patterns of errors.
Maybe also for corpora, right?
Text often doesn't contain a lot of errors.
But speech, you know, as I speak now, I make
errors.
I use the wrong word, I don't finish my sentences
and so on.
Maybe this is also interesting in terms of evidence.
And then a fourth type of evidence is from language
acquisition to see what happens during language acquisition.
People obviously don't start speaking from the day they're born.
They start speaking sort of between 12 and 15 months.
And they don't they're not fluent speakers of the language
from from the start.
It actually takes quite a lot of time.
And so it's interesting to look at this process again.
Look at the errors.
Look at what kind of utterances they are able to
produce, how they become more complicated, how they mirror the
language that they hear from adults and so on.
So the acquisition hopefully also tells us about how language
is actually used in cognitive terms.
And here we'll look at in particular past tense because
that is an interesting example where some things are memorised
and some things are rule based.
And it's a bit a fruit fly, right?
If you're a geneticist and you always study fruit flies
as a model organism.
So we look at the past tense as a type
of model organism.
Okay.
So some quick facts about the past tense.
So well there's a very simple rule.
Just add ed to the present tense or to the
to the base form of a verb.
And then you get the past tense liked.
And at least for regular verbs, there isn't really a
finite set.
You can always make new verbs in English.
It's very easy to turn a noun into a verb
, and so new concepts arise all the time.
So new words are coined.
And these can be verbs as well as nouns.
So here are some simple examples.
Jog becomes jog to walk becomes walked.
Play becomes played.
Kids becomes kissed.
But also these relatively new verbs spam, mosh, which
is a type of, you know, dancing Pluto.
You can say this planet was Pluto, which means it
was relegated to a dwarf planet.
And there's actually quite a few dwarf planets, not just
Pluto.
Grok is sort of a I don't know.
For me, this is a Gen Z word of, and
it means understanding something.
, and of course, the same rule applies even though
these are new words spammed, mosh, Pluto dropped, and so
on.
Okay, so far, so good.
However, the story is slightly more complicated.
If we look at verbs this buy, hold still,
go.
So buy it, hold it still, and so on are
not grammatical in English.
So this asterisk means it's not grammatical.
And instead we have these forms here.
Bought.
Held.
Stole.
Went.
, these are so familiar and frequent that we don't
even notice, right?
Irregular verbs this.
If you learn the language, you somehow need to pick
up these irregular verbs.
And you need to somehow learn.
And you don't say buy it.
You need to say bought instead.
And then, of course, there is sort of islands of
regularity, even in the irregular verbs.
So it's sing sang ring rang.
So these obviously are similar in even though they're both
irregular, but it's not clean clang or think tank.
Right.
So there's, , irregular verbs and some of them have
a common pattern.
But then there's exceptions to those common patterns as well.
So it's reasonably interesting what's going on here.
And how could the child possibly master that?
, right.
Yeah.
But just to summarise what I've said.
So some past tense forms are not just Ed, but
they are still somewhat regular, singsong ring rung.
, historically, actually this was a different type of, ,
past tense, which no longer exists except in these irregular
verbs.
, so it is a bit chaotic and idiosyncratic and,
but it's a small number of irregular verbs, 150 to
180 and, , hasn't been really augmented recently, so sneak
snuck seems to be the latest addition in the 19th
century.
, so let's actually have a short quiz so that
you can try this out yourself.
So if you haven't, , scan the QR code, I'll
give you 30s to do it now.
You.
Can do.
That.
We're going to do that.
Okay.
Okay.
Let's maybe move on.
So if you could please guess the past tense forms
of these verbs.
And of course, I've, I've taken some verbs that are
a bit more complicated and new or infrequent and so
on.
So just, , try to type the past tense forms
for those.
There's just a lot.
Of support.
So as you can see, they come in two categories.
The first.
The first part are weird.
Irregular verbs in the second part are low, frequent low
frequency verbs that you might not have seen.
They all occur in the dictionary.
So I did check that I didn't just make them
up.
Some.
It's.
Okay, let's leave it there.
See what you found.
So we can see the most frequent answers here.
They're still changing a little bit.
So bit.
Most people think bid is, .
Is the past tense in.
Yesterday I bid on something on eBay.
And that's correct.
But, .
Bid bid it.
I don't think is possible, but bad is also possible.
, I bathe her.
Goodbye.
It's slightly archaic.
So that already makes it more tricky if you have
multiple past tense forms this.
Closed.
Closed.
So that works.
But clad is also a form only a few people
have guessed this.
So typically sort of in fixed expressions ironclad, but
it's an old past tense form.
And then for sake.
So forsaken would actually be the past participle, not the
past tense.
So for succ is the.
Is the correct the correct past tense form?
Forsaken in I have forsaken him, right?
So that's the past participle.
It's not the past tense.
And here, apparently slunk is correct.
I actually need to check myself.
Slang is correct.
And the other ones are just regular.
Simp is apparently giving someone unwanted attention.
Nerf I think is attacking someone with a Nerf gun,
but I'm not sure.
And Josh is making something more interesting and more exciting.
So, , he's juiced up his wardrobe.
Something that.
And there you have.
Maybe not seen this before, but you can tell they're
probably regular.
Whereas for slunk, something tells you that this is a
bit sink sunk, right?
Even if you haven't seen it before.
So it's irregular.
So and this just sort of reinforces the point.
How do children figure it out?
Well, from the rules and word's point of view this
is really straightforward.
You can have a very simple rule, right.
You can have a verb in a base form.
You add ed and then you get the verb in
the past tense form.
Or if you write it as a rule then this
would just be v.
Past goes to V, let's say base form and Eddie.
Okay, so super simple rule, which of course doesn't account
for the, , for the irregular ones.
So you could assume that the irregular ones.
So this is for, , for the regular ones, you
have this rule, and for the irregular ones, you just
stick it in the lexicon, hold in the lexicon as
the base form held in the lexicon, in the past
form, the past tense form, and together with all the
other stuff that sits in the lexicon.
What's the sound, what's the spelling, the meaning of, of
the verb and so on.
So.
That's really easy, right?
Or is there a potential problem in this story?
You just assume more lexical entries to cover the irregular
verbs, the regular ones.
You can just write a rule what's what can go
wrong?
So the problem is that nothing prevents me from applying
this rule also to the irregular ones, right?
So I can apply it to this lexical entry and
get hold it as well.
As well as health which is already in the lexicon.
So irregular ones are stored as words.
A rule generates the regular past tenses.
And then how do I avoid hold it and steal
and so on.
And the answer that linguists normally give.
Well, if a form already exists in the lexicon, then
the rule is blocked.
So somehow you need to assume a mechanism that before
you apply this rule here you check the lexicon whether
the output already exists.
So whether there is already a past tense form for
hold, for example.
And then you don't apply this.
So you have some mechanism that prevents you from applying,
, applying this rule.
And , well, this works as a mechanism, but it's
not really an explanation, right?
So it's actually a bit ugly.
And it's also something that you don't, you know, it's
outside the grammar and the lexicon.
It's an extra thing that you need to assume.
You see in in other places too.
So if you think about the process that turns an
animal into food.
So for example, you can say, I ate chicken tonight,
I will eat chicken tonight, but you can't say I
will eat cow tonight.
You have to say I'll eat beef tonight.
Okay.
So there's this process that turns an animal into an
animal word into a food word.
But it's blocked for things beef and pork and
maybe game as well.
So the blocking happens in other parts of the lexicon
as well.
But it's it's an unexplained mechanism basically that we have
to assume.
And this already tells you, well, maybe there's something not
so great about the words and rules account if it
needs to rely on stuff blocking as well.
Okay, but let's look at what actually happens during language
acquisition.
So you can obviously study what children do at a
certain age.
So these are examples from preschoolers.
And you can look at the verb form.
So they say things it was neat.
You should have seen it instead of seen.
So this sounds an irregular verb but it's not
the right one.
Doggy bad to me.
Rather than beat me again, this is an irregular one,
but somehow the child has picked up the wrong form,
or maybe generated the wrong form using some sort of
rule that the Cheerios got eaten by Maki instead of
eaten.
I know how to do that.
I truck myself well, truck is an irregular version of
trick.
Trick truck stick stuck.
.
This is the best place I ever thought.
Rather than sad.
Okay, so children actually do make mistakes that.
And, , quite frequently at least, , in the early
years.
So, , these don't suddenly stop when children start school.
, the, .
It it cannot all be education.
I mean, some of it is that children get corrected
at school, perhaps, but they persist making these these errors
for quite a while.
And, , it could be that certain, you know, ,
very infrequent past tense forms the children may never encounter.
So how are they going to learn those?
And maybe sometimes they just create them by analogy.
So this seems to be what is happening here.
truck instead of tricked.
Right.
, stick.
Stuck.
Trick truck makes sense and is a bit the
ones we've seen here, right where the sing song ring
rang.
Clang clang might be an overgeneralisation.
So we see these patterns, and it's also interesting in
the sense that the types of error they change with
time.
And they're not just confined to plurals, so they overuse
other processes plural formation, man man's foot foods to
tooth, mouth, mouses, fish, fishes.
My children said this all the time.
Fishes overuse the third person singular dos bees.
Normally you just stick an s at the end of
the base form if you want to have the third
person, but obviously have and do and b are irregular
comparatives.
Imperatives.
Special special.
Powerful powerful list.
So there's a lot of overgeneralisation and and morphological errors
that going on numerals one tooth instead of first
and second.
They're quite creative.
Here's an example.
So parent says no booze in the house.
And the child says what's the booze?
So they think booze is a plural.
.
It did.
It's new instead of it's node.
So again, one of these fake irregulars that can happen
as as errors in child language.
Okay, so if you observe children over a longer period
of time than you actually load a particular pattern, which
is sometimes called the U-shaped curve in development.
So in the first stage, children produce both irregular and
irregular forms.
They don't make a lot of errors.
Also, they don't know a lot of verbs yet at
that point.
Right.
So they're still very young, I don't know, 15 months,
18 months, something that.
, then later on, , they produce a lot of
regular verbs and also try to regularise the irregular ones.
So you would hear things hold it and bite
and so on.
So suddenly you have a lot of errors and they're
mostly overregulate that.
And then later on the errors reduce again, and the
children have mastered both the regular and the irregular form,
and the errors go down and close to zero.
So if you if you draw this here, you have
time on, on the x axis and proportion.
Correct.
Here on the y axis.
Then initially there's a lot of correct forms.
Then they go down a lot of errors.
And then there's again a lot of correct forms.
And this is actually a general pattern that you will
see in other domains as well.
It's not just for past tense, it's not just for
language.
There's this U-shaped curve.
And let's try to figure out why we see this
U-shaped curve.
What is going on.
So why do the errors first increase and then decrease
again?
And where is my wall club.
Okay.
So let's assume there's a few stages that happen during
language acquisition.
And that corresponds to the different elements of the U-shaped
curve.
So we have our curve.
And then we have four stages.
What do these stages correspond to?
So we're basically assuming that different mechanisms are engaged during
those four stages.
So irregular past tense forms are memorised.
Past tense forms are generated using rules.
All past tense forms are memorised.
Regular past tense forms are generated using rules.
So which, , which order would you have to assume
so that you get this pattern?
Arrow first goes down and then first goes up and
then goes down again.
Okay, I'll give you another 30s.
Okay.
Let's see.
So the most frequent answers answer is regular past tense
forms are generated using rules.
Then all past tense forms are generated using rules.
And then irregular past tense forms are memorised.
And then all past tense forms are memorised.
So that doesn't sound right.
Right.
Because you would have to assume that.
let's see.
So regular past tense forms so.
We.
Don't get errors and then you get more errors.
Irregular forms and memories.
Yeah.
And this would actually produce the patterns.
But of course the problem is why do you abandon
the rules.
Right.
That doesn't seem a very plausible assumption that you
start with rules and then you move from there to
memorising everything.
So presumably it's the other way round.
So the the correct answer is this.
Right.
Hold on.
This is the correct answer.
So all past tense forms are memorised whether they're regular
or irregular.
I can of course also memorise the irregular ones.
Right.
walked liked.
.
But it doesn't matter whether regular or not.
I just put everything in the lexicon and then I
have very high correctness, right?
So here's correctness.
So I mean this in this stage.
But then the child suddenly realises, well, there's really many
more verbs than I can care to memorise.
And there is actually a regularity as well.
I can just tick ed on things.
So let's forget all the memorised forms, and instead just
use this rule and generate all the forms regular or
irregular.
And that explains the overgeneralisation, right?
The the application that gives you bite and held it
and hold it and so on.
But of course, the error rate, the correctness goes down.
So I'm down here.
And.
Then the opposite.
Potentially the child tries the opposite.
It tries to memorise all the forms, but then it
acquires this blocking principle and realises that some past tense
forms are acquired, are generated using rules and some are
memorised.
And then that's a possibility.
That's a way of getting everything correct.
All right.
You can generate all the regular verbs and even the
ones you haven't seen before, but the irregular ones you
need to memorise.
And for that, you also need to apply the blocking
principle.
So it's basically memorising everything, acquiring the rule and over
generalising, then realising that the rule doesn't apply to all
the all the verbs.
And then you increase the correctness again.
So and this is nice because it makes use of
this distinction between rules and lexical items.
Right.
Between things that are stored.
And you can just retrieve as a, as a chunk
and things that are generated on the fly using a
rule.
So more generally this is known as a knowledge acquisition
problem.
Right.
Where does the knowledge come from.
We're certainly we're born with some things, but we're not
born knowing everything we need to know.
Otherwise, you know, we didn't really have to go to
school or university or anything.
If we knew everything we ever needed to know.
But then, of course, you can ask the the opposite
question.
Do we know anything at birth?
Or is everything just learned through exposure?
So is the mind as as we're born completely blank,
or is there some rudimentary understanding of the world?
And this is the old debate about the maintenance that
we already talked about last time.
So which parts of our knowledge, which mechanisms and which
representations are innate?
What's the contribution of biology to our cognitive abilities as
opposed to experience.
Experience things that we've seen and learned and that are
determined through communication, cultural interaction, and so on.
If it's innate, then people normally assume it's genetically determined.
It's ultimately in the DNA.
If it's experienced, then it's not in the DNA.
It's something that we need to learn.
Maybe the learning mechanism is innate.
That would be a compromise.
And this corresponds to the distinction between biological evolution and
cultural transmission.
If you remember this idea of cognitive gadgets, things
reading is something that enhances our cognitive function but it's
not something that is genetic.
So for reading, we know because it's too young to
be encoded in our DNA, because biological evolution is a
very slow process, so it must be culturally transmitted.
But here, these regular irregular verbs.
Is that something that's innate?
Maybe the blocking mechanism is innate.
Maybe the ability to infer rules from data.
Maybe that's innate.
And we will see more about that starting tomorrow.
Sorry.
Starting Thursday.
This debate you can also.
So if you're maybe there's some philosophers in the room,
you can link this back to an old debate in
philosophy between rationalism and empiricism.
So philosophers Descartes, Spinoza or Leibniz who assumed essentially
that our cognitive mechanisms are innate.
There is things causality.
Things time.
There's categories that we're born with and we don't have
to learn.
That basically make it possible to even experience things or
learn things.
And then there's another bunch of philosophers, Locke, Berkeley, Hume,
who basically thought we were born as a blank slate.
We we just learn everything from scratch, from experience.
And this distinction between rationalism and empiricism mirrors the distinction
in modern times between innate knowledge and learned knowledge.
, yeah.
So rationalism is often also associated with a certain mechanism,
with a mechanism based on rules and symbols and logic
and so on.
And of course, our grammar rules are an example there.
In modern times, people Noam Chomsky advocated this, sometimes
called nativism as well because it assumes innate structures.
Knowledge is logical.
It's based on calculations, and empiricism is about connecting together
what we've experienced and building experience from build building knowledge
from experience bottom up.
In psychology, people Pavlov and Skinner are associated with
this.
Or neural networks in some sense are a very radical
form of empiricism because the network really doesn't have anything
right.
You just have a very powerful learning learning algorithm.
So you start with a blank slate and you generalise
from observations.
And the truth, as often, is probably somewhere in the
middle.
So we have clear evidence that some things are innate.
So here are the pictures of babies.
And the babies have quite a lot of reflexes when
they're born.
So immediately when they're born, they don't have any time
to experience anything.
And they have, for example, the grasping reflex, sucking reflex
step reflex where they you put them in a certain
position, they can't walk yet nothing.
But they have the reflex to take a step sucking
reflex obviously needed for feeding and so on.
So there's definitely some things that are innate, and that's
maybe not surprising because animals have that too.
They have innate reflexes, and some animals only have these
reflexes.
They're not able to learn much.
I don't know if you are a tapeworm or something
that.
So they come into the world with a variety of
skills.
These are often domain specific, so they can only do
one very specialised thing.
And even if you were an empiricist and you think
everything is acquired by learning from from experience, then still
you need to assume that the learning mechanism is is
innate, and you need to explain why certain things are
learned by children from very early on, for example, language,
but others are not.
For example, calculus.
Right?
Why do we not have a learning mechanism that allows
us to learn calculus at the age of 18 months?
Okay, so.
There is probably both elements innate innate mechanisms and innate
representations and ones that are gained by experience.
Okay.
So let's think about this distinction a little bit more.
And there's another quiz.
And.
Here are some candidates for cognitive abilities that may be
innate.
So you get a vote.
Which ones do you think are innate.
Reading and writing.
estimating and comparing quantities, social norms and etiquette, and so
on.
So take a few minutes and think about whether this
could possibly be innate or not.
But I.
Still want to start with.
You know.
What I mean?
Yeah.
I mean.
I didn't know.
I.
Was going to be.
Honest.
Okay, a few more seconds.
Okay.
What do people think?
Reading and writing.
Yeah.
So I've already talked about that reading.
Writing very unlikely to be innate given that it is
so recent.
In evolutionary terms, it's a cultural skill, which is amazing
because reading is almost as automatic as many other cognitive
functions.
Right?
If if I know how to read, then it's something
that is extremely fast, is extremely reliable.
It's it works unconsciously in the sense that if I
see a sign in a language that I speak, I
cannot not read it right.
I cannot consciously avoid reading it.
And, .
It's also extremely fast, faster than listening and speaking, actually.
So the reading rate of the average adult is about
200 words per second.
Speaking is less is about 120 words per second.
So it's pretty amazing since something that isn't innate
given how, , clever and efficient it is.
Estimating and comparing quantities.
Yes, there's a pretty good evidence that this is innate,
, because of the developmental trajectory.
This is similar to language in that children make a
very predictable pattern of errors.
And we'll actually talk about that later on in the
course.
We'll talk about also that quantity seems to be something
that is cross-culturally Truly universal.
Even in cultures that don't have the same number system,
that you don't even have words for numbers.
Counting and comparing quantities is still acquired in the same
way.
Social norms and etiquettes, basically, by definition, almost are not
innate.
It may be that there's a certain universality to what
is a possible social norm, but the individual norms are
probably not innate spatial navigation.
Again, very good evidence that it's innate also because animals
are very good at it.
And that is often an indication that, yes, perhaps it's
it's innate if it doesn't require, you know, because animals
don't normally take 18 years to acquire all the knowledge
they need, and then another three years or four years
at university.
Right.
Animals don't have the time.
So if they can do it, then it's more likely
to be innate scientific reasoning.
Maybe certain concepts time and causality.
Those could be innate, but by and large it shouldn't
be innate.
Playing an instrument again is evolutionarily probably too young to
be innate.
Face recognition is a very good example.
And yeah, you're right in saying that this is probably
innate in the sense that children react even at birth,
react to faces differently than to other visual stimuli.
And obviously they can't recognise individual faces, but they can
recognise that something is a face.
And so it seems they're born with this ability
to distinguish faces from non faces.
And the same with emotion recognition and even in animals,
primates and so on.
They are good at recognising emotions.
Okay.
So this is really a preview for Thursday's lecture.
So this is an approach that emerged in the 1980s.
, and yes, so some people think deep learning is
sort of, you know, started with Jeff Hinton in 2015
or something that.
But it's actually a lot older.
The technology was invented in the 1980s.
Some aspects even earlier than that.
And it started with a book called Parallel Distributed Processing
and actually two volumes.
The first one contains models, the second one contains.
Well, the first one contains models.
The second one contains architectures, particular architectures recurrent neural
networks or feedforward neural networks.
And in particular contains the the training algorithm back propagation
that is used to this day for training neural networks,
even modern ones transformers that weren't invented yet in
the 80s.
And we'll actually talk about back propagation on Friday.
, it was called parallel distributed processing, which is not
really a name that people use at this point, parallel
because it contains, well, the networks, they contain lots of
simple independent computations and distributed because the information is distributed
across the network.
It's not represented in an easily accessible way, but it
uses these so-called distributed representations.
And that's still the case for today's networks.
And we'll actually see examples later on in the course
when we talk about semantic embeddings, which are examples of
these distributed representations.
And so he isn't on on this on the title,
but , Geoff Hinton, who I already mentioned and who
obviously just got the Nobel Prize, , he was one
of the authors of the PDP books.
Right?
He's is actually the author of several articles.
The books contain a bunch of different articles for different
models, and he's the author of several of those.
Incidentally, he did his PhD in Edinburgh.
So no bragging.
But, , anyway, so why do we care?
We care because if you believe in this distinction between
radical, between empiricism and rationalism, then neural networks or PDP
or deep learning or whatever you want to call it,
is an incarnation of empiricism in the most radical possible
way, in that you assume a very simple architecture, and
we'll get to know the architecture on Thursday and Friday.
We start with a very simple one called a perceptron.
Then we make it slightly more complicated, called a feed
forward network, which is basically an assembly of assembly of
perceptrons.
So you have this very general architecture and you have
a very general learning algorithm, backpropagation or gradient descent more
generally.
And this is it.
Basically this is the technology that can do all these
amazing things, from image recognition to controlling a robot to
ChatGPT and so on.
And you need that and you need a lot of
data.
Well, and a lot of compute ultimately.
But it is basically assuming a blank slate where you
only have this powerful architecture and the powerful learning algorithm.
You don't need to have anything language specific, anything specific
to vision or motor control for all the domains, you
can use the same architecture.
And that is pretty amazing.
In the 1980s, people couldn't really exploit it because we
didn't have the the compute and we didn't have the
data and we didn't have the training algorithms.
But then in the 2000 People had all these things
and then, well, the rest is history.
Okay, so to summarise, we looked at the past tense
as a simple way of analysing and modelling language acquisition,
sort of a fruit fly, a model organism, a very
simple mechanism that we can study and we'll actually come
back to next week.
, we looked at children's acquisition of the past tense
and of irregular things in the language in general, and
we saw these stages that give you a U-shaped curve
where initially they make very few errors, then they make
a lot of errors, and then they stop making errors
again.
And we explained this by saying they memorise everything first,
then they discover the rule and then apply the rule
to everything.
And then they recognise that the rule needs to apply
to certain things, but not to others.
Some things need to be memorised, some things need to
be rule based.
And that is the official story of the words, word
and rules framework.
But starting next week, we look at the alternative story
of the neural network literature, where you don't have to
assume any rules, you don't have to assume any, ,
any lexicon, but you can still get this behaviour.
, and more generally, we talked about things being innate
or things being learned.
I still have five minutes.
I think, , things being innate or things being learned.
And we situated this in the more general philosophical frameworks
of empiricism and rationalism.
And, , I'm very happy to take questions.
As I said, there's five minutes left in the in
the lecture.
Any questions?
People are keen to leave.
Well, see you the day after tomorrow, then.
Thank you.