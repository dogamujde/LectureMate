You know, everyone is just more comfortable with it.
Okay.
Welcome to lecture 14.
, today is again about biases in decision making.
You probably remember that we talked about biases yesterday.
And in particular we looked at what's called the framing
effect.
And we looked at one particular way of explaining it
which was prospect theory.
So today we're not going to talk about prospect theory,
but we're going to talk about other biases in decision
making and how we could possibly explain those.
, let's get started just with a quick recap.
, so last time we saw rational rational analysis, which
is not a theory, but is a framework for cognitive
modelling, where the assumption is that you think about what
the optimal the rational behaviour would be in a certain
situation to solve a certain task.
What kind of data would you need?
What kind of environment would you operate in?
How would you test your evidence?
How do you make a decision, and so on.
And you try to formulate this in an optimal way.
And then the next step is to look at data
from human participants and see to what extent that deviates
from the optimal, the rational way.
And that's been a successful approach in the field for
the last 20 or so years.
We looked at one example, which was decisions under uncertainty,
where you have a bet and you have a certain
chance of winning or losing.
And how much attention do you pay to probabilities?
Do you compute expected values?
It turns out that straight expected values as we know
them from probability theory, are not able to explain the
human behaviour in this situation.
And we've seen another approach called prospect theory, which is
based on expected value but also makes additional assumptions.
Today we are going to look at other cognitive biases,
not just the framing effect.
, cognitive bias, just to remind you, is a is
a deviation from the optimal normatively correct behaviour from the
rules of logic or the rules of probability?
, if humans behave differently, then we're talking about a
bias, a cognitive bias.
This was different from the inductive biases we saw earlier
in the course, for example in language acquisition.
Okay.
So for example, if you choose a bet, a bet
is preferred even though it doesn't have the optimal expected
gain.
Then that would be an example of a cognitive bias.
, we've seen the framing effects as I mentioned.
So these were , expressed in terms of joint bets
where you have to make two bets.
One is typically about gains, the other one is typically
about losses.
And people choose the Non-optimal option in both cases.
And it turns out how the bed is framed matters.
That's why it's called a framing effect.
So if I present it in terms of expected values,
expected gains and losses, then people make the optimal decision.
But if it's framed in in another way, in terms
of surer gains or surer losses, then people don't behave
in an optimal way.
And the expectation we've seen is that.
So people evaluate about it at a time, and they
have a utility function which doesn't focus on the overall
wealth but on gains and losses.
And we've seen that gains seem to be more important
than losses.
So if you have a utility function or a value
function here, then first of all you can see that
the gains the utility flattens out.
So there's diminishing returns.
And the curve is steeper for losses.
So losses have a more substantial effect than gains of
the same value.
And there's also here a larger period of the curve
where this is a linear or nearly linear effect.
The other thing we've seen is that people tend to
overweight extreme probabilities.
So really low probabilities or really high probabilities.
People are something people are attracted to and they overestimate
how probable it is.
For example, to die in a plane crash or to
win the lottery.
Right?
These are really, really low probability events.
But people overestimate, , overestimate the probability.
And we'll talk about why in today's lecture.
So, , more examples of cognitive biases today.
, and then we will reflect a little bit on
why these biases arise.
So it could be that something's wrong with the experiment.
And the experiment actually thought that there was an optimal
solution.
But the participants actually they interpret the experiment differently.
So it could be a problem with that.
, and it's also important not to forget that humans
operate under resource limits.
Right.
We have a limited amount of time, limited amount of
memory, computational limitations and so on.
So maybe it's a resource optimal decision.
Right.
Given given the constraints we're operating under rather than a
globally optimal decision that we would be able to arrive
at if we didn't have any constraints in terms of
compute, memory and so on.
Okay.
Let's start with the bias though.
And , this is again from Tversky and Kahneman.
, here's a description of of a person.
So Bill is 34 years old.
He's intelligent but unimaginative, compulsive, generally lifeless.
In school he was strong in mathematics, but weakened social
studies and humanities.
And then we have a bunch of statements.
And to ask in Kahneman, they asked participants to rate
how to rank, sorry, to rank the statements in order
of how likely they are to be true of of
Bill.
So Bill is a physician who plays poker for hobby.
Bill is an architect.
Bill is an accountant.
Bill plays jazz for hobby.
Serves for hobby as a reporter.
Is an accountant who plays jazz, climbs mountains for a
hobby, and so on.
And so let's focus on these statements here.
Bill is an accountant.
Bill place jazz for a hobby.
And then the conjunction of those two claims.
Bill is an accountant and plays jazz for a hobby.
And, , let's call the first statement A, the jazz
statement J and then the conjunction A and J.
And I'm using this very simple logical notation here where
this is an and a, not an a logical implication.
So most people think that, , Bill is an accountant
who plays jazz for a hobby is more likely than
Bill plays jazz for a hobby.
Okay.
, and that seems to be a fallacy, right?
Because obviously, the set of the set of jazz playing
accountants is a subset of the jazz players.
Right?
So, , there's this is strictly more likely than this.
But people still think that somehow being an accountant who
plays jazz is more likely than just being a just
playing jazz.
, and this is, of course, because this description that
we're giving up here, , is, You know, stereotypical description
of an accountant.
Okay, so, , people don't seem to, , to deal
with these conjunctions in the expected way.
And this is sometimes called the conjunction fallacy.
, more specifically, , there is a bias towards a
representative.
, answer.
So if we come up with a description here that
is typical of an accountant, then people will pick out
the accountant, but they will also pick out accountant and
something else.
Right.
Which is, strictly speaking, the wrong answer, because it's less
likely that you're an accountant and something else.
But compared to just an accountant.
So this is called the representative.
, bias.
And, , the explanation is that the description is representative
of being an accountant.
And what's going on under the hood is in probabilistic
terms, is that the probability of the description given the
category.
So description is the text that we've seen and the
category is accountant is high.
Right.
If it's an accountant category then you're likely to describe
it that.
But here of course we are inferring the opposite.
We are inferring the probability of the category given the
description.
Right.
So in this task participants were given the description.
And then they had to guess the category.
So , people somehow seem to reason that this here,
which is the likelihood is the same as the posterior,
right.
If you think about it in Bayesian terms, then, ,
of course, the probability of x given Y is not
the same as the probability of y given x.
And what's the difference between those terms?
It's the prior right.
If we were to compute the posterior here then we
would need to take the prior into account.
But people don't seem to be able to do that.
And this is sometimes called base rate neglect.
It's something we'll come back to later in this lecture.
So there's a bunch of things going on.
First of all, the fact that the description is representative
of a certain category, but also the fact that people
don't seem to be able to distinguish between likelihoods, this
probability and posteriors.
This probability.
Okay.
Let's try to estimate some probabilities ourselves.
So if you scan the QR code, , you will
get a, , a short probability estimation exercise.
And here's the code again.
Okay, let's get started.
Here is a few causes of death.
Sorry, this is slightly morose, and you're supposed to order
them by probability.
So starting with the most likely going down to the
least likely cause of death.
Is most likely at the top or bottom.
, it's at the top, I think.
Let me quickly check.
I'll turn off my screen so that you don't see
the.
Oh.
Yeah.
Most likely at the top, please.
Oh.
Wrong button.
So sorry.
Most likely at the top.
Of that.
Okay.
Yeah.
Okay.
Yeah.
I don't.
Think.
Okay.
All.
Right.
Thank you.
So.
Much.
Thank.
You very.
Much.
Okay.
I seem to have managed to reset the projector.
.
Okay.
42 answers.
Great.
So let's have a look at, first of all, what
you thought.
Okay, so five people say car accident, bike accident, drowning
accident, shark attack, car accident, bike accident.
Okay, so people don't really cars and bikes.
, shark is always last, which is great.
Cars?
Always first.
Yeah.
So this is actually pretty good.
Let's have a look at the correct answer.
So car accident maybe.
Not surprising then.
Poisoning seems to be really frequent.
Drowning also bike accident.
, air slash.
So the statistics I have taken this from says air
slash space accident.
I'm not quite sure what the space accident is, but,
hey, .
Train crash.
Unlikely.
Shark attack.
Really unlikely.
Okay, , so what this is supposed to demonstrate, ,
is that people have a bias called the availability bias.
So I mean, you obviously didn't fall for this, but.
Yes.
Question.
Explain why the representation bias is about.
Is it a policy because it sounds correct.
You're taking information you've gathered from data and applying it.
That's right.
Okay.
Yeah.
Let's let's go back.
.
You mean this one?
The representativeness bias?
.
It is.
Is it incorrect?
So it depends on the situation, right?
So in in this situation, it leads to an incorrect
outcome because it overrides other factors.
Right.
So here if I just choose accountant then that's great
because the, , the description is representative of being an
accountant.
But then people also think that being an accountant who
plays jazz is more likely than just playing jazz.
And that would be a fallacy then, because the representativeness
of the description overrides the logical rule that if something
is a subset, then it's less likely than the superset,
right?
Because people who play jazz are a superset of people
who are accountants and play jazz.
So people get that.
, people make that mistake, right?
This is a sort of specific fallacy, the conjunction fallacy,
which is caused by the representativeness bias.
And in not in all cases, the representativeness bias causes
problems.
But here in this particular case where they even though
something is, you know, provably less likely, they still find
it more likely because it's representative.
Okay.
Yeah, sorry, I probably didn't explain this very well.
But moving on to the next bias, and this is
sort of based on the observation that people find things
that are, , that are unlikely, but highly available.
They find this.
They overestimate the probability.
So train crashes.
Plane crashes.
These are heavily reported in the media when they happen.
Terrorist attacks, school shootings and so on.
Even though these are all low probability events, they are
highly available because of media reporting.
Car crashes, on the other hand, are very, very frequent
and they're not reported in the media in most cases.
So they're less available.
And in comparison we underestimate the probability of car crashes.
So that's the argument here.
Here's another example which is maybe.
So the probability is generated by just trying to remember
examples.
That's the claim here.
Here's another example.
So consider the letter r.
So is R more likely to appear in the first
position of a word or in the third position of
a word.
So most participants think first position is more likely if
you actually count it in a body of text.
Then the third position is more likely, but the first
position is just much more available.
Right.
If I asked you name ten words that start with
R, you can probably easily do this.
If I ask you name ten words where the third
letters are.
That is actually really hard, right?
So it is something with being available in memory being
easy to recall.
And we're biased towards events or in this case lexical
items that are easy to remember and easy to recall.
Okay.
And this is called availability bias.
, and again just with the representativeness bias, it
is a useful heuristic.
And it works in a lot of cases.
Right.
Something is highly available because it's very frequent most of
the time.
You're maybe find it easier to, , to categorise dogs
compared to, I don't know, marsupials, , because you've seen
hundreds of dogs, but you've only seen a few marsupials
in the zoo or something that, right?
So there's an availability bias, which influences how good you
are at making a certain judgement.
And there is actually a research area which is called
inference by sampling, where you estimate probabilities by sampling and
seeing how frequent an event are, an event is.
And so it's probably true that we are using that
to some extent.
And if something is available, the dog example, you
have lots of examples of dogs to draw upon.
Then probably that means it is actually more likely.
Right?
So if you take a new animal is more likely
to be a dog than, , a marsupial, And also
extreme events are often, , important ones.
Right.
So in in a plane crash, this is a very
important event.
Lots of people die if a plane crashes.
So, , you the utility, right?
The losses or the gains are often very large for
these low frequency events.
Just a lottery win, right?
The probability is really low, but it is a and
a very important event okay.
And so that has to do with this idea that
we overestimate highly probable or highly improbable , events.
So that probably also plays into this availability bias.
, if you want the the probabilities, the odds actually.
Here's the link for the shark attacks.
Comparison to all other causes of death.
Okay, let's look at another bias.
And this goes back to Bayesian reasoning.
If you remember Bayes rule where we compute a posterior
probability based on a prior and a likelihood term.
It turns out people also struggle with that.
And there's a well-known example here.
Goes back also to Kahneman and also to Berkeley Hillel.
So they have the following example.
So let's assume in a city we have two types
of cubs, 80% of the cubs are blue, 15% are
green, and one of the cubs is involved in a
hit and run.
It's happening at night.
So there's a witness.
But the witness isn't entirely sure.
But the witness claims it's a green cab that was
involved in the accident.
And then the police does some tests and they find
out that the witness is correct 80% of the time
when they're given, let's say a picture of a cop
and it's in the lighting is the way it was
at the time of the accident.
So it's at night.
So you're not 100% sure when you're recognising the colour.
So and then from this you're supposed to infer was
the cab involved in the hit and run, was it
green or blue.
So most people say green.
That's the typical judgement.
But of course we have learned how to compute this
using Bayes rule.
Right?
Because we're given a few numbers here.
This here is basically our prior probability, right?
Overall, without any witness, any more information, what's the probability
that the cab is green.
It's 15%.
What's the probability that it's blue.
It's 85%.
That's our prior probability.
And this here the witness is correct 80% of the
time.
That is our likelihood.
So given that the cab is really green.
How likely is it that the witness says it's green
and this number is 80%?
How likely is it that they're mistaken and they identify
it as blue?
It's 20%.
Okay.
So this is our likelihood here.
And so we can plug this into Bayes rule.
So we're computing the probability of green given that the
witness says green.
So the witness is correct.
And that would be the prior probability that the car
is green times the probability that the witness says green
given that it is green right.
This is the likelihood.
This is the posterior.
Note the inversion of the conditional here.
That's what we're trying to achieve.
And then we're dividing by the probability of the data.
So that would be the probability of green given the
witness says it's green.
And the probability of blue given that the witness says
it's blue, that it's witnesses green.
But the cab is really blue.
And then we work this out.
This is 15% times 80% up here and then down
here 15 times 80 plus 85 times 20.
Right.
The complement the complement of the probabilities.
And that works out as 0.41.
So it's actually less likely than 50% that the car
is the cab is green.
Okay.
So the correct answer would actually be blue.
And but people say green.
And the question is why.
So the traditional explanation for this is something called base
rate neglect.
So that's a cognitive bias where people don't take the
base rate into account.
Base rate is just a fancy name for prior right?
So if you don't take the prior into account here,
then of course the posterior is the same as the
likelihood.
So it's 80%.
So of course you're going to say green.
So that is the what is claimed to be the
fallacy here.
And yeah.
So participants ignore the prior.
Ignore the base rate.
And what's the explanation for this now?
Well, in the literature.
So Barrell says we have a heuristic that determines the
relevance of the information.
Kahneman would say the green cup is more representative of
the witnesses report, so they don't really have a very
good explanation.
, the phenomenon is important because you see it in
other cases, too.
So most concerningly, perhaps, is that doctors make the same
mistake, and there's a bunch of experiments showing this where
the base rate would be the prevalence of a disease,
, let's say prevalence of a certain form of skin
cancer, , is 1 in 1000, say.
So that's a base rate.
That's a prior probability.
And then you have a diagnostic test.
But of course a the diagnostic test is one that
is not 100% accurate typically.
So you assume the test is accurate.
I don't know 90% of the time when there is
a cancer, it gives a positive result.
That would be your likelihood.
And so based on that you can use Bayes rule
in the same way right.
You have the probability of skin cancer.
You have the probability that the test is gives
gives an accurate , gives a positive result given that
there is skin cancer.
And then you can compute the posterior.
So that's how you should reason about these things.
But doctors make the same mistake.
They don't take the the likelihood into account sorry.
The prior into account.
They only compute this based on the on the likelihood.
In legal settings, it's often the case.
I mean, this is really a legal case that we
saw with the cabs, right?
You could imagine a jury having to decide whether.
How likely is it that this was really a green
cab?
Given that we have a witness, a witness is 80%
accurate.
And then there's the tendency to say, yes, it was
a green cab, ignoring the base rate, ignoring the fact
that green cabs are quite rare.
Okay, so let's maybe take stock and look at the
biases that we've seen so far.
So there's another quick quiz.
So here on the left side we have a number
of observations.
and on the right side we have a number
of biases that could potentially explain those observations.
So we have availability bias base rate neglect, framing effect
representative bias.
And then here on the left side People prefer certain
gains, even if they don't result in the best expected
value.
.
A student is more likely to be judged to study
physics rather than to study science.
People overestimate the probability of events that appear in the
news a lot.
Or, , if we interpret a test result, we don't
think about the prevalence of the disease.
Okay.
Let's have a look.
Okay.
So when interpreting a test result the prevalence of the
disease is ignored.
So this is the base rate neglect.
Yes.
And we we just talked about this that people are
quite bad at making use of prior probabilities.
They focus too much on the likelihood when they draw
an inference.
Given the detailed description, student is judged to be more
likely to study physics than to study science.
That would be representative bias.
Yeah, that's correct, because obviously it's more likely that someone
studies study science, right?
Because physics is a science, but there's lots of other
sciences.
So it's a subset.
But maybe our description is such that somehow it's very
typical for a physicist rather than, say, a chemist or
a biologist.
So that would be representative bias.
, overestimates the probability of events that appear frequently in
the news.
That would be availability bias.
Right.
If you've encountered something very frequently or you've seen it
in your daily life, you've seen it in the news.
It's being reported upon all the time.
Then it's more available.
It's easier to access.
You overestimate the probability.
So it's essentially something to do with memory.
How easily you remember something.
And then finally frame your effect.
People prefer certain gains even when they don't result in
the best expected value.
So this depends on how the problem is framed.
If I frame it in, you have a certain loss
of £100, or you have a certain gain of £25,
then this overrides people's, , probabilistic reasoning.
If I frame it in terms of this bet has
an expected value of £20, this bet has an expected
value of £30.
Then people can do it right.
So it depends on how the problem is framed.
Okay.
These are just the yeah, most of you got this
correct.
Very good.
And.
.
So, , for the rest of the lecture, I have
one more puzzle.
, it's not a cognitive bias in the traditional sense.
But before that I would quickly.
Talk a little bit about reading.
Just take a moment.
, so the reading I've only said this one paper
as the reading for today and for yesterday.
But if you have some time over Flexible Learning Week,
this is a really good book to have a look
at.
It's called Thinking Fast and Slow.
It's by Kahneman himself.
It's very accessible.
And , if you look in particular on at the
second and the fourth part in the book, , he
has many more biases than we have seen in the
lectures.
So if that's something you find fascinating.
So for example, there's the halo effect.
The halo effect is when you see that someone is
competent and Spoken, , and , for example, interviews.
Well, then you think they are also good at their
job.
So the someone who's just, you know, a nice person
and well-spoken and maybe well-dressed as well.
Then they built up this halo, and people infer that
they're competent.
So you overestimate someone's competence.
And this is called the halo effect.
And that's just one of the biases that people have
studied.
We've only seen 4 or 5 in this course.
But if you want to find out a little bit
more, Kahneman also has a whole theoretical framework where there's
two cognitive systems called system one and system two.
System one is fast and making heuristic judgements, which are
sometimes wrong, where systems two is slow, deliberative thinking less
likely to biases, and less likely to make mistakes.
So he has this whole framework anyway.
So if you want to go beyond a little bit
of what the course is, , what I'm covering here.
So.
But let's look at a particular puzzle, which is called
the Watson card selection task.
, and it goes this.
So you have a bunch of cards, they have numbers,
, or letters on them.
So here we have E27.
And you're given a rule as well.
The rule is if there's a vowel on one side
of the card, then there's an even number on the
other side.
Right.
So we have two types of letters.
We have vowels and consonants.
And we have two types of numbers even in odd
numbers.
And you're supposed to turn over the cards that allow
you to test if this rule is true.
Okay.
And.
So logic dictates that what you're supposed to do is,
, turn over E and seven E is pretty clear,
right?
It has a vowel on one side.
So I can I can test whether it's an even
number on the other side.
Seven I need to turn over because of the inverse
of this, right.
It has an odd number.
So the other if I find, , .
If I find a vowel on the other side, then
the rule is incorrect.
So both allow you to, , disprove the rule.
What people do is, though, .
Oh, yeah.
So this explains what I've just said.
So we will write this as, , p and not
Q, right.
E and seven seven is not an even number, so
it's an odd number.
So that would be P and this would be not
Q.
, and , rather than, for example, turning over x
or two, which would be not P and Q.
So .
All right.
So it would be this one.
And here.
And the rule is that p I have a vowel
on one side implies q, and I have an even
number on the other side.
So if we flip over the e and we see
an odd number, then we have falsified the rule.
If we flip over the seven and we see a
vowel, then we have also falsified the rule, right?
So there's no way of actually confirming the rule.
But by turning over these two cards, we can falsify
the rule.
People, however, they, , they tend to pick, , some
of the other cards.
So if you, for example, take the X, then it
doesn't matter if the number is even odd, right?
It cannot falsify the rule because x is a consonant.
And if we flip over the two, then again it
doesn't matter if there's a vowel or not.
On the other side, it doesn't falsify the rule because
it's an even number.
And this is when you actually run an experiment and
you ask participants which rules they take, which cards they
take, then p and Q is the most common.
So q here is two, so e and two.
And then a certain number also just turn over e.
But the correct answer here p and not q is
very small one out of 34 £0.34 and not P
obviously doesn't doesn't make sense.
But one person did it anyway.
And then you had four.
Who did?
P and Q and not Q, which again is a
contradiction.
So that shouldn't happen.
And p and not p.
And q is one out of 34.
And then there's one two that turned over everything.
So .
The important thing is that people either only turn over
the P, or they turn over the P and the
Q, and that is the correct the incorrect answer, because
the Q doesn't really tell you anything, right?
P fair enough.
But Q doesn't tell you anything.
So, , what is going on?
How could we explain this behaviour?
So way back in the day and this is an
old experiment.
, he said that this requires some knowledge of logic.
You need to reason in terms of logical rules.
But that's not what people do.
People are really bad at these logical rules and at
formal operations in general.
That was his explanation.
And that is obviously a statement of, you know, this
is how things are and it's not an explanation.
Right?
It's not a very good way of, , of dealing
with this particular finding.
So people have been trying to understand what's going on
ever since.
And there's various various ways of explaining this.
And let's maybe try quickly whether we can use our
biases that we've seen to explain this.
Okay.
So we go back here.
Here's all the biases we've seen in the course.
Do you think one of these biases could explain the
fact that people are not really able to reason logically
in the Ways and cards Card selection task.
Could it be the framing effect, availability bias, representative bias,
and so on?
Okay, let's have a look.
Framing effect.
So 20% thought framing effect.
.
Does anyone want to explain framing effect?
Why?
Okay.
Yeah.
, actually, this information is flexible.
So that you think that if you apply the second
set of about 20s.
Maybe next week or two more.
Yeah.
So the explanation, let me just repeat it, is that
it's something about information gain.
Right.
So turning over the first card gives a certain amount
of information.
And then maybe you don't need the second card anymore.
So it's really about maximising some form of informativeness out
of the data rather than following logical rules.
And this is actually this is an explanation that has
been advanced in the literature.
And I'll talk about this briefly at the end.
So , in general, it turns out that how the
problem is formulated is important.
So if I give you an abstract rule, there's
a vowel on one side and an even number on
the other side, then this is really hard for people.
If I give you a familiar rule, if you're
over 16, you're allowed to drink alcohol, Then people are
actually quite good at reasoning with examples this.
So that is also an aspect of the framing, right?
How familiar is the situation and how is it?
Can you relate it to an everyday situation.
So that would be a type of framing effect.
Or maybe it's availability right.
If you've encountered rules this before and you think
about implications a if A then B all the
time, then maybe you find this easier.
So it could be availability as well.
Representativeness I'm not sure.
Base rate neglect I'm also not sure because there's no
probabilities given a priori.
Right.
So maybe if we know that vowels and consonants are
not the same probability, then something base rate could
come in.
Conjunction fallacy could be because people don't seem to be
able to, , to, you know, to to deal with
these logical conjunctions that we've seen.
, it's basically.
, p is okay, and P and Q is okay.
But then if you have a negative and here you
even have a contradiction, right?
P and not P.
, so things maybe that is one aspect of logical
reasoning that people find hard rather than saying they are
bad at formal operations in general.
Okay, so none of these explanations are entirely convincing.
And as I said, there's a long sort of tradition
of research there.
Let's look at one particular, , alternative explanation.
, so there's basically two competing hypotheses, right?
If you write this the rule down as an implication.
If P, then q can either be true or can
be false.
And maybe we're actually interested in finding out the most
information, the information that we need.
And this is a bit what your colleague said.
Finding out how to falsify this rule.
Right.
And if we have one bit of information that might
already be enough and we can stop.
So and in particular, we don't only want to say
this is evidence against the rule, we also want evidence
for the rule.
Okay.
So people.
That's the claim here.
And this is a paper here by Oxford and Charter.
They don't just want to falsify.
They also want to verify.
I mean a scientist where we're trained to think in
terms of falsification.
But maybe that's not true in everyday situations.
Here's an example.
If you eat tripe P, then you feel sick.
So this is not abstract with vowels and Consonants and
even in odd numbers.
But it's an everyday situation, and here p and not
p would correspond to.
Hello tripe eater.
How do you feel?
Right.
So are you sick or not?
And Q and not Q would be hello sick person
or well person.
Have you had tripe recently?
That would be the the equivalent in this real world
situation.
And then we can come up with two policies either
P and Q which would be Alice's.
Call it Alice's policy.
Ask the tripe eaters how they feel.
Right?
Are they sick or not?
And then secondly, ask sick people if maybe recently they've
been they've been eating tripe and that's why they got
sick.
Whereas Bob has a strict logical approach here of P
and not Q.
Ask eaters how they feel and ask well people if
well people if they have eaten tripe recently.
So which policy seems more reasonable here?
Probably it's Alice's policy, right?
Because talking to the sick people is bound to be
more informative.
And there's a lot fewer of them probably, than, well,
people.
So just randomly asking people that are not feeling sick
whether they've eaten tripe, that's not going to be very
informative.
So what this means is that we're now taking account
the probability of the evidence.
Right.
We're taking into account the fact that being sick is
more informative for our for the rule we're trying to
test than being well.
Right.
Remember we didn't have an abstract rule.
We had this concrete rule.
If you eat tripe, you will feel sick.
And so the important things are that being sick and
the eating tripe.
And , so now we've turned this into a problem
where we reason with probabilities as well.
And the assumption is, at least in this take on
the wasting card selection task, is that people always have
probabilities, even if they're implicit, right?
They're making assumptions about probabilities because in everyday life there's
always a some degree of certainty or some degree of
risk attached to the information that we're dealing with and
attached to the decisions that we make.
And you can actually formalise this, and you can make
the assumption that people choose the options P and Q
in this case that are most informative for testing this
rule.
So the probability of p and q should then affect
the choices, right.
Because I'm no longer assuming this is purely a logical
problem, I'm assuming it's a problem about probabilities as well.
So if I manipulate the probability of p and q
then people's behaviour should change.
And you can actually show this.
So in this plot you have the probability of q
here on the y axis, the probability of p on
the x axis, and here the black region where the
probability of p is between zero and point two and
sorry of q and the probability of p, , roughly
matches the probability of q.
, is the region where, , turning over p and
q would be optimal, right?
So the answer is basically p.
And q is a good strategy sometimes.
Right.
It depends on the probability of p and q.
If the probability of q, for example, becomes too low,
then it's a good idea to look at not q
rather than q okay.
Because the probability of not q obviously is one minus
the probability of q and Oxford.
And later they have a fairly complicated story to do
with the expected information gain and so on.
So you can make this mathematically precise, but the intuition
is that the logical story is not the whole story.
You need to take into account the probability of the
different forms of evidence as well.
Okay.
So today we've seen a number of classic experiments to
do with decision making.
We've looked at framing effects as you see them in
paired lotteries.
We've looked at the representative bias where if the description
of of an event or a person is very representative,
then this overrides other evidence, the availability bias, where we
go for things that are easily accessible, they are present
in memory or in the environment or in the news.
We've seen base rate neglect where people don't take the
prior probability into account and when they reason.
And we've seen the waste and card selection task, which
is about how logical reasoning and probabilistic reasoning interact in
complicated Ways.
And overall, the enterprise is to try to understand the
limits of of rational behaviour, of optimal behaviour and try
to explain why in some situations we behave in a
rational, optimal way.
When we make decisions, in other situations we don't.
And that's it for today.
And that's also my part of the course.
So thank you for your attention.