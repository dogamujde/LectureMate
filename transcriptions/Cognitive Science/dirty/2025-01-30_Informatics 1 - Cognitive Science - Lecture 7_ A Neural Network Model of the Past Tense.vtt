WEBVTT

00:00:05.360 --> 00:00:05.640
<v Speaker 0>Lord.

NOTE CONF {"raw":[100]}

00:00:07.680 --> 00:00:08.000
<v Speaker 0>Jesus.

NOTE CONF {"raw":[100]}

00:00:12.680 --> 00:00:13.760
<v Speaker 0>Christ Jesus our.

NOTE CONF {"raw":[100,94,94]}

00:00:21.120 --> 00:00:21.360
<v Speaker 0>Lord.

NOTE CONF {"raw":[94]}

00:00:24.640 --> 00:00:25.080
<v Speaker 0>And.

NOTE CONF {"raw":[66]}

00:00:28.000 --> 00:00:30.000
<v Speaker 0>Saviour Jesus.

NOTE CONF {"raw":[66,95]}

00:00:35.400 --> 00:00:38.120
<v Speaker 0>Christ our Lord.

NOTE CONF {"raw":[89,73,91]}

00:00:40.000 --> 00:00:40.400
<v Speaker 0>Amen.

NOTE CONF {"raw":[42]}

00:00:42.000 --> 00:00:42.360
<v Speaker 0>Amen.

NOTE CONF {"raw":[58]}

00:00:42.400 --> 00:00:42.720
<v Speaker 0>Amen.

NOTE CONF {"raw":[42]}

00:00:49.920 --> 00:00:50.240
<v Speaker 0>The Lord.

NOTE CONF {"raw":[61,80]}

00:00:58.720 --> 00:00:59.040
<v Speaker 0>Jesus.

NOTE CONF {"raw":[74]}

00:01:17.010 --> 00:01:17.450
<v Speaker 1>So.

NOTE CONF {"raw":[100]}

00:01:17.450 --> 00:01:18.050
<v Speaker 1>Hi everyone.

NOTE CONF {"raw":[100,100]}

00:01:18.050 --> 00:01:19.130
<v Speaker 1>Let's get started.

NOTE CONF {"raw":[100,100,100]}

00:01:19.730 --> 00:01:23.530
<v Speaker 1>Um, unfortunately, there's technical problems again.

NOTE CONF {"raw":[96,100,97,100,100,100]}

00:01:24.330 --> 00:01:26.850
<v Speaker 1>So I think I'll be okay, but you won't be

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:01:26.850 --> 00:01:29.810
<v Speaker 1>able to see the whole club because somehow the the

NOTE CONF {"raw":[100,100,100,100,52,56,100,100,100,100]}

00:01:29.810 --> 00:01:31.930
<v Speaker 1>system here doesn't recognise my laptop.

NOTE CONF {"raw":[100,100,100,64,100,100]}

00:01:33.010 --> 00:01:35.810
<v Speaker 1>Uh, so you should still be able to do the

NOTE CONF {"raw":[65,100,100,100,100,100,100,100,100,100]}

00:01:35.810 --> 00:01:36.290
<v Speaker 1>whole club.

NOTE CONF {"raw":[38,100]}

00:01:36.330 --> 00:01:36.930
<v Speaker 1>You just.

NOTE CONF {"raw":[100,100]}

00:01:36.970 --> 00:01:38.810
<v Speaker 1>We just won't see the results.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:01:39.090 --> 00:01:40.450
<v Speaker 1>So let's see how it goes.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:01:40.490 --> 00:01:43.770
<v Speaker 1>Anyway, the slides are there and the recording should work

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:01:43.770 --> 00:01:44.130
<v Speaker 1>as well.

NOTE CONF {"raw":[100,100]}

00:01:44.130 --> 00:01:45.050
<v Speaker 1>It worked last time.

NOTE CONF {"raw":[100,100,100,100]}

00:01:45.050 --> 00:01:46.850
<v Speaker 1>You've probably seen that.

NOTE CONF {"raw":[100,100,100,100]}

00:01:47.690 --> 00:01:48.210
<v Speaker 1>Uh, sorry.

NOTE CONF {"raw":[74,100]}

00:01:48.250 --> 00:01:49.690
<v Speaker 1>The last time in this room.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:01:50.170 --> 00:01:53.250
<v Speaker 1>Okay, so we talked about backpropagation.

NOTE CONF {"raw":[100,100,100,100,100,96]}

00:01:53.690 --> 00:01:56.650
<v Speaker 1>We talked about neural networks and perceptrons.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:01:57.050 --> 00:01:59.590
<v Speaker 1>And today we're going to see an application in cognitive

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:01:59.590 --> 00:02:01.590
<v Speaker 1>science of those methods.

NOTE CONF {"raw":[100,100,100,100]}

00:02:01.670 --> 00:02:05.390
<v Speaker 1>So we'll discuss a classical model of past transformation.

NOTE CONF {"raw":[100,100,100,93,100,100,100,100,58]}

00:02:05.790 --> 00:02:09.750
<v Speaker 1>So we're basically looking at this as an alternative way

NOTE CONF {"raw":[100,93,100,100,100,100,100,100,100,100]}

00:02:09.750 --> 00:02:11.470
<v Speaker 1>of looking at past tense formation.

NOTE CONF {"raw":[100,100,100,100,76,74]}

00:02:11.510 --> 00:02:11.750
<v Speaker 1>Right.

NOTE CONF {"raw":[97]}

00:02:11.790 --> 00:02:16.910
<v Speaker 1>We've seen the words and rules approach where you analyse

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,87]}

00:02:16.910 --> 00:02:21.870
<v Speaker 1>it as a word being acquired initially words being acquired,

NOTE CONF {"raw":[100,100,100,100,100,90,100,98,100,100]}

00:02:21.870 --> 00:02:23.230
<v Speaker 1>then rules being acquired.

NOTE CONF {"raw":[98,100,100,100]}

00:02:23.550 --> 00:02:26.990
<v Speaker 1>Then the two of them interacting in an interesting way.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:02:27.550 --> 00:02:31.830
<v Speaker 1>And all of this was to explain how children learn

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:02:31.830 --> 00:02:35.270
<v Speaker 1>the past tense and other phenomena in the language that

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,65]}

00:02:35.310 --> 00:02:36.350
<v Speaker 1>they're exposed to.

NOTE CONF {"raw":[91,100,100]}

00:02:37.230 --> 00:02:42.270
<v Speaker 1>And the neural network approach is the the total opposite

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:02:42.270 --> 00:02:45.190
<v Speaker 1>of this in a sense, because it doesn't assume any

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:02:45.190 --> 00:02:46.270
<v Speaker 1>rules, any words.

NOTE CONF {"raw":[100,100,100]}

00:02:46.590 --> 00:02:49.510
<v Speaker 1>It just assumes a general learning mechanism.

NOTE CONF {"raw":[100,100,100,94,100,100,100]}

00:02:50.110 --> 00:02:52.670
<v Speaker 1>So we're going to see a model called the McClelland

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,98,82]}

00:02:52.670 --> 00:02:52.990
<v Speaker 1>model.

NOTE CONF {"raw":[100]}

00:02:52.990 --> 00:02:56.310
<v Speaker 1>As I said, a classic model first invented in the

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:02:56.350 --> 00:02:56.830
<v Speaker 1>80s.

NOTE CONF {"raw":[100]}

00:02:57.760 --> 00:03:01.680
<v Speaker 1>We'll look in particular on the at the features, because

NOTE CONF {"raw":[85,100,100,100,100,100,100,100,100,100]}

00:03:01.680 --> 00:03:04.520
<v Speaker 1>even if you assume a neural network model and you

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:03:04.520 --> 00:03:07.840
<v Speaker 1>assume a general learning algorithm like backpropagation, you still need

NOTE CONF {"raw":[100,100,100,100,100,100,96,100,100,100]}

00:03:07.840 --> 00:03:10.320
<v Speaker 1>to be very careful about how you represent the input

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:03:10.320 --> 00:03:11.720
<v Speaker 1>and the output of your model.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:03:12.520 --> 00:03:14.680
<v Speaker 1>And we're going to discuss this in some detail.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:03:14.880 --> 00:03:16.920
<v Speaker 1>And then we'll talk a little bit about the model

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:03:17.040 --> 00:03:22.360
<v Speaker 1>and ask the question whether it actually adequately captures the

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:03:22.360 --> 00:03:24.840
<v Speaker 1>phenomenon which is past tense acquisition here.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:03:25.240 --> 00:03:29.400
<v Speaker 1>And then we'll sort of fast forward 25 years and

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:03:29.400 --> 00:03:32.240
<v Speaker 1>look at a more contemporary model of past tense acquisition

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,57,100]}

00:03:32.240 --> 00:03:38.000
<v Speaker 1>by Kirov and Cotterell, which applies all the latest deep

NOTE CONF {"raw":[100,100,100,54,100,100,100,100,100,93]}

00:03:38.000 --> 00:03:38.240
<v Speaker 1>learning.

NOTE CONF {"raw":[93]}

00:03:38.240 --> 00:03:39.840
<v Speaker 1>I mean, this model is also a few years old

NOTE CONF {"raw":[98,98,100,100,100,100,100,100,100,100]}

00:03:39.840 --> 00:03:42.920
<v Speaker 1>now, but much more advanced deep learning techniques than were

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:03:42.920 --> 00:03:44.040
<v Speaker 1>available in the 80s.

NOTE CONF {"raw":[100,100,100,100]}

00:03:44.440 --> 00:03:46.680
<v Speaker 1>And the results are somewhat surprising.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:03:46.680 --> 00:03:47.840
<v Speaker 1>So we'll come to that.

NOTE CONF {"raw":[100,100,100,100,100]}

00:03:48.720 --> 00:03:49.040
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:03:49.040 --> 00:03:52.080
<v Speaker 1>But just to quickly remind you, and I've just said

NOTE CONF {"raw":[100,100,100,100,100,100,100,93,100,100]}

00:03:52.080 --> 00:03:55.560
<v Speaker 1>something along those lines, so we've seen the sort of

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:03:55.600 --> 00:03:59.490
<v Speaker 1>classical symbolic rationalist word and rules approach.

NOTE CONF {"raw":[100,100,100,99,100,100,100]}

00:03:59.850 --> 00:04:02.530
<v Speaker 1>Where you have a rule, for example, stick ed at

NOTE CONF {"raw":[100,100,100,100,100,100,100,99,94,100]}

00:04:02.530 --> 00:04:04.530
<v Speaker 1>the end of the base form of the verb to

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:04:04.570 --> 00:04:08.410
<v Speaker 1>generate past tense forms, and then you need a large

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:04:08.410 --> 00:04:09.010
<v Speaker 1>lexicon.

NOTE CONF {"raw":[100]}

00:04:09.010 --> 00:04:11.970
<v Speaker 1>You need to put irregular verbs directly into the lexicon.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:04:11.970 --> 00:04:17.530
<v Speaker 1>You need something like blocking to avoid people generating both

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:04:17.530 --> 00:04:20.090
<v Speaker 1>the regular and the irregular form, for example.

NOTE CONF {"raw":[100,100,100,100,100,98,100,100]}

00:04:20.489 --> 00:04:22.650
<v Speaker 1>Hold, hold and hold.

NOTE CONF {"raw":[100,62,100,100]}

00:04:22.690 --> 00:04:23.330
<v Speaker 1>Hold it.

NOTE CONF {"raw":[100,100]}

00:04:23.610 --> 00:04:25.370
<v Speaker 1>You don't want both of them, at least not at

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:04:25.370 --> 00:04:26.090
<v Speaker 1>the same time.

NOTE CONF {"raw":[100,100,100]}

00:04:26.650 --> 00:04:30.650
<v Speaker 1>And that is already sort of hinting at problems with

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:04:30.650 --> 00:04:31.210
<v Speaker 1>that approach.

NOTE CONF {"raw":[100,100]}

00:04:31.250 --> 00:04:31.490
<v Speaker 1>Right.

NOTE CONF {"raw":[83]}

00:04:31.530 --> 00:04:32.170
<v Speaker 1>Blocking.

NOTE CONF {"raw":[100]}

00:04:32.450 --> 00:04:33.810
<v Speaker 1>That's just a stipulation.

NOTE CONF {"raw":[100,100,62,100]}

00:04:33.810 --> 00:04:35.770
<v Speaker 1>There isn't really good evidence for it.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:04:35.810 --> 00:04:37.010
<v Speaker 1>It's ad hoc.

NOTE CONF {"raw":[100,100,100]}

00:04:37.370 --> 00:04:40.210
<v Speaker 1>And maybe we won't necessarily like that in our system.

NOTE CONF {"raw":[100,100,100,93,100,100,100,100,100,100]}

00:04:41.690 --> 00:04:44.330
<v Speaker 1>And here we're going to look at the alternative.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:04:44.450 --> 00:04:45.690
<v Speaker 1>Let's assume we have no rules.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:04:45.690 --> 00:04:47.090
<v Speaker 1>We don't even have a lexicon.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:04:48.050 --> 00:04:51.090
<v Speaker 1>Children just learn past tense forms by analogy.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:04:51.650 --> 00:04:51.970
<v Speaker 1>Right.

NOTE CONF {"raw":[100]}

00:04:52.010 --> 00:04:55.510
<v Speaker 1>So they have forms like fold folded, mould Mould.

NOTE CONF {"raw":[100,100,100,100,100,100,100,73,90]}

00:04:55.510 --> 00:04:56.590
<v Speaker 1>It's called scolded.

NOTE CONF {"raw":[97,97,50]}

00:04:56.750 --> 00:04:57.830
<v Speaker 1>So hold, hold.

NOTE CONF {"raw":[100,100,100]}

00:04:57.830 --> 00:05:02.550
<v Speaker 1>It is a natural analogy which ultimately turns out to

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:05:02.550 --> 00:05:06.430
<v Speaker 1>be incorrect, but we can explain where it comes from.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:05:06.630 --> 00:05:09.830
<v Speaker 1>We don't really need rules, we just need things to

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:05:09.830 --> 00:05:11.510
<v Speaker 1>be similar in a certain way.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:05:12.830 --> 00:05:16.910
<v Speaker 1>And that sort of represents the empiricist approach or the

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,88]}

00:05:18.430 --> 00:05:21.390
<v Speaker 1>the approach where everything is learned rather than innate.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:05:21.630 --> 00:05:22.030
<v Speaker 1>Right.

NOTE CONF {"raw":[100]}

00:05:22.070 --> 00:05:25.630
<v Speaker 1>Remember we contrasted rationalism where things are assumed to be

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:05:25.630 --> 00:05:30.030
<v Speaker 1>innate and genetically encoded in some way to a learning

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:05:30.030 --> 00:05:34.030
<v Speaker 1>based approach, an empiricist approach where things are acquired by

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:05:34.030 --> 00:05:35.030
<v Speaker 1>cultural transmission.

NOTE CONF {"raw":[100,100]}

00:05:35.030 --> 00:05:37.150
<v Speaker 1>They're not encoded in your genes.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:05:38.430 --> 00:05:41.230
<v Speaker 1>Neural networks we've seen we've seen very simple examples.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:05:41.350 --> 00:05:46.470
<v Speaker 1>It's computer modelling approach inspired very loosely by biological networks.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:05:46.750 --> 00:05:49.790
<v Speaker 1>And we've seen perceptions and feedforward networks, which are sort

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,75,100]}

00:05:49.790 --> 00:05:55.440
<v Speaker 1>of an assembly of perceptrons and Of course, a neural

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:05:55.440 --> 00:05:58.200
<v Speaker 1>network model learns by example, right?

NOTE CONF {"raw":[100,100,100,100,100,95]}

00:05:58.240 --> 00:06:00.120
<v Speaker 1>Sees lots of examples of past tense.

NOTE CONF {"raw":[54,100,100,100,100,100,100]}

00:06:00.440 --> 00:06:02.640
<v Speaker 1>And then it's able to pick up general patterns.

NOTE CONF {"raw":[100,100,52,100,100,100,100,100,100]}

00:06:02.640 --> 00:06:05.640
<v Speaker 1>So this should work for both regular and irregular verbs.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:06:06.120 --> 00:06:07.480
<v Speaker 1>That's that's the hope.

NOTE CONF {"raw":[100,100,100,100]}

00:06:08.560 --> 00:06:12.040
<v Speaker 1>Um, but that's not everything.

NOTE CONF {"raw":[82,100,100,100,100]}

00:06:12.080 --> 00:06:12.240
<v Speaker 1>Right.

NOTE CONF {"raw":[100]}

00:06:12.280 --> 00:06:14.880
<v Speaker 1>So we obviously want the model to learn the past

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:06:14.880 --> 00:06:15.240
<v Speaker 1>tense.

NOTE CONF {"raw":[100]}

00:06:15.240 --> 00:06:15.960
<v Speaker 1>That's fine.

NOTE CONF {"raw":[100,100]}

00:06:16.400 --> 00:06:19.560
<v Speaker 1>But we also want the model to have some cognitively

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:06:19.560 --> 00:06:20.720
<v Speaker 1>plausible features.

NOTE CONF {"raw":[100,100]}

00:06:21.040 --> 00:06:22.720
<v Speaker 1>For example U-shaped learning.

NOTE CONF {"raw":[100,100,99,100]}

00:06:22.920 --> 00:06:23.160
<v Speaker 1>Right.

NOTE CONF {"raw":[93]}

00:06:23.200 --> 00:06:27.520
<v Speaker 1>If you remember this, um, in the acquisition of the

NOTE CONF {"raw":[100,100,100,100,94,100,100,100,100,100]}

00:06:27.520 --> 00:06:30.160
<v Speaker 1>past tense and actually in the acquisition of quite a

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:06:30.160 --> 00:06:35.440
<v Speaker 1>lot of other cognitive, um, capabilities, we see this U-shaped

NOTE CONF {"raw":[100,100,100,100,72,100,100,100,100,100]}

00:06:35.440 --> 00:06:40.560
<v Speaker 1>curve where initially during development we get a fairly low

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:06:40.600 --> 00:06:41.280
<v Speaker 1>error rate.

NOTE CONF {"raw":[100,100]}

00:06:41.400 --> 00:06:44.360
<v Speaker 1>So most of the verbs are produced correctly.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:06:44.720 --> 00:06:47.920
<v Speaker 1>Then with time, the error rate goes up and correctness

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:06:47.920 --> 00:06:53.490
<v Speaker 1>goes down somewhere here, and then correctness increases again.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:06:54.570 --> 00:06:56.130
<v Speaker 1>So we get a U-shaped curve.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:06:56.530 --> 00:06:57.290
<v Speaker 1>High accuracy.

NOTE CONF {"raw":[100,100]}

00:06:57.330 --> 00:06:59.370
<v Speaker 1>Low accuracy than high accuracy again.

NOTE CONF {"raw":[100,100,83,100,100,100]}

00:07:00.210 --> 00:07:03.970
<v Speaker 1>And for example, if you take a single verb spoke.

NOTE CONF {"raw":[100,100,100,100,76,100,100,100,100,100]}

00:07:04.450 --> 00:07:05.330
<v Speaker 1>Speak spoke.

NOTE CONF {"raw":[100,100]}

00:07:05.730 --> 00:07:07.810
<v Speaker 1>Then the child would initially get this correct.

NOTE CONF {"raw":[100,100,100,100,100,100,99,100]}

00:07:07.810 --> 00:07:09.930
<v Speaker 1>But then I would say speak for a while and

NOTE CONF {"raw":[100,100,74,100,100,100,100,100,100,100]}

00:07:09.930 --> 00:07:11.090
<v Speaker 1>then go back to spoke.

NOTE CONF {"raw":[100,100,100,100,98]}

00:07:12.090 --> 00:07:12.330
<v Speaker 1>Right.

NOTE CONF {"raw":[91]}

00:07:12.370 --> 00:07:17.170
<v Speaker 1>And the explanation in a rule based framework is that

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:07:18.010 --> 00:07:22.890
<v Speaker 1>in the first stage, children just remember, just memorise everything.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:07:23.410 --> 00:07:26.370
<v Speaker 1>Say they don't know a lot of past tense forms,

NOTE CONF {"raw":[66,100,100,100,100,100,100,100,100,100]}

00:07:26.650 --> 00:07:28.810
<v Speaker 1>and they just memorise both the regular ones and the

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:07:28.810 --> 00:07:29.650
<v Speaker 1>irregular ones.

NOTE CONF {"raw":[100,100]}

00:07:29.890 --> 00:07:32.770
<v Speaker 1>Then once they see more and more verbs in stage

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:07:32.770 --> 00:07:37.410
<v Speaker 1>two, the error rates go down because they realise the

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,95,100]}

00:07:37.410 --> 00:07:39.610
<v Speaker 1>error rates go up because they realise you can just

NOTE CONF {"raw":[100,100,100,100,100,100,94,100,100,100]}

00:07:39.610 --> 00:07:41.250
<v Speaker 1>stick ed at the end of a verb.

NOTE CONF {"raw":[100,82,100,100,100,100,98,100]}

00:07:41.570 --> 00:07:45.210
<v Speaker 1>And they apply this everywhere they overgeneralise so they assume

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:07:45.210 --> 00:07:45.970
<v Speaker 1>everything is regular.

NOTE CONF {"raw":[100,100,100]}

00:07:45.970 --> 00:07:47.810
<v Speaker 1>We've learned this fancy new rule.

NOTE CONF {"raw":[100,99,100,100,100,100]}

00:07:47.930 --> 00:07:49.410
<v Speaker 1>So we just use it everywhere.

NOTE CONF {"raw":[100,100,100,99,100,100]}

00:07:49.650 --> 00:07:52.390
<v Speaker 1>And this manifests itself as an increased error rate.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:07:52.910 --> 00:07:56.510
<v Speaker 1>Then later on in stage three, they pick up on

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:07:56.510 --> 00:07:59.670
<v Speaker 1>the fact that some verbs do not use the rule

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:08:00.190 --> 00:08:02.670
<v Speaker 1>and they memorise those ones, the irregular ones.

NOTE CONF {"raw":[100,100,77,100,100,100,100,100]}

00:08:03.070 --> 00:08:05.430
<v Speaker 1>But the rule remains and can be used for everything

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:08:05.430 --> 00:08:05.750
<v Speaker 1>else.

NOTE CONF {"raw":[100]}

00:08:05.750 --> 00:08:08.830
<v Speaker 1>And then the error rate goes down again, correctness goes

NOTE CONF {"raw":[100,100,94,100,100,100,100,100,100,100]}

00:08:08.830 --> 00:08:09.110
<v Speaker 1>up.

NOTE CONF {"raw":[100]}

00:08:09.150 --> 00:08:12.630
<v Speaker 1>And so that's sort of the canonical explanation for the

NOTE CONF {"raw":[100,100,92,100,100,100,100,100,100,100]}

00:08:12.630 --> 00:08:13.470
<v Speaker 1>U-shaped curve.

NOTE CONF {"raw":[98,100]}

00:08:14.310 --> 00:08:17.990
<v Speaker 1>And now the neural net guys, they assume they can

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:08:17.990 --> 00:08:22.310
<v Speaker 1>get this curve just through a general learning algorithm.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:08:22.310 --> 00:08:25.630
<v Speaker 1>No rules, no lexicon, no blocking.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:08:25.630 --> 00:08:27.510
<v Speaker 1>None of this is necessary.

NOTE CONF {"raw":[100,100,100,100,100]}

00:08:27.670 --> 00:08:28.550
<v Speaker 1>That's the claim.

NOTE CONF {"raw":[100,100,100]}

00:08:29.390 --> 00:08:29.670
<v Speaker 1>Okay.

NOTE CONF {"raw":[80]}

00:08:29.710 --> 00:08:31.390
<v Speaker 1>And this is the model Y hat.

NOTE CONF {"raw":[100,100,100,100,100,91,91]}

00:08:31.390 --> 00:08:35.310
<v Speaker 1>And McClelland, as I said, goes back to the 80s.

NOTE CONF {"raw":[99,98,100,100,100,100,100,100,100,100]}

00:08:35.349 --> 00:08:38.030
<v Speaker 1>It was part of a new paradigm in psychology or

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:08:38.030 --> 00:08:42.030
<v Speaker 1>psychological modelling at that time called parallel distributed processing, which

NOTE CONF {"raw":[100,93,100,100,100,100,100,100,100,100]}

00:08:42.030 --> 00:08:44.990
<v Speaker 1>we now call deep learning or neural nets, sometimes called

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:08:44.990 --> 00:08:46.870
<v Speaker 1>connectionism, though nobody.

NOTE CONF {"raw":[100,100,89]}

00:08:47.630 --> 00:08:51.160
<v Speaker 1>I don't think a lot of people use that Uh,

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,49]}

00:08:51.200 --> 00:08:51.920
<v Speaker 1>Monica.

NOTE CONF {"raw":[42]}

00:08:52.520 --> 00:08:56.280
<v Speaker 1>Um, and the idea is not only to build a

NOTE CONF {"raw":[48,100,100,100,100,100,100,100,100,100]}

00:08:56.280 --> 00:08:57.680
<v Speaker 1>model that can learn the past tense.

NOTE CONF {"raw":[100,100,100,100,86,100,100]}

00:08:57.680 --> 00:09:00.200
<v Speaker 1>That is maybe not so difficult, but to build a

NOTE CONF {"raw":[100,98,98,100,100,100,100,100,100,100]}

00:09:00.200 --> 00:09:02.560
<v Speaker 1>model that is able to actually.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:09:02.760 --> 00:09:08.040
<v Speaker 1>Sorry, wrong direction that is able to simulate.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:09:10.960 --> 00:09:13.760
<v Speaker 1>Simulate the U-shaped curve that we had here.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:09:15.080 --> 00:09:15.560
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:09:16.720 --> 00:09:22.800
<v Speaker 1>And the general approach is that you build a model

NOTE CONF {"raw":[96,100,100,100,100,100,100,100,100,100]}

00:09:22.800 --> 00:09:23.800
<v Speaker 1>that just does that.

NOTE CONF {"raw":[100,100,100,100]}

00:09:23.800 --> 00:09:26.640
<v Speaker 1>It takes a base form and generates the past tense

NOTE CONF {"raw":[100,100,100,100,100,100,100,80,100,100]}

00:09:26.640 --> 00:09:26.960
<v Speaker 1>form.

NOTE CONF {"raw":[100]}

00:09:27.160 --> 00:09:29.160
<v Speaker 1>It doesn't do language processing in general.

NOTE CONF {"raw":[100,100,100,100,100,77,100]}

00:09:29.160 --> 00:09:34.080
<v Speaker 1>It doesn't do actually, uh, understanding.

NOTE CONF {"raw":[100,100,100,100,62,100]}

00:09:34.080 --> 00:09:35.480
<v Speaker 1>It only does generation.

NOTE CONF {"raw":[100,100,100,100]}

00:09:35.480 --> 00:09:40.120
<v Speaker 1>So it's a very limited, uh, a limited approach that

NOTE CONF {"raw":[100,94,100,100,100,100,92,100,100,100]}

00:09:40.120 --> 00:09:42.840
<v Speaker 1>can only model this particular phenomenon, this particular set of

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:09:42.840 --> 00:09:45.680
<v Speaker 1>data, which is quite common in cognitive modelling.

NOTE CONF {"raw":[100,100,100,100,100,100,100,90]}

00:09:45.840 --> 00:09:50.100
<v Speaker 1>You don't build a full model of, uh, even one

NOTE CONF {"raw":[100,100,100,100,100,100,100,81,100,100]}

00:09:50.100 --> 00:09:52.580
<v Speaker 1>cognitive faculty like language processing.

NOTE CONF {"raw":[100,100,100,100,100]}

00:09:52.580 --> 00:09:55.260
<v Speaker 1>But you build a model of a particular phenomenon and

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:09:55.260 --> 00:09:58.660
<v Speaker 1>you want to capture a particular data just because it's

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:09:58.660 --> 00:10:01.380
<v Speaker 1>so hard to try to model everything at once.

NOTE CONF {"raw":[78,100,100,100,100,100,100,100,100]}

00:10:02.340 --> 00:10:05.420
<v Speaker 1>And more specifically, what we do, we take a phonological

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:10:05.420 --> 00:10:06.140
<v Speaker 1>representation.

NOTE CONF {"raw":[100]}

00:10:06.140 --> 00:10:09.180
<v Speaker 1>So representation in terms of the sounds of the stem

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:10:09.220 --> 00:10:10.140
<v Speaker 1>of the base form.

NOTE CONF {"raw":[100,100,91,100]}

00:10:10.340 --> 00:10:13.700
<v Speaker 1>And we output a phonological representation of the correct past

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:10:13.700 --> 00:10:14.140
<v Speaker 1>tense.

NOTE CONF {"raw":[100]}

00:10:14.860 --> 00:10:18.660
<v Speaker 1>And then you can test this with new verbs, with

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:10:18.660 --> 00:10:20.540
<v Speaker 1>verbs that the model hasn't seen before.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:10:20.980 --> 00:10:23.220
<v Speaker 1>Obviously you give it some verbs with the correct past

NOTE CONF {"raw":[100,100,100,100,100,100,100,90,100,100]}

00:10:23.220 --> 00:10:26.460
<v Speaker 1>tense during training, but you can test it then with

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:10:26.460 --> 00:10:28.380
<v Speaker 1>new verbs that you haven't seen before and should be

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:10:28.380 --> 00:10:32.660
<v Speaker 1>able to generalise to those new verbs to generate the

NOTE CONF {"raw":[100,100,100,100,100,100,100,73,100,100]}

00:10:32.660 --> 00:10:33.860
<v Speaker 1>correct past tense form.

NOTE CONF {"raw":[100,100,100,100]}

00:10:34.700 --> 00:10:35.460
<v Speaker 1>Okay, so.

NOTE CONF {"raw":[100,100]}

00:10:38.540 --> 00:10:41.860
<v Speaker 1>In this, in this model there's no lexicon, no rules.

NOTE CONF {"raw":[100,100,100,100,100,73,100,100,92,100]}

00:10:41.860 --> 00:10:46.020
<v Speaker 1>As I've already said, it is a two level fully

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:10:46.020 --> 00:10:50.110
<v Speaker 1>connected feedforward perceptron network very similar to the network we've

NOTE CONF {"raw":[100,100,95,100,100,100,100,100,100,100]}

00:10:50.110 --> 00:10:51.230
<v Speaker 1>seen in The Last Lecture.

NOTE CONF {"raw":[100,100,100,100,100]}

00:10:52.430 --> 00:10:55.110
<v Speaker 1>The initial version didn't even use hidden units, just had

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:10:55.110 --> 00:10:57.790
<v Speaker 1>an output layer and an input layer and still worked.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:10:58.790 --> 00:11:01.950
<v Speaker 1>Later versions were more fancy, added hidden units, and so

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:11:01.950 --> 00:11:02.190
<v Speaker 1>on.

NOTE CONF {"raw":[100]}

00:11:03.590 --> 00:11:04.870
<v Speaker 1>As I said, we're taking.

NOTE CONF {"raw":[100,100,100,100,100]}

00:11:05.110 --> 00:11:09.950
<v Speaker 1>So I'm using these slashes here to indicate the phonological

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:11:09.950 --> 00:11:10.350
<v Speaker 1>form.

NOTE CONF {"raw":[100]}

00:11:10.590 --> 00:11:12.630
<v Speaker 1>So that would be dance and sync.

NOTE CONF {"raw":[100,51,100,100,100,100,68]}

00:11:13.110 --> 00:11:15.710
<v Speaker 1>And then the past dance form would be danced and

NOTE CONF {"raw":[100,100,100,96,100,100,100,100,100,100]}

00:11:15.710 --> 00:11:16.110
<v Speaker 1>sang.

NOTE CONF {"raw":[65]}

00:11:16.950 --> 00:11:19.390
<v Speaker 1>Okay so this is the what goes in as the

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:11:19.390 --> 00:11:19.870
<v Speaker 1>input.

NOTE CONF {"raw":[100]}

00:11:20.110 --> 00:11:21.990
<v Speaker 1>And this is what comes out as the output.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:11:22.030 --> 00:11:23.190
<v Speaker 1>So base form goes in.

NOTE CONF {"raw":[100,94,100,100,100]}

00:11:23.550 --> 00:11:25.870
<v Speaker 1>We have the neural network doing the magic.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:11:25.950 --> 00:11:29.670
<v Speaker 1>And then it produces the past tense form of the

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:11:29.670 --> 00:11:31.190
<v Speaker 1>verb okay.

NOTE CONF {"raw":[100,85]}

00:11:31.230 --> 00:11:34.190
<v Speaker 1>And now the question is of course how do we

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:11:34.190 --> 00:11:36.510
<v Speaker 1>get these verbs into the model.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:11:37.190 --> 00:11:37.430
<v Speaker 1>Right.

NOTE CONF {"raw":[92]}

00:11:37.470 --> 00:11:40.910
<v Speaker 1>Because the model as we saw last time, just takes

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:11:40.910 --> 00:11:42.710
<v Speaker 1>in numbers and outputs numbers.

NOTE CONF {"raw":[100,100,100,100,100]}

00:11:42.910 --> 00:11:43.110
<v Speaker 1>Right.

NOTE CONF {"raw":[76]}

00:11:43.110 --> 00:11:43.870
<v Speaker 1>It doesn't.

NOTE CONF {"raw":[100,100]}

00:11:44.470 --> 00:11:45.390
<v Speaker 1>It's not there.

NOTE CONF {"raw":[100,100,100]}

00:11:46.630 --> 00:11:51.240
<v Speaker 1>It doesn't process letters or words or linguistic input or

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:11:51.240 --> 00:11:52.560
<v Speaker 1>speech or anything like that.

NOTE CONF {"raw":[100,100,100,100,100]}

00:11:53.080 --> 00:11:56.240
<v Speaker 1>It's a vector of numbers, goes in vector numbers.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,96]}

00:11:56.280 --> 00:11:57.320
<v Speaker 1>Numbers comes out.

NOTE CONF {"raw":[100,100,100]}

00:11:57.680 --> 00:12:01.560
<v Speaker 1>So we now need to represent our verbs as a

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:12:01.560 --> 00:12:03.600
<v Speaker 1>sequence of numbers, as a vector of numbers that we

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:12:03.600 --> 00:12:04.960
<v Speaker 1>can input into the neural net.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:12:05.080 --> 00:12:08.160
<v Speaker 1>And this is in any sort of neural network modelling.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,96]}

00:12:08.160 --> 00:12:10.880
<v Speaker 1>That's part of the problem designing the input and output

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:12:10.880 --> 00:12:11.840
<v Speaker 1>representations.

NOTE CONF {"raw":[100]}

00:12:12.400 --> 00:12:12.640
<v Speaker 1>Right.

NOTE CONF {"raw":[98]}

00:12:12.680 --> 00:12:16.600
<v Speaker 1>Because it's a neural network is basically computing a function

NOTE CONF {"raw":[100,100,72,100,100,100,100,100,100,100]}

00:12:16.600 --> 00:12:18.600
<v Speaker 1>on vectors from vector to vector.

NOTE CONF {"raw":[99,64,100,100,100,100]}

00:12:19.000 --> 00:12:22.120
<v Speaker 1>So we need to design what these vectors look like.

NOTE CONF {"raw":[100,100,100,100,95,100,100,100,100,100]}

00:12:22.960 --> 00:12:26.520
<v Speaker 1>And so I will spend quite a lot of time

NOTE CONF {"raw":[100,100,100,77,100,100,100,100,100,100]}

00:12:26.520 --> 00:12:28.760
<v Speaker 1>on discussing these feature representations.

NOTE CONF {"raw":[82,100,100,100,100]}

00:12:30.600 --> 00:12:30.960
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:12:31.000 --> 00:12:32.880
<v Speaker 1>So as I said the design of the features of

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:12:32.880 --> 00:12:35.200
<v Speaker 1>the input and output representation is really important.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:12:35.200 --> 00:12:38.560
<v Speaker 1>And that's still true for modern neural networks right.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,85]}

00:12:38.600 --> 00:12:41.320
<v Speaker 1>Modern neural networks, they use something called word embeddings when

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:12:41.320 --> 00:12:43.320
<v Speaker 1>you use them for language processing.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:12:43.800 --> 00:12:45.680
<v Speaker 1>We'll talk about those actually next week.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:12:46.860 --> 00:12:49.900
<v Speaker 1>They are more advanced and more complicated than the simple

NOTE CONF {"raw":[52,56,100,100,100,100,100,100,100,100]}

00:12:50.020 --> 00:12:51.700
<v Speaker 1>representations we'll discuss today.

NOTE CONF {"raw":[100,81,100,100]}

00:12:51.980 --> 00:12:55.180
<v Speaker 1>But the question of representation is is still a crucial

NOTE CONF {"raw":[100,100,100,100,100,78,100,100,100,100]}

00:12:55.180 --> 00:12:55.540
<v Speaker 1>one.

NOTE CONF {"raw":[100]}

00:12:56.380 --> 00:12:56.740
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:12:56.780 --> 00:13:03.140
<v Speaker 1>So Raymond McClelland assumed that you have a you represent

NOTE CONF {"raw":[100,85,90,92,100,100,100,100,100,100]}

00:13:03.140 --> 00:13:06.100
<v Speaker 1>a word like came, for example, as a sequence of

NOTE CONF {"raw":[100,100,100,97,100,100,100,100,100,100]}

00:13:06.100 --> 00:13:06.500
<v Speaker 1>phonemes.

NOTE CONF {"raw":[100]}

00:13:06.500 --> 00:13:09.060
<v Speaker 1>So this would be a and m.

NOTE CONF {"raw":[100,100,100,100,100,98,48]}

00:13:09.740 --> 00:13:13.780
<v Speaker 1>And these things are sort of I don't know.

NOTE CONF {"raw":[99,100,100,100,100,100,100,100,100]}

00:13:13.820 --> 00:13:16.460
<v Speaker 1>If you've studied some linguistics then you'll know about the,

NOTE CONF {"raw":[100,99,100,100,100,100,100,100,100,100]}

00:13:16.460 --> 00:13:18.620
<v Speaker 1>the IPA, the International Phonetic Alphabet.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:13:18.900 --> 00:13:24.140
<v Speaker 1>These are sort of Ascii representations of IPA, and they

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:13:24.140 --> 00:13:28.620
<v Speaker 1>assume 35 phonemes, which is enough to represent all the

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:13:28.620 --> 00:13:29.460
<v Speaker 1>sounds in English.

NOTE CONF {"raw":[100,100,100]}

00:13:29.900 --> 00:13:31.860
<v Speaker 1>Note that there's more phonemes than letters.

NOTE CONF {"raw":[61,100,100,100,100,100,100]}

00:13:34.300 --> 00:13:38.700
<v Speaker 1>And now we need to represent the context as well.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:13:38.780 --> 00:13:41.380
<v Speaker 1>So we cannot just say okay, here we have a

NOTE CONF {"raw":[100,100,89,100,100,100,100,100,100,76]}

00:13:41.700 --> 00:13:43.940
<v Speaker 1>phoneme and a phoneme and so on.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:13:44.660 --> 00:13:48.910
<v Speaker 1>Uh, we need to represent what comes before and after

NOTE CONF {"raw":[64,100,100,100,100,100,100,100,100,100]}

00:13:48.910 --> 00:13:49.630
<v Speaker 1>that phoneme.

NOTE CONF {"raw":[100,100]}

00:13:49.990 --> 00:13:51.230
<v Speaker 1>Why is that important?

NOTE CONF {"raw":[100,100,100,100]}

00:13:51.310 --> 00:13:56.350
<v Speaker 1>Well, because verbs behave differently based on their context.

NOTE CONF {"raw":[100,100,100,100,100,100,100,77,100]}

00:13:56.350 --> 00:14:00.310
<v Speaker 1>So for example, there's patterns like sing sang, ring rang,

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:14:00.310 --> 00:14:01.150
<v Speaker 1>springs rang.

NOTE CONF {"raw":[51,88]}

00:14:01.630 --> 00:14:05.910
<v Speaker 1>Where if you have this, if you have an eye

NOTE CONF {"raw":[95,100,100,100,100,100,100,100,100,65]}

00:14:06.070 --> 00:14:08.590
<v Speaker 1>and phoneme.

NOTE CONF {"raw":[100,100]}

00:14:08.630 --> 00:14:08.910
<v Speaker 1>Right.

NOTE CONF {"raw":[98]}

00:14:08.950 --> 00:14:11.630
<v Speaker 1>So like in sing you have the e and then

NOTE CONF {"raw":[100,99,100,92,100,100,100,88,100,100]}

00:14:11.630 --> 00:14:11.950
<v Speaker 1>you have.

NOTE CONF {"raw":[97,60]}

00:14:12.790 --> 00:14:15.310
<v Speaker 1>And if you have that then you have this pattern

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:14:15.310 --> 00:14:15.670
<v Speaker 1>here.

NOTE CONF {"raw":[100]}

00:14:15.870 --> 00:14:17.270
<v Speaker 1>Sing sang, ring rung and so on.

NOTE CONF {"raw":[84,95,100,58,100,100,100]}

00:14:17.270 --> 00:14:18.630
<v Speaker 1>This is common to all of these.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:14:19.150 --> 00:14:21.230
<v Speaker 1>And we want the model to be able to represent

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:14:21.230 --> 00:14:22.230
<v Speaker 1>that right.

NOTE CONF {"raw":[100,97]}

00:14:22.270 --> 00:14:25.030
<v Speaker 1>So individual phonemes would not be sufficient.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:14:25.350 --> 00:14:27.110
<v Speaker 1>So we encode the context.

NOTE CONF {"raw":[100,100,100,100,100]}

00:14:28.670 --> 00:14:31.830
<v Speaker 1>And that also gives us information about the relative position

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:14:31.830 --> 00:14:32.990
<v Speaker 1>of the phoneme.

NOTE CONF {"raw":[100,100,100]}

00:14:33.670 --> 00:14:37.190
<v Speaker 1>So the the authors call this with phones.

NOTE CONF {"raw":[100,100,100,100,100,70,33,100]}

00:14:37.670 --> 00:14:40.230
<v Speaker 1>This is named after an author called Winkle gren who

NOTE CONF {"raw":[100,100,100,100,100,100,100,18,63,100]}

00:14:40.230 --> 00:14:42.790
<v Speaker 1>invented that representation, but doesn't matter.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:14:43.510 --> 00:14:46.400
<v Speaker 1>So sing things would become this, right?

NOTE CONF {"raw":[100,100,47,100,100,100,90]}

00:14:46.440 --> 00:14:48.280
<v Speaker 1>Or came would become that.

NOTE CONF {"raw":[100,93,94,100,100]}

00:14:48.800 --> 00:14:53.440
<v Speaker 1>And then in terms of triples or phonemes, this would

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:14:53.440 --> 00:15:00.600
<v Speaker 1>be hash s I s I I n I mean

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,93]}

00:15:00.640 --> 00:15:04.600
<v Speaker 1>hash and hash is the word boundary beginning or end

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:15:04.640 --> 00:15:06.560
<v Speaker 1>of the word okay.

NOTE CONF {"raw":[100,100,91,71]}

00:15:06.600 --> 00:15:11.880
<v Speaker 1>So we know here that there's two phones, two phonemes

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:15:11.880 --> 00:15:15.440
<v Speaker 1>involved, and they are at the beginning of the word

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,98]}

00:15:15.680 --> 00:15:16.280
<v Speaker 1>here.

NOTE CONF {"raw":[96]}

00:15:16.320 --> 00:15:21.080
<v Speaker 1>These three phones phonemes are also involved.

NOTE CONF {"raw":[100,100,76,100,100,100,100]}

00:15:21.080 --> 00:15:22.560
<v Speaker 1>And then in the middle of the word and so

NOTE CONF {"raw":[57,32,98,100,100,100,100,94,100,100]}

00:15:22.560 --> 00:15:22.800
<v Speaker 1>on.

NOTE CONF {"raw":[100]}

00:15:22.880 --> 00:15:26.960
<v Speaker 1>So we we take our word represented as phonemes and

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:15:26.960 --> 00:15:29.200
<v Speaker 1>then we take all possible triples.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:15:30.160 --> 00:15:30.480
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:15:30.520 --> 00:15:33.680
<v Speaker 1>So that's a very simple representation.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:15:34.160 --> 00:15:37.280
<v Speaker 1>Now we need to figure out how to encode those

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:15:37.280 --> 00:15:37.920
<v Speaker 1>triples.

NOTE CONF {"raw":[100]}

00:15:38.560 --> 00:15:41.120
<v Speaker 1>And then we're basically in a position where we can

NOTE CONF {"raw":[100,100,73,100,100,100,100,100,100,100]}

00:15:41.360 --> 00:15:43.980
<v Speaker 1>um where we can train the network.

NOTE CONF {"raw":[49,100,100,100,100,100,100]}

00:15:52.420 --> 00:15:56.540
<v Speaker 1>So for example, using this representation we can represent that

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:15:56.820 --> 00:15:58.060
<v Speaker 1>sing becomes sang.

NOTE CONF {"raw":[99,85,81]}

00:15:58.540 --> 00:16:03.420
<v Speaker 1>So I n end of word becomes an end of

NOTE CONF {"raw":[100,36,27,86,100,100,100,100,100,100]}

00:16:03.420 --> 00:16:07.140
<v Speaker 1>word independent of word length.

NOTE CONF {"raw":[100,100,100,100,100]}

00:16:07.380 --> 00:16:10.700
<v Speaker 1>Remember we're mapping the input onto the output right.

NOTE CONF {"raw":[100,92,100,100,100,100,100,100,98]}

00:16:10.740 --> 00:16:13.620
<v Speaker 1>So if we have this representation then we don't care

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:16:13.620 --> 00:16:16.500
<v Speaker 1>what's preceding this.

NOTE CONF {"raw":[100,100,100]}

00:16:16.860 --> 00:16:20.500
<v Speaker 1>We can still learn this mapping from in to an

NOTE CONF {"raw":[100,100,100,100,100,100,100,56,100,100]}

00:16:21.420 --> 00:16:21.900
<v Speaker 1>okay.

NOTE CONF {"raw":[100]}

00:16:22.100 --> 00:16:24.540
<v Speaker 1>So that's what we want the model to to to

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:16:24.580 --> 00:16:24.940
<v Speaker 1>learn.

NOTE CONF {"raw":[100]}

00:16:25.020 --> 00:16:27.380
<v Speaker 1>So we need to enable it to represent that.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:16:30.180 --> 00:16:30.540
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:16:30.580 --> 00:16:36.420
<v Speaker 1>So now we have these 35 phonemes and they're in

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,95,100]}

00:16:36.420 --> 00:16:36.980
<v Speaker 1>triples.

NOTE CONF {"raw":[100]}

00:16:37.180 --> 00:16:42.060
<v Speaker 1>So we have 35 to the power of three possibilities.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:16:42.830 --> 00:16:43.150
<v Speaker 1>right?

NOTE CONF {"raw":[93]}

00:16:43.190 --> 00:16:49.190
<v Speaker 1>So if we take all, all possible triples like this

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:16:50.310 --> 00:16:55.950
<v Speaker 1>over our 35 phonemes, and this is about 40,000 possibilities,

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:16:56.550 --> 00:16:59.390
<v Speaker 1>which is fairly long for an input.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:16:59.430 --> 00:17:00.950
<v Speaker 1>Right now we have an input.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:17:01.470 --> 00:17:03.990
<v Speaker 1>In our simple examples we only had 2 or 3

NOTE CONF {"raw":[100,100,100,100,100,100,97,100,100,100]}

00:17:04.030 --> 00:17:04.750
<v Speaker 1>input units.

NOTE CONF {"raw":[100,100]}

00:17:04.790 --> 00:17:06.390
<v Speaker 1>Now we need 40,000.

NOTE CONF {"raw":[100,100,100,100]}

00:17:07.069 --> 00:17:10.470
<v Speaker 1>And for today's technology that's not really a problem.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:17:10.470 --> 00:17:12.310
<v Speaker 1>We can we can have very long inputs.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:17:12.310 --> 00:17:15.670
<v Speaker 1>But back in the 80s they weren't really able to

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:17:15.709 --> 00:17:16.350
<v Speaker 1>train this.

NOTE CONF {"raw":[100,100]}

00:17:16.790 --> 00:17:21.510
<v Speaker 1>So they opted for a simpler representation where they took

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:17:21.510 --> 00:17:22.870
<v Speaker 1>phonological features.

NOTE CONF {"raw":[100,100]}

00:17:23.470 --> 00:17:28.470
<v Speaker 1>So for example k, the phoneme k can be represented

NOTE CONF {"raw":[100,100,100,95,100,100,100,100,100,100]}

00:17:28.470 --> 00:17:31.070
<v Speaker 1>as interrupted because it's a plosive.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:17:31.510 --> 00:17:34.150
<v Speaker 1>It's at the articulated at the back of the mouth.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:17:34.150 --> 00:17:38.830
<v Speaker 1>It's a stop like stops and it's unvoiced.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:17:38.990 --> 00:17:41.680
<v Speaker 1>Good would be the voiced version, right?

NOTE CONF {"raw":[87,98,100,100,100,100,90]}

00:17:41.720 --> 00:17:42.760
<v Speaker 1>So linguists.

NOTE CONF {"raw":[100,100]}

00:17:42.760 --> 00:17:43.200
<v Speaker 1>Phonology.

NOTE CONF {"raw":[99]}

00:17:43.360 --> 00:17:46.760
<v Speaker 1>They thought of ways of representing phonemes in terms of

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:17:46.760 --> 00:17:47.560
<v Speaker 1>these features.

NOTE CONF {"raw":[100,100]}

00:17:48.520 --> 00:17:54.760
<v Speaker 1>So uh, and using features like that for, for uh

NOTE CONF {"raw":[99,96,100,100,100,100,100,97,100,65]}

00:17:54.800 --> 00:17:57.320
<v Speaker 1>consonants this would be the feature representation for k.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,96]}

00:17:57.760 --> 00:17:59.760
<v Speaker 1>This would be the feature representation for a.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:17:59.800 --> 00:18:01.400
<v Speaker 1>This would be the feature representation for.

NOTE CONF {"raw":[100,100,100,100,100,100,99]}

00:18:01.440 --> 00:18:01.760
<v Speaker 1>Mhm.

NOTE CONF {"raw":[50]}

00:18:02.840 --> 00:18:06.520
<v Speaker 1>Uh we can get a simpler and smaller set of

NOTE CONF {"raw":[90,100,100,100,99,100,100,100,100,100]}

00:18:07.200 --> 00:18:07.840
<v Speaker 1>features.

NOTE CONF {"raw":[100]}

00:18:08.440 --> 00:18:12.120
<v Speaker 1>So this gives us 11 binary features.

NOTE CONF {"raw":[93,100,100,100,100,100,100]}

00:18:12.640 --> 00:18:14.920
<v Speaker 1>So which we can represent as zero and one.

NOTE CONF {"raw":[100,100,100,100,100,91,100,100,100]}

00:18:16.040 --> 00:18:18.560
<v Speaker 1>Uh so and these are now called whittle features rather

NOTE CONF {"raw":[70,100,97,100,100,100,100,100,100,100]}

00:18:18.560 --> 00:18:19.560
<v Speaker 1>than whittle phones.

NOTE CONF {"raw":[100,87,100]}

00:18:20.200 --> 00:18:23.680
<v Speaker 1>And now we have 11 to the third uh possible

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,96,100]}

00:18:23.680 --> 00:18:24.640
<v Speaker 1>combinations.

NOTE CONF {"raw":[100]}

00:18:25.080 --> 00:18:28.320
<v Speaker 1>There's some redundancy and there are some combinations that can't

NOTE CONF {"raw":[100,100,100,100,88,88,100,100,100,100]}

00:18:28.320 --> 00:18:28.760
<v Speaker 1>occur.

NOTE CONF {"raw":[100]}

00:18:29.000 --> 00:18:32.160
<v Speaker 1>So we take those away and we're left with 460.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:18:33.280 --> 00:18:37.080
<v Speaker 1>So now we only need 460 input units.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:18:38.320 --> 00:18:41.980
<v Speaker 1>Um which makes the network a lot more compact and

NOTE CONF {"raw":[82,100,100,100,100,100,100,100,100,100]}

00:18:41.980 --> 00:18:42.820
<v Speaker 1>easier to train.

NOTE CONF {"raw":[100,100,100]}

00:18:46.260 --> 00:18:50.020
<v Speaker 1>Okay, so at this point, yeah.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:18:50.020 --> 00:18:52.740
<v Speaker 1>So 460 input units.

NOTE CONF {"raw":[100,100,100,100]}

00:18:53.540 --> 00:18:56.460
<v Speaker 1>And these are the same as the output units.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:18:56.500 --> 00:18:56.700
<v Speaker 1>Right.

NOTE CONF {"raw":[100]}

00:18:56.700 --> 00:19:00.020
<v Speaker 1>Because the input and the output are both verbs represented

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:19:00.020 --> 00:19:02.460
<v Speaker 1>as phonological features.

NOTE CONF {"raw":[100,100,100]}

00:19:03.500 --> 00:19:08.660
<v Speaker 1>So for a given word we activate all the features

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:19:08.660 --> 00:19:10.100
<v Speaker 1>in the phonemes together.

NOTE CONF {"raw":[100,100,100,100]}

00:19:10.660 --> 00:19:13.420
<v Speaker 1>So thing would activate those features here.

NOTE CONF {"raw":[100,39,100,100,100,100,100]}

00:19:14.060 --> 00:19:18.940
<v Speaker 1>Beginning of word c s I, n in end of

NOTE CONF {"raw":[100,100,100,49,44,44,42,100,96,100]}

00:19:18.940 --> 00:19:24.020
<v Speaker 1>word, and the other phonemes are not being represented, and

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:19:24.020 --> 00:19:26.580
<v Speaker 1>the order of the phonemes is not being represented.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:19:27.100 --> 00:19:29.140
<v Speaker 1>We still get a reasonably large network, right?

NOTE CONF {"raw":[100,100,90,100,100,100,100,100]}

00:19:29.180 --> 00:19:31.980
<v Speaker 1>We have 460 input units, 460 output units.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:19:32.180 --> 00:19:36.420
<v Speaker 1>So this gives about 200,000 connections weights that we will

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:19:36.420 --> 00:19:37.100
<v Speaker 1>need to learn.

NOTE CONF {"raw":[100,100,100]}

00:19:38.220 --> 00:19:39.830
<v Speaker 1>And we are not assuming a hidden layer.

NOTE CONF {"raw":[91,94,94,100,100,100,100,100]}

00:19:40.230 --> 00:19:42.550
<v Speaker 1>We start the training by setting them all to zero,

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:19:44.110 --> 00:19:49.270
<v Speaker 1>and the authors have a set of 200 sorry, 420

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:19:49.310 --> 00:19:50.510
<v Speaker 1>input output pairs.

NOTE CONF {"raw":[100,100,100]}

00:19:51.070 --> 00:19:51.590
<v Speaker 1>Right?

NOTE CONF {"raw":[82]}

00:19:51.750 --> 00:19:55.910
<v Speaker 1>Pairs like sing, sang, walk walked where the model knows

NOTE CONF {"raw":[100,100,100,100,100,89,100,100,100,100]}

00:19:55.910 --> 00:19:56.870
<v Speaker 1>the correct answer.

NOTE CONF {"raw":[100,100,100]}

00:19:57.590 --> 00:20:00.790
<v Speaker 1>Remember backpropagation is is a supervised learning algorithm.

NOTE CONF {"raw":[100,50,100,100,100,100,100,100]}

00:20:00.790 --> 00:20:03.270
<v Speaker 1>So during training we need to know the correct answer.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:20:03.630 --> 00:20:06.030
<v Speaker 1>And if the model doesn't generate the correct answer, then

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:20:06.030 --> 00:20:08.950
<v Speaker 1>this gives us an error signal which we propagate into

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:20:09.630 --> 00:20:11.830
<v Speaker 1>the network to adjust the weights.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:20:12.670 --> 00:20:13.990
<v Speaker 1>That's how the training works.

NOTE CONF {"raw":[100,100,100,100,100]}

00:20:15.630 --> 00:20:15.990
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:20:16.110 --> 00:20:19.470
<v Speaker 1>So let's try to get our heads around the features.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:20:19.470 --> 00:20:22.630
<v Speaker 1>And this should work.

NOTE CONF {"raw":[100,100,100,100]}

00:20:22.710 --> 00:20:26.590
<v Speaker 1>Even though you cannot see it on the screen it

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:20:26.590 --> 00:20:27.710
<v Speaker 1>should work on your phones.

NOTE CONF {"raw":[100,100,100,100,100]}

00:20:28.470 --> 00:20:30.590
<v Speaker 1>So if you could try this for a second.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,64]}

00:20:35.950 --> 00:20:38.960
<v Speaker 1>And then you should see the first question.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:20:46.920 --> 00:20:47.160
<v Speaker 0>Yes, sir.

NOTE CONF {"raw":[88,15]}

00:20:51.400 --> 00:20:51.680
<v Speaker 0>Yes.

NOTE CONF {"raw":[100]}

00:20:55.560 --> 00:20:55.760
<v Speaker 0>Sir.

NOTE CONF {"raw":[33]}

00:20:58.760 --> 00:20:59.320
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:21:00.040 --> 00:21:00.880
<v Speaker 1>And.

NOTE CONF {"raw":[100]}

00:21:03.520 --> 00:21:05.160
<v Speaker 1>Hope people can see the question.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:21:06.000 --> 00:21:07.840
<v Speaker 1>I cannot display it because the.

NOTE CONF {"raw":[100,98,100,100,100,100]}

00:21:09.080 --> 00:21:11.800
<v Speaker 1>This device doesn't have a network and that device doesn't

NOTE CONF {"raw":[100,100,99,99,60,90,100,100,100,100]}

00:21:11.800 --> 00:21:13.120
<v Speaker 1>connect to the projectors.

NOTE CONF {"raw":[100,100,100,91]}

00:21:16.400 --> 00:21:16.720
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:21:16.760 --> 00:21:20.640
<v Speaker 1>So the question is we're taking a verb spring, and

NOTE CONF {"raw":[100,100,100,100,97,100,100,100,100,100]}

00:21:20.640 --> 00:21:23.120
<v Speaker 1>we have the phonological representation for that.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:21:23.640 --> 00:21:27.760
<v Speaker 1>And then you're supposed to break this down into, uh,

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,76]}

00:21:27.800 --> 00:21:29.240
<v Speaker 1>into the wiggle phones.

NOTE CONF {"raw":[100,100,39,100]}

00:21:31.840 --> 00:21:33.360
<v Speaker 1>So there's one correct answer here.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:21:33.480 --> 00:21:34.600
<v Speaker 1>One two 3 or 4.

NOTE CONF {"raw":[100,100,99,99,99]}

00:22:07.420 --> 00:22:07.620
<v Speaker 0>It's.

NOTE CONF {"raw":[58]}

00:22:11.900 --> 00:22:12.580
<v Speaker 1>Okay.

NOTE CONF {"raw":[82]}

00:22:13.100 --> 00:22:14.900
<v Speaker 1>I'll give you another 30s.

NOTE CONF {"raw":[100,100,100,100,100]}

00:22:21.380 --> 00:22:22.620
<v Speaker 1>Let's see what people think.

NOTE CONF {"raw":[100,100,100,100,100]}

00:22:24.220 --> 00:22:26.900
<v Speaker 1>Can you actually see the results now on your screens?

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:22:29.580 --> 00:22:29.980
<v Speaker 1>Yes.

NOTE CONF {"raw":[100]}

00:22:29.980 --> 00:22:31.300
<v Speaker 1>No, no.

NOTE CONF {"raw":[100,100]}

00:22:33.020 --> 00:22:34.260
<v Speaker 1>Okay, I'll read it out.

NOTE CONF {"raw":[100,100,100,100,100]}

00:22:34.620 --> 00:22:37.230
<v Speaker 1>18% have opted for number one.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:22:37.590 --> 00:22:39.390
<v Speaker 1>56% for number two.

NOTE CONF {"raw":[100,100,100,100]}

00:22:40.310 --> 00:22:43.590
<v Speaker 1>25% for number three and 0% for number four.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:22:44.510 --> 00:22:48.310
<v Speaker 1>And the correct answer is number two.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:22:49.310 --> 00:22:49.710
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:22:49.750 --> 00:22:52.470
<v Speaker 1>So let's quickly look at this.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:22:52.470 --> 00:22:53.470
<v Speaker 1>So number one.

NOTE CONF {"raw":[100,100,100]}

00:22:53.790 --> 00:22:56.310
<v Speaker 1>Uh what's wrong here for number one.

NOTE CONF {"raw":[64,100,100,100,100,100,100]}

00:23:00.550 --> 00:23:04.670
<v Speaker 1>Any anything wrong here with the first solution.

NOTE CONF {"raw":[35,100,100,100,100,100,100,100]}

00:23:06.830 --> 00:23:07.390
<v Speaker 1>Exactly.

NOTE CONF {"raw":[100]}

00:23:07.390 --> 00:23:10.430
<v Speaker 1>So number one these are obviously this is a representation

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,79,100]}

00:23:10.430 --> 00:23:13.110
<v Speaker 1>you could choose, but it's not the one that the

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:23:13.110 --> 00:23:14.270
<v Speaker 1>authors have chosen here.

NOTE CONF {"raw":[100,100,100,100]}

00:23:14.550 --> 00:23:20.070
<v Speaker 1>It's using sequences of four phonemes rather than three.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:23:20.470 --> 00:23:20.710
<v Speaker 1>Right.

NOTE CONF {"raw":[78]}

00:23:20.750 --> 00:23:24.030
<v Speaker 1>So it's a four grammar rather than a trigram representation.

NOTE CONF {"raw":[100,100,100,93,83,100,100,100,100,100]}

00:23:24.150 --> 00:23:27.750
<v Speaker 1>Then two was the correct one, where we are chopping

NOTE CONF {"raw":[84,100,100,100,100,100,100,95,95,100]}

00:23:27.750 --> 00:23:32.670
<v Speaker 1>it up so that we have each phoneme represented and

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:23:32.670 --> 00:23:34.850
<v Speaker 1>the context of that phoneme, right?

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:23:34.890 --> 00:23:37.050
<v Speaker 1>The previous one, and that includes the beginning and the

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:23:37.050 --> 00:23:39.130
<v Speaker 1>end of the word and the following one.

NOTE CONF {"raw":[100,100,100,81,100,100,100,100]}

00:23:40.330 --> 00:23:41.650
<v Speaker 1>And what's wrong with three?

NOTE CONF {"raw":[100,100,100,100,100]}

00:23:43.050 --> 00:23:44.290
<v Speaker 1>Why is that not correct?

NOTE CONF {"raw":[100,100,100,100,100]}

00:23:46.370 --> 00:23:49.570
<v Speaker 1>We've forgotten the beginning and end of the word.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:23:49.970 --> 00:23:51.570
<v Speaker 1>Why is that even important?

NOTE CONF {"raw":[100,100,100,100,100]}

00:23:51.730 --> 00:23:53.690
<v Speaker 1>Why do we want to represent the end of the

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:23:53.690 --> 00:23:55.010
<v Speaker 1>word in the beginning of the word?

NOTE CONF {"raw":[67,92,100,100,100,100,100]}

00:23:59.050 --> 00:24:03.650
<v Speaker 1>Well, because certain things happen at syllable boundaries and at

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,94]}

00:24:03.650 --> 00:24:04.370
<v Speaker 1>word boundaries.

NOTE CONF {"raw":[100,100]}

00:24:04.410 --> 00:24:04.570
<v Speaker 1>Right?

NOTE CONF {"raw":[97]}

00:24:04.610 --> 00:24:06.090
<v Speaker 1>Certain linguistic processes.

NOTE CONF {"raw":[100,100,100]}

00:24:07.330 --> 00:24:11.810
<v Speaker 1>And for example, the thing in spring in English, that

NOTE CONF {"raw":[100,100,100,95,54,100,100,100,100,100]}

00:24:11.810 --> 00:24:13.810
<v Speaker 1>really can only happen at the end of the word.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,91]}

00:24:13.850 --> 00:24:15.970
<v Speaker 1>It's not something that's can be in the middle of

NOTE CONF {"raw":[100,100,100,68,100,100,100,100,100,100]}

00:24:15.970 --> 00:24:16.370
<v Speaker 1>the word.

NOTE CONF {"raw":[100,96]}

00:24:16.450 --> 00:24:19.170
<v Speaker 1>So we want the model to be able to you

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:24:19.170 --> 00:24:20.770
<v Speaker 1>know, it's only sees numbers, right?

NOTE CONF {"raw":[100,99,100,77,100,98]}

00:24:20.810 --> 00:24:23.930
<v Speaker 1>So we want the model to know that this is

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:24:23.930 --> 00:24:24.770
<v Speaker 1>an end of the word.

NOTE CONF {"raw":[68,100,100,100,84]}

00:24:25.010 --> 00:24:27.370
<v Speaker 1>And then for finally it's just a representation of the

NOTE CONF {"raw":[100,95,63,100,86,100,82,100,100,100]}

00:24:27.370 --> 00:24:28.410
<v Speaker 1>individual phonemes.

NOTE CONF {"raw":[100,100]}

00:24:28.450 --> 00:24:31.810
<v Speaker 1>It doesn't use triples or phonemes.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:24:32.980 --> 00:24:33.540
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:24:33.660 --> 00:24:35.460
<v Speaker 1>So moving on to the next question.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:24:36.300 --> 00:24:39.460
<v Speaker 1>And this is these calculations.

NOTE CONF {"raw":[100,100,100,100,100]}

00:24:39.460 --> 00:24:42.460
<v Speaker 1>And as I said, contemporary neural networks are able to

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:24:42.500 --> 00:24:46.740
<v Speaker 1>really represent large inputs, often millions of words or hundreds

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:24:46.740 --> 00:24:47.820
<v Speaker 1>of thousands of words.

NOTE CONF {"raw":[99,100,100,100]}

00:24:49.220 --> 00:24:52.340
<v Speaker 1>But still even then, we need to think about what

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:24:52.340 --> 00:24:53.460
<v Speaker 1>the input should be like.

NOTE CONF {"raw":[100,100,100,100,100]}

00:24:53.460 --> 00:24:55.780
<v Speaker 1>And we don't want it to be too big because

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:24:55.780 --> 00:24:57.340
<v Speaker 1>it makes computation difficult.

NOTE CONF {"raw":[100,100,100,100]}

00:24:57.940 --> 00:25:04.060
<v Speaker 1>So if we use triples of phones or phonemes, how

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:25:04.060 --> 00:25:07.540
<v Speaker 1>many input units do we do we need?

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:25:08.340 --> 00:25:12.500
<v Speaker 1>If we use the 11 features and again triples?

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:25:12.500 --> 00:25:15.380
<v Speaker 1>How many inputs do we need if we use just

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:25:15.380 --> 00:25:16.540
<v Speaker 1>individual phonemes.

NOTE CONF {"raw":[100,100]}

00:25:16.940 --> 00:25:20.660
<v Speaker 1>So we're just using a representation that shows whether something

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:25:20.660 --> 00:25:24.100
<v Speaker 1>is a phoneme is present in the world or not,

NOTE CONF {"raw":[100,100,100,100,100,100,100,74,100,100]}

00:25:25.060 --> 00:25:27.700
<v Speaker 1>or we use a sequence of individual phonemes.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:25:28.100 --> 00:25:31.940
<v Speaker 1>So you're supposed to find the the correct calculation in

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:25:31.940 --> 00:25:33.110
<v Speaker 1>terms of input units.

NOTE CONF {"raw":[100,100,100,100]}

00:25:33.430 --> 00:25:35.070
<v Speaker 1>For each of these representations.

NOTE CONF {"raw":[100,100,100,100,100]}

00:25:43.150 --> 00:25:45.750
<v Speaker 1>So in this case you need to think how many

NOTE CONF {"raw":[100,100,71,100,100,100,100,100,100,100]}

00:25:45.750 --> 00:25:46.830
<v Speaker 1>possible phonemes.

NOTE CONF {"raw":[100,100]}

00:25:46.950 --> 00:25:48.470
<v Speaker 1>How many possible features.

NOTE CONF {"raw":[100,100,100,100]}

00:25:49.910 --> 00:25:53.310
<v Speaker 1>How do we compute the number or the number of

NOTE CONF {"raw":[100,100,100,100,100,100,45,100,100,100]}

00:25:53.470 --> 00:25:54.430
<v Speaker 1>combinations.

NOTE CONF {"raw":[100]}

00:25:55.470 --> 00:25:56.230
<v Speaker 1>And so on.

NOTE CONF {"raw":[100,100,100]}

00:26:03.750 --> 00:26:04.150
<v Speaker 0>And so.

NOTE CONF {"raw":[58,70]}

00:26:11.470 --> 00:26:11.790
<v Speaker 0>Forth.

NOTE CONF {"raw":[39]}

00:26:14.230 --> 00:26:14.630
<v Speaker 0>And.

NOTE CONF {"raw":[32]}

00:26:50.930 --> 00:26:51.370
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:26:51.370 --> 00:26:53.090
<v Speaker 1>Let's have a look at what people think.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:26:59.730 --> 00:27:05.650
<v Speaker 1>Okay, so the most popular pairing was 11 feature combinations.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:27:05.650 --> 00:27:11.890
<v Speaker 1>Triples of phonemes requires 11 to the third binary input

NOTE CONF {"raw":[100,96,100,100,100,100,100,100,100,100]}

00:27:11.890 --> 00:27:14.050
<v Speaker 1>units and then the next popular ones.

NOTE CONF {"raw":[100,100,100,100,100,99,93]}

00:27:14.050 --> 00:27:17.490
<v Speaker 1>Individual phonemes requires 35 binary input units.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:27:18.410 --> 00:27:21.610
<v Speaker 1>A sequence of three individual phonemes is 35 times three.

NOTE CONF {"raw":[94,100,100,100,100,100,100,100,100,100]}

00:27:21.650 --> 00:27:22.770
<v Speaker 1>That's also correct.

NOTE CONF {"raw":[100,100,100]}

00:27:23.130 --> 00:27:23.730
<v Speaker 1>Okay, great.

NOTE CONF {"raw":[100,100]}

00:27:23.730 --> 00:27:26.010
<v Speaker 1>Let's go through each of them.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:27:28.490 --> 00:27:29.250
<v Speaker 1>One by one.

NOTE CONF {"raw":[100,100,100]}

00:27:29.250 --> 00:27:31.460
<v Speaker 1>So tripled cell phones, right?

NOTE CONF {"raw":[100,49,98,98,100]}

00:27:31.500 --> 00:27:33.340
<v Speaker 1>We have 35 phones.

NOTE CONF {"raw":[100,100,100,100]}

00:27:33.460 --> 00:27:37.460
<v Speaker 1>And of course, in each of the positions in our

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:27:37.460 --> 00:27:39.260
<v Speaker 1>triple, we can have 35 combinations.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:27:39.260 --> 00:27:41.620
<v Speaker 1>So it would be 35 times 35 times 35.

NOTE CONF {"raw":[100,77,100,100,100,100,100,100,100]}

00:27:42.060 --> 00:27:46.940
<v Speaker 1>So that's uh, 42,875.

NOTE CONF {"raw":[100,100,81,100]}

00:27:47.420 --> 00:27:49.660
<v Speaker 1>So that's a lot of different input units.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:27:49.980 --> 00:27:54.380
<v Speaker 1>And it's it was too big for McClelland, for Hamilton

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:27:54.380 --> 00:27:55.100
<v Speaker 1>and McClelland.

NOTE CONF {"raw":[83,100]}

00:27:56.380 --> 00:28:00.060
<v Speaker 1>If we instead use the feature combinations right now, a

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:28:00.060 --> 00:28:05.220
<v Speaker 1>phone is not a binary unit, but it's a combination

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:28:05.220 --> 00:28:05.900
<v Speaker 1>of features.

NOTE CONF {"raw":[100,100]}

00:28:06.420 --> 00:28:09.620
<v Speaker 1>And we have 11 different different features.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:28:09.620 --> 00:28:11.580
<v Speaker 1>So it would be 11 to the third.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:28:11.940 --> 00:28:14.100
<v Speaker 1>That's 1331.

NOTE CONF {"raw":[100,100]}

00:28:15.620 --> 00:28:18.820
<v Speaker 1>If we just look at individual phones then we just

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:28:18.820 --> 00:28:20.220
<v Speaker 1>have 35 units, right.

NOTE CONF {"raw":[100,100,100,100]}

00:28:20.260 --> 00:28:21.780
<v Speaker 1>We don't represent triples.

NOTE CONF {"raw":[100,100,100,100]}

00:28:22.060 --> 00:28:24.340
<v Speaker 1>We just say, I don't know, Kat.

NOTE CONF {"raw":[100,100,100,100,100,100,63]}

00:28:25.100 --> 00:28:28.660
<v Speaker 1>The phones are K, A and T and we we

NOTE CONF {"raw":[100,100,100,52,100,100,52,100,92,100]}

00:28:28.710 --> 00:28:29.630
<v Speaker 1>represent those.

NOTE CONF {"raw":[100,100]}

00:28:29.870 --> 00:28:32.510
<v Speaker 1>We have 35 units and we switch on the ones

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:28:32.510 --> 00:28:34.390
<v Speaker 1>that are present in a word.

NOTE CONF {"raw":[100,100,100,100,100,97]}

00:28:34.710 --> 00:28:37.870
<v Speaker 1>So that's really short, but it's arguably too simple as

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:28:37.870 --> 00:28:38.870
<v Speaker 1>a representation.

NOTE CONF {"raw":[100,100]}

00:28:39.550 --> 00:28:43.590
<v Speaker 1>Then if we have three individual phones.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:28:43.910 --> 00:28:48.910
<v Speaker 1>So by that I mean we take 35 units representing

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:28:48.910 --> 00:28:52.670
<v Speaker 1>the first phone, 35 units a second, 35 to the

NOTE CONF {"raw":[100,100,100,100,100,91,100,100,91,99]}

00:28:52.670 --> 00:28:53.110
<v Speaker 1>third.

NOTE CONF {"raw":[100]}

00:28:53.270 --> 00:28:56.990
<v Speaker 1>So that would be 35 times 3 to 105 units.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:28:57.830 --> 00:28:58.310
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:28:58.790 --> 00:29:01.550
<v Speaker 1>That is not a bad representation, but it has one

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:29:01.550 --> 00:29:02.750
<v Speaker 1>crucial limitation.

NOTE CONF {"raw":[100,100]}

00:29:03.030 --> 00:29:05.590
<v Speaker 1>What's the limitation of a representation like this where we

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:29:05.590 --> 00:29:08.510
<v Speaker 1>just have three different phones in each represented as its

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,98]}

00:29:08.510 --> 00:29:11.350
<v Speaker 1>features or as as the phones themselves?

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:29:14.030 --> 00:29:17.190
<v Speaker 1>It seems pretty, pretty neat as a representation.

NOTE CONF {"raw":[49,100,100,100,100,100,100,100]}

00:29:17.190 --> 00:29:19.830
<v Speaker 1>But what's what's the what's the limitation?

NOTE CONF {"raw":[100,100,100,84,100,100,100]}

00:29:20.870 --> 00:29:21.950
<v Speaker 1>Why don't we do that?

NOTE CONF {"raw":[100,100,100,100,100]}

00:29:23.710 --> 00:29:26.990
<v Speaker 1>Well, because we can only represent three three letter words,

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:29:27.390 --> 00:29:27.590
<v Speaker 1>right?

NOTE CONF {"raw":[96]}

00:29:27.630 --> 00:29:30.250
<v Speaker 1>We can't represent any words that are longer or shorter

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:29:30.250 --> 00:29:30.770
<v Speaker 1>than that.

NOTE CONF {"raw":[100,100]}

00:29:31.330 --> 00:29:34.930
<v Speaker 1>And in particular in in the case of the past

NOTE CONF {"raw":[100,100,100,81,100,100,100,100,100,100]}

00:29:34.930 --> 00:29:37.890
<v Speaker 1>tense, often the length actually changes, right?

NOTE CONF {"raw":[100,100,100,100,100,100,96]}

00:29:37.930 --> 00:29:41.570
<v Speaker 1>Because you add a past tense suffix.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:29:41.570 --> 00:29:43.810
<v Speaker 1>So that's not a good representation for us.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:29:44.890 --> 00:29:47.330
<v Speaker 1>What people do if they want a representation like this,

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:29:47.330 --> 00:29:48.290
<v Speaker 1>they make it much longer.

NOTE CONF {"raw":[100,100,100,100,100]}

00:29:48.330 --> 00:29:48.570
<v Speaker 1>Right.

NOTE CONF {"raw":[100]}

00:29:48.610 --> 00:29:51.050
<v Speaker 1>So let's assume instead of three letters you assume 20

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:29:51.090 --> 00:29:51.490
<v Speaker 1>letters.

NOTE CONF {"raw":[100]}

00:29:51.490 --> 00:29:54.490
<v Speaker 1>And that's probably long enough for most verbs in English.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:29:55.290 --> 00:29:57.130
<v Speaker 1>And then you still have the problem.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:29:57.450 --> 00:29:58.810
<v Speaker 1>What about words that are shorter.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:29:58.810 --> 00:29:59.930
<v Speaker 1>So you need to pad.

NOTE CONF {"raw":[100,100,100,100,91]}

00:30:00.930 --> 00:30:04.290
<v Speaker 1>Um, how can you represent the.

NOTE CONF {"raw":[63,100,100,100,100,92]}

00:30:04.330 --> 00:30:06.650
<v Speaker 1>The same suffix can appear in different places.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:30:06.690 --> 00:30:06.850
<v Speaker 1>Right.

NOTE CONF {"raw":[100]}

00:30:06.890 --> 00:30:09.650
<v Speaker 1>So you have the suffix ed in English, but of

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:30:09.650 --> 00:30:11.450
<v Speaker 1>course in a three letter word and in a five

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:30:11.490 --> 00:30:14.010
<v Speaker 1>letter word it appears at a different position.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:30:14.090 --> 00:30:16.050
<v Speaker 1>So you need to somehow represent that as well.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:30:16.170 --> 00:30:20.530
<v Speaker 1>So these fixed length representations, they are popular representations.

NOTE CONF {"raw":[100,100,100,100,100,89,100,100,100]}

00:30:20.530 --> 00:30:23.850
<v Speaker 1>But for this particular task they're probably not very suitable.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:30:27.740 --> 00:30:31.180
<v Speaker 1>Okay, I will skip the next one in the interest

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:30:31.180 --> 00:30:36.580
<v Speaker 1>of time and move back to the lecture.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:30:39.900 --> 00:30:42.380
<v Speaker 1>Okay, let's talk about the training and the structure of

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:30:42.380 --> 00:30:43.420
<v Speaker 1>the model a little bit.

NOTE CONF {"raw":[100,100,100,100,100]}

00:30:44.060 --> 00:30:47.740
<v Speaker 1>So as we've said, this is a very simple model

NOTE CONF {"raw":[100,100,49,100,100,100,100,100,100,100]}

00:30:47.740 --> 00:30:49.700
<v Speaker 1>that only has an input layer and an output layer.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:30:49.700 --> 00:30:50.860
<v Speaker 1>There's nothing in between.

NOTE CONF {"raw":[100,100,100,100]}

00:30:50.860 --> 00:30:54.420
<v Speaker 1>So there's no hidden no hidden units which would normally

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:30:54.420 --> 00:30:54.900
<v Speaker 1>be here.

NOTE CONF {"raw":[100,100]}

00:30:55.820 --> 00:30:59.060
<v Speaker 1>It works anyway because the task is fairly simple.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:30:59.260 --> 00:31:02.140
<v Speaker 1>And then we have each of these units representing a

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:31:02.140 --> 00:31:07.060
<v Speaker 1>triple of different feature combinations.

NOTE CONF {"raw":[65,100,100,100,100]}

00:31:07.060 --> 00:31:12.620
<v Speaker 1>So for example this feature combination or that feature combination

NOTE CONF {"raw":[100,100,100,100,100,100,100,55,100,100]}

00:31:12.980 --> 00:31:17.620
<v Speaker 1>which I've just here written for example as back vowel

NOTE CONF {"raw":[100,97,100,100,100,100,100,100,83,99]}

00:31:19.260 --> 00:31:21.660
<v Speaker 1>and an n and then the end of the word

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:31:21.660 --> 00:31:23.900
<v Speaker 1>and so on, or two consonants at the beginning of

NOTE CONF {"raw":[100,100,100,98,100,100,80,100,100,100]}

00:31:23.900 --> 00:31:24.700
<v Speaker 1>the word and so on.

NOTE CONF {"raw":[100,100,100,100,100]}

00:31:24.700 --> 00:31:28.630
<v Speaker 1>So there's 460 of these and the same in the

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:31:28.630 --> 00:31:30.870
<v Speaker 1>output 460 of those.

NOTE CONF {"raw":[100,97,100,100]}

00:31:32.030 --> 00:31:35.150
<v Speaker 1>And uh, how do we train this?

NOTE CONF {"raw":[100,73,100,100,100,100,100]}

00:31:35.510 --> 00:31:37.790
<v Speaker 1>So we use the perceptron learning algorithm.

NOTE CONF {"raw":[100,100,99,100,100,100,100]}

00:31:37.790 --> 00:31:40.550
<v Speaker 1>We don't need to use backpropagation because there's no hidden

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:31:40.550 --> 00:31:40.910
<v Speaker 1>layer.

NOTE CONF {"raw":[100]}

00:31:41.870 --> 00:31:45.670
<v Speaker 1>Uh, but we assume we have correct input output pairs.

NOTE CONF {"raw":[95,100,100,100,100,100,100,100,100,100]}

00:31:45.950 --> 00:31:48.910
<v Speaker 1>So pairs like walk walked or sink sunk.

NOTE CONF {"raw":[100,100,100,100,100,100,98,71]}

00:31:49.470 --> 00:31:52.630
<v Speaker 1>And then we initialise the weights and the threshold.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:31:52.910 --> 00:31:55.390
<v Speaker 1>They just said everything to the zero to start with.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:31:56.230 --> 00:31:58.390
<v Speaker 1>And then we go through a loop where we look

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:31:58.390 --> 00:32:02.230
<v Speaker 1>at each training example, calculate the output for the training

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:32:02.230 --> 00:32:04.750
<v Speaker 1>example compared to the correct answer.

NOTE CONF {"raw":[95,100,100,100,100,93]}

00:32:04.750 --> 00:32:09.230
<v Speaker 1>So we input walk for example, but of course represented

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:32:09.230 --> 00:32:11.430
<v Speaker 1>as a set of triples of features.

NOTE CONF {"raw":[100,100,100,100,100,97,100]}

00:32:12.150 --> 00:32:16.710
<v Speaker 1>And then we generate the output again represented as a

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:32:17.310 --> 00:32:18.510
<v Speaker 1>as triples of features.

NOTE CONF {"raw":[100,100,100,100]}

00:32:18.830 --> 00:32:21.910
<v Speaker 1>And compared to our target is it worked or not.

NOTE CONF {"raw":[100,100,100,100,100,100,100,95,100,100]}

00:32:22.630 --> 00:32:27.450
<v Speaker 1>And we update our weights and the threshold, which we've

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:32:27.450 --> 00:32:30.850
<v Speaker 1>just encoded as a bias term in order to make

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:32:30.850 --> 00:32:32.570
<v Speaker 1>the answer similar.

NOTE CONF {"raw":[100,100,100]}

00:32:32.970 --> 00:32:35.090
<v Speaker 1>The output similar to the target.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:32:35.810 --> 00:32:37.050
<v Speaker 1>Remember we take the output.

NOTE CONF {"raw":[100,100,100,100,100]}

00:32:37.050 --> 00:32:39.210
<v Speaker 1>We take the target, which in this case is just

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,87,100]}

00:32:39.650 --> 00:32:41.810
<v Speaker 1>a long list of ones and zeros.

NOTE CONF {"raw":[100,100,100,100,100,100,93]}

00:32:42.210 --> 00:32:44.930
<v Speaker 1>And we want to make them more similar.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:32:45.090 --> 00:32:48.090
<v Speaker 1>So we compute the error and we adjust our weights

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:32:48.090 --> 00:32:48.970
<v Speaker 1>based on the error.

NOTE CONF {"raw":[100,100,100,100]}

00:32:49.130 --> 00:32:52.410
<v Speaker 1>So exactly using the perceptron learning rule that we saw

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:32:52.410 --> 00:32:53.650
<v Speaker 1>in the last two lectures.

NOTE CONF {"raw":[100,100,100,100,100]}

00:32:54.370 --> 00:32:55.850
<v Speaker 1>And then we keep doing this right.

NOTE CONF {"raw":[100,100,100,100,100,100,91]}

00:32:55.890 --> 00:32:58.010
<v Speaker 1>We iterate through all the training examples.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:32:58.010 --> 00:33:01.330
<v Speaker 1>And then we iterate through the whole training set multiple

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:33:01.330 --> 00:33:04.890
<v Speaker 1>times until the error becomes small enough.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:33:05.210 --> 00:33:07.530
<v Speaker 1>And here it's up to you to define what means

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:33:07.730 --> 00:33:09.010
<v Speaker 1>what you mean by small enough.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:33:09.370 --> 00:33:11.730
<v Speaker 1>The error will in most cases not go down to

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:33:11.730 --> 00:33:14.650
<v Speaker 1>zero, but it will become very small and then it

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:33:14.650 --> 00:33:17.610
<v Speaker 1>will not reduce anymore, or it will just fluctuate a

NOTE CONF {"raw":[100,100,100,95,100,100,100,100,100,100]}

00:33:17.610 --> 00:33:18.130
<v Speaker 1>little bit.

NOTE CONF {"raw":[100,100]}

00:33:18.410 --> 00:33:21.290
<v Speaker 1>And people often draw an error curve, start with a

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:33:21.290 --> 00:33:23.820
<v Speaker 1>large error and then sort of goes down and eventually

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:33:23.820 --> 00:33:24.860
<v Speaker 1>it stabilises.

NOTE CONF {"raw":[100,93]}

00:33:25.140 --> 00:33:25.660
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:33:25.700 --> 00:33:27.540
<v Speaker 1>You also call this convergence.

NOTE CONF {"raw":[100,100,100,100,100]}

00:33:27.660 --> 00:33:30.780
<v Speaker 1>The network has converged and then you stop training.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:33:31.620 --> 00:33:33.300
<v Speaker 1>So you need a criterion here, right?

NOTE CONF {"raw":[100,100,100,100,100,100,91]}

00:33:33.340 --> 00:33:36.300
<v Speaker 1>You need to sort of say the error is small

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:33:36.300 --> 00:33:37.500
<v Speaker 1>enough and then I stop.

NOTE CONF {"raw":[100,100,100,100,96]}

00:33:39.540 --> 00:33:39.900
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:33:39.940 --> 00:33:41.140
<v Speaker 1>So how do they do this.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:33:41.180 --> 00:33:42.700
<v Speaker 1>In in in practice.

NOTE CONF {"raw":[100,100,100,100]}

00:33:43.140 --> 00:33:45.860
<v Speaker 1>So they need a lot of training iterations.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:33:46.380 --> 00:33:47.340
<v Speaker 1>84,000.

NOTE CONF {"raw":[100]}

00:33:47.380 --> 00:33:52.140
<v Speaker 1>So they go through this loop basically 84,000 times.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:33:53.340 --> 00:33:58.300
<v Speaker 1>Um, and this worked for the 420 verbs in the

NOTE CONF {"raw":[90,100,100,100,100,100,100,100,100,100]}

00:33:58.300 --> 00:33:58.820
<v Speaker 1>training set.

NOTE CONF {"raw":[100,100]}

00:33:58.860 --> 00:34:01.300
<v Speaker 1>So they're able to to learn all these verbs.

NOTE CONF {"raw":[100,87,100,100,100,100,100,100,100]}

00:34:02.260 --> 00:34:04.940
<v Speaker 1>This in itself is not super surprising, right?

NOTE CONF {"raw":[100,100,100,100,100,100,100,95]}

00:34:05.260 --> 00:34:07.860
<v Speaker 1>A model is supposed to learn the training set and

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:34:08.179 --> 00:34:10.700
<v Speaker 1>supposed to get the error down to zero or close

NOTE CONF {"raw":[100,100,100,100,91,100,100,100,100,100]}

00:34:10.700 --> 00:34:11.220
<v Speaker 1>to zero.

NOTE CONF {"raw":[100,100]}

00:34:11.580 --> 00:34:14.139
<v Speaker 1>The interesting thing is, when you try it on new

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,85]}

00:34:14.139 --> 00:34:17.659
<v Speaker 1>data, when you see whether it generalises to unseen data.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:34:18.100 --> 00:34:21.340
<v Speaker 1>So they tested the model on 86 unseen verbs or

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,95]}

00:34:21.379 --> 00:34:24.919
<v Speaker 1>regular and irregular irregular verbs that they hadn't trained on,

NOTE CONF {"raw":[100,81,100,100,100,100,100,100,100,100]}

00:34:25.520 --> 00:34:29.320
<v Speaker 1>and two thirds of them were assigned the correct past

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:34:29.320 --> 00:34:29.760
<v Speaker 1>tense.

NOTE CONF {"raw":[100]}

00:34:30.280 --> 00:34:34.600
<v Speaker 1>So, you know, an error rate of 25% and the

NOTE CONF {"raw":[100,58,58,37,86,100,100,100,100,100]}

00:34:34.600 --> 00:34:37.120
<v Speaker 1>irregular ones typically were over generalised.

NOTE CONF {"raw":[100,100,100,100,76,69]}

00:34:37.120 --> 00:34:37.399
<v Speaker 1>So.

NOTE CONF {"raw":[91]}

00:34:38.000 --> 00:34:38.919
<v Speaker 1>Dig, dig.

NOTE CONF {"raw":[98,100]}

00:34:38.960 --> 00:34:39.399
<v Speaker 1>Catch.

NOTE CONF {"raw":[100]}

00:34:39.399 --> 00:34:39.960
<v Speaker 1>Catched.

NOTE CONF {"raw":[76]}

00:34:42.159 --> 00:34:45.080
<v Speaker 1>And so it was able to learn verbs.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:34:45.080 --> 00:34:49.080
<v Speaker 1>And it was able to generalise to some extent to

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:34:49.120 --> 00:34:49.840
<v Speaker 1>new verbs.

NOTE CONF {"raw":[100,100]}

00:34:49.919 --> 00:34:51.200
<v Speaker 1>But it still made errors.

NOTE CONF {"raw":[100,100,100,100,100]}

00:34:51.360 --> 00:34:52.639
<v Speaker 1>Once the training was done.

NOTE CONF {"raw":[100,100,100,100,100]}

00:34:53.800 --> 00:34:55.960
<v Speaker 1>And that is interesting.

NOTE CONF {"raw":[100,100,100,100]}

00:34:55.960 --> 00:34:58.520
<v Speaker 1>But the most interesting thing is, do we actually see

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:34:58.760 --> 00:35:00.840
<v Speaker 1>the same behaviour that we see in children?

NOTE CONF {"raw":[100,100,91,100,100,100,100,100]}

00:35:01.440 --> 00:35:04.920
<v Speaker 1>So the authors claim that they see U-shaped learning.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:35:05.400 --> 00:35:10.400
<v Speaker 1>So initially, for example, the model correctly generated give gave

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:35:10.960 --> 00:35:14.240
<v Speaker 1>but then it shifted to give and then shifted back

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:35:14.240 --> 00:35:14.840
<v Speaker 1>to gave.

NOTE CONF {"raw":[100,100]}

00:35:15.840 --> 00:35:17.840
<v Speaker 1>Uh there are certain error patterns.

NOTE CONF {"raw":[46,100,100,100,96,100]}

00:35:17.840 --> 00:35:21.160
<v Speaker 1>So for example if a verb ended in t or

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,69,100]}

00:35:21.530 --> 00:35:26.170
<v Speaker 1>It was reluctant to apply the rule, and it made

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:35:26.170 --> 00:35:28.410
<v Speaker 1>overgeneralisation errors that we see in children.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:35:28.410 --> 00:35:33.930
<v Speaker 1>Clang clang set where verbs that are regular, the model

NOTE CONF {"raw":[100,100,41,100,100,100,100,100,100,100]}

00:35:33.930 --> 00:35:35.370
<v Speaker 1>things are irregular.

NOTE CONF {"raw":[100,100,100]}

00:35:37.970 --> 00:35:40.050
<v Speaker 1>And sorry, this was the wrong.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:35:42.290 --> 00:35:42.890
<v Speaker 1>Um.

NOTE CONF {"raw":[74]}

00:35:42.930 --> 00:35:44.490
<v Speaker 1>So how did they achieve this?

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:35:44.490 --> 00:35:45.330
<v Speaker 1>So they they.

NOTE CONF {"raw":[100,50,100]}

00:35:45.810 --> 00:35:50.050
<v Speaker 1>The authors Rumelhart and McClelland, uh, they looked at what

NOTE CONF {"raw":[100,100,100,100,100,92,100,100,100,100]}

00:35:50.050 --> 00:35:54.490
<v Speaker 1>children do, and they assume that children learn common verbs

NOTE CONF {"raw":[100,100,100,100,98,100,100,100,100,100]}

00:35:54.810 --> 00:35:58.370
<v Speaker 1>first, like walk and talk and so on, and then

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:35:58.370 --> 00:36:02.170
<v Speaker 1>infrequent ones, I don't know, rationalise or something like that

NOTE CONF {"raw":[100,100,100,100,100,66,100,100,100,100]}

00:36:02.170 --> 00:36:02.730
<v Speaker 1>later.

NOTE CONF {"raw":[100]}

00:36:03.170 --> 00:36:05.810
<v Speaker 1>And so they built this into their model.

NOTE CONF {"raw":[100,100,100,100,100,100,97,100]}

00:36:05.810 --> 00:36:09.890
<v Speaker 1>So they trained first using ten verbs, and eight of

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:36:09.890 --> 00:36:11.130
<v Speaker 1>them were irregular.

NOTE CONF {"raw":[100,100,100]}

00:36:11.850 --> 00:36:15.890
<v Speaker 1>And the irregular ones they observe are learned earlier than

NOTE CONF {"raw":[100,100,100,100,100,92,100,94,100,100]}

00:36:15.930 --> 00:36:17.450
<v Speaker 1>the irregular ones.

NOTE CONF {"raw":[100,100,100]}

00:36:17.690 --> 00:36:21.380
<v Speaker 1>So they trained it with the remaining 410 verbs, which

NOTE CONF {"raw":[100,100,100,93,100,97,100,100,100,100]}

00:36:21.380 --> 00:36:22.500
<v Speaker 1>were mostly regular.

NOTE CONF {"raw":[100,100,100]}

00:36:23.500 --> 00:36:25.500
<v Speaker 1>And they also observe that children.

NOTE CONF {"raw":[100,100,100,68,100,100]}

00:36:25.580 --> 00:36:28.220
<v Speaker 1>If you remember this, there is this growth spurt where

NOTE CONF {"raw":[100,100,100,100,76,76,100,100,100,100]}

00:36:28.260 --> 00:36:32.220
<v Speaker 1>children suddenly learn a lot of verbs and ones or

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:36:32.340 --> 00:36:34.020
<v Speaker 1>words in general, not just verbs.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:36:34.580 --> 00:36:39.220
<v Speaker 1>And so they they see something like that in that

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:36:39.220 --> 00:36:42.500
<v Speaker 1>the error rates increased dramatically at the start of this

NOTE CONF {"raw":[100,100,83,89,100,100,100,100,100,100]}

00:36:42.500 --> 00:36:44.860
<v Speaker 1>phase, where they learn lots of verbs and then they

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:36:44.860 --> 00:36:45.740
<v Speaker 1>recover again.

NOTE CONF {"raw":[100,100]}

00:36:46.260 --> 00:36:50.300
<v Speaker 1>And the model made errors just such as break, break.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,85,50]}

00:36:52.180 --> 00:36:55.260
<v Speaker 1>So this is the learning curve that they found where

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,99]}

00:36:55.300 --> 00:36:57.380
<v Speaker 1>on the x axis you have the number of trials

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:36:58.860 --> 00:37:01.460
<v Speaker 1>during learning, and then you have the irregular and irregular

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:37:01.460 --> 00:37:02.180
<v Speaker 1>verbs here.

NOTE CONF {"raw":[100,100]}

00:37:02.660 --> 00:37:05.540
<v Speaker 1>And you see that they they both converge to a

NOTE CONF {"raw":[100,100,100,100,49,100,100,100,100,100]}

00:37:05.540 --> 00:37:06.700
<v Speaker 1>high level of correctness.

NOTE CONF {"raw":[100,100,100,100]}

00:37:06.980 --> 00:37:12.780
<v Speaker 1>But there's this, uh, this blip where accuracy goes down

NOTE CONF {"raw":[100,100,100,81,100,100,100,100,100,100]}

00:37:13.300 --> 00:37:14.740
<v Speaker 1>and then it goes up again.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:37:16.060 --> 00:37:18.780
<v Speaker 1>And the authors argue that this is our U-shaped curve.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:37:20.200 --> 00:37:22.800
<v Speaker 1>So it doesn't really look like the you that we've.

NOTE CONF {"raw":[100,100,100,100,100,100,99,48,100,99]}

00:37:23.400 --> 00:37:24.000
<v Speaker 1>We've seen.

NOTE CONF {"raw":[100,100]}

00:37:24.000 --> 00:37:24.440
<v Speaker 1>Right.

NOTE CONF {"raw":[100]}

00:37:24.440 --> 00:37:28.520
<v Speaker 1>And it's also only there for the irregular ones which

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:37:28.520 --> 00:37:29.480
<v Speaker 1>is also strange.

NOTE CONF {"raw":[100,100,100]}

00:37:30.160 --> 00:37:32.400
<v Speaker 1>And in general it seems like a pretty small effect.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:37:33.600 --> 00:37:34.040
<v Speaker 1>So.

NOTE CONF {"raw":[100]}

00:37:36.120 --> 00:37:39.440
<v Speaker 1>When other authors saw this, they were quite sceptical.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:37:39.880 --> 00:37:44.720
<v Speaker 1>And there has been a lot of subsequent work looking

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:37:44.720 --> 00:37:47.440
<v Speaker 1>at this and trying to replicate this, this U shaped

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,97,97]}

00:37:47.440 --> 00:37:47.920
<v Speaker 1>learning.

NOTE CONF {"raw":[100]}

00:37:49.120 --> 00:37:53.320
<v Speaker 1>So and also conceptually, you can think and say, okay,

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:37:53.360 --> 00:38:00.080
<v Speaker 1>so in that training regime, they switched from mostly irregular

NOTE CONF {"raw":[100,100,98,100,100,100,100,100,100,100]}

00:38:00.080 --> 00:38:03.000
<v Speaker 1>verbs in the beginning to mostly irregular verbs later on.

NOTE CONF {"raw":[100,100,100,100,100,100,94,100,100,100]}

00:38:03.640 --> 00:38:06.960
<v Speaker 1>And when children hear language, is it actually of that

NOTE CONF {"raw":[100,100,100,96,100,100,100,100,100,100]}

00:38:06.960 --> 00:38:07.360
<v Speaker 1>pattern.

NOTE CONF {"raw":[100]}

00:38:07.360 --> 00:38:08.120
<v Speaker 1>Probably not.

NOTE CONF {"raw":[100,100]}

00:38:08.360 --> 00:38:08.680
<v Speaker 1>Right.

NOTE CONF {"raw":[97]}

00:38:08.720 --> 00:38:11.960
<v Speaker 1>It's not like parents suddenly decide to use lots of

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:38:11.960 --> 00:38:15.520
<v Speaker 1>regular verbs after initially having used only irregular verbs.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:38:15.520 --> 00:38:16.600
<v Speaker 1>That would be very strange.

NOTE CONF {"raw":[100,100,100,100,100]}

00:38:17.440 --> 00:38:19.650
<v Speaker 1>And so this is actually something we don't have any

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:38:19.650 --> 00:38:20.330
<v Speaker 1>evidence for.

NOTE CONF {"raw":[100,100]}

00:38:20.810 --> 00:38:24.050
<v Speaker 1>Then they claim that they model this growth spurt in

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:38:24.050 --> 00:38:24.730
<v Speaker 1>the vocabulary.

NOTE CONF {"raw":[100,100]}

00:38:24.810 --> 00:38:29.170
<v Speaker 1>It turns out that that's also not correct because this

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,91]}

00:38:29.170 --> 00:38:33.610
<v Speaker 1>growth spurt happens early on in in the second year

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:38:33.610 --> 00:38:37.760
<v Speaker 1>of life, whereas the overgeneralisation errors that actually happen between

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:38:37.760 --> 00:38:40.010
<v Speaker 1>2 and 3, they happen later in life.

NOTE CONF {"raw":[100,100,100,89,100,100,100,100]}

00:38:40.810 --> 00:38:43.690
<v Speaker 1>Initially, kids don't know a lot of verbs.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:38:43.690 --> 00:38:46.570
<v Speaker 1>They actually start with nouns in the first year of

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:38:46.570 --> 00:38:46.890
<v Speaker 1>life.

NOTE CONF {"raw":[100]}

00:38:47.370 --> 00:38:49.930
<v Speaker 1>So that claim is also not quite right.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:38:50.450 --> 00:38:52.490
<v Speaker 1>And then what's with the training scheme?

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:38:52.490 --> 00:38:54.610
<v Speaker 1>So they had this complicated training scheme right.

NOTE CONF {"raw":[100,100,88,100,100,100,100,94]}

00:38:54.650 --> 00:38:57.610
<v Speaker 1>Ten verbs which are irregular and frequent.

NOTE CONF {"raw":[100,100,100,100,100,100,85]}

00:38:57.610 --> 00:39:01.450
<v Speaker 1>And then 410 verbs which are mostly regular and less

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:39:01.450 --> 00:39:02.090
<v Speaker 1>frequent.

NOTE CONF {"raw":[100]}

00:39:02.690 --> 00:39:05.530
<v Speaker 1>And they get the U-shaped curve.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:39:05.530 --> 00:39:09.050
<v Speaker 1>But actually if you change the training regime, it's quite

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:39:09.050 --> 00:39:12.530
<v Speaker 1>brittle and the effect is not necessarily replicated.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:39:12.970 --> 00:39:15.050
<v Speaker 1>So this is not a very stable finding.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:39:15.330 --> 00:39:16.170
<v Speaker 1>Unfortunately.

NOTE CONF {"raw":[100]}

00:39:16.170 --> 00:39:16.260
<v Speaker 1>me.

NOTE CONF {"raw":[100]}

00:39:18.740 --> 00:39:20.140
<v Speaker 1>Um, there's other problems.

NOTE CONF {"raw":[72,85,100,100]}

00:39:20.420 --> 00:39:22.780
<v Speaker 1>So it only produces past tense forms.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:39:22.780 --> 00:39:24.780
<v Speaker 1>It cannot understand past tense forms.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:39:24.780 --> 00:39:29.020
<v Speaker 1>You cannot sort of use the model in reverse for

NOTE CONF {"raw":[100,67,100,100,100,100,100,100,100,100]}

00:39:29.020 --> 00:39:30.420
<v Speaker 1>rule based account.

NOTE CONF {"raw":[100,100,100]}

00:39:30.420 --> 00:39:31.420
<v Speaker 1>That's not a problem, right?

NOTE CONF {"raw":[100,100,100,100,89]}

00:39:31.660 --> 00:39:34.300
<v Speaker 1>The rules in the lexicon are the same for doesn't

NOTE CONF {"raw":[100,100,95,100,100,100,100,100,100,100]}

00:39:34.300 --> 00:39:35.420
<v Speaker 1>matter what you do with them.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:39:35.740 --> 00:39:39.420
<v Speaker 1>You can generate, you can understand, you can read and

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:39:39.420 --> 00:39:40.180
<v Speaker 1>write and so on.

NOTE CONF {"raw":[98,100,100,100]}

00:39:40.180 --> 00:39:43.420
<v Speaker 1>And here this is just a model of listening, right.

NOTE CONF {"raw":[100,100,100,100,100,96,100,100,100,97]}

00:39:43.460 --> 00:39:48.100
<v Speaker 1>Because it uses phonological representations sorry speech production rather than

NOTE CONF {"raw":[100,100,100,100,100,64,100,100,100,100]}

00:39:48.100 --> 00:39:48.620
<v Speaker 1>listening.

NOTE CONF {"raw":[100]}

00:39:49.980 --> 00:39:53.420
<v Speaker 1>Um, then it has a very detailed model of the

NOTE CONF {"raw":[94,100,100,100,100,100,100,100,100,100]}

00:39:53.420 --> 00:39:54.260
<v Speaker 1>pronunciation, right?

NOTE CONF {"raw":[100,95]}

00:39:54.300 --> 00:39:59.060
<v Speaker 1>In terms of these features, um, obviously we would actually

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:39:59.060 --> 00:40:00.660
<v Speaker 1>like this to be general.

NOTE CONF {"raw":[100,100,100,100,100]}

00:40:00.660 --> 00:40:04.740
<v Speaker 1>And these feature representations should appear in other parts of

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:40:04.740 --> 00:40:06.140
<v Speaker 1>the language system as well.

NOTE CONF {"raw":[100,100,100,100,100]}

00:40:06.540 --> 00:40:10.060
<v Speaker 1>So every network needs to needs to model this separately.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,93,100]}

00:40:10.060 --> 00:40:11.740
<v Speaker 1>That doesn't make sense either.

NOTE CONF {"raw":[100,100,100,100,100]}

00:40:12.820 --> 00:40:17.720
<v Speaker 1>And then these wicked vehicle features are also simplified.

NOTE CONF {"raw":[100,100,100,66,70,100,73,100,100]}

00:40:17.720 --> 00:40:20.880
<v Speaker 1>For example, they don't really represent the order, right?

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:40:20.920 --> 00:40:23.440
<v Speaker 1>They only have the context, the previous and the next

NOTE CONF {"raw":[100,100,100,100,100,100,100,52,100,100]}

00:40:24.520 --> 00:40:27.440
<v Speaker 1>phoneme, but they don't represent the order of the of

NOTE CONF {"raw":[100,100,100,100,100,100,100,79,49,89]}

00:40:27.440 --> 00:40:28.320
<v Speaker 1>of the features.

NOTE CONF {"raw":[100,100,100]}

00:40:28.960 --> 00:40:31.440
<v Speaker 1>They miss other aspects of phonology as well.

NOTE CONF {"raw":[100,69,100,100,100,100,100,100]}

00:40:31.440 --> 00:40:35.520
<v Speaker 1>So it's in many, in many ways too simple what

NOTE CONF {"raw":[100,100,100,100,100,100,100,84,100,100]}

00:40:35.520 --> 00:40:36.080
<v Speaker 1>they're doing.

NOTE CONF {"raw":[100,100]}

00:40:37.840 --> 00:40:38.320
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:40:38.360 --> 00:40:40.040
<v Speaker 1>And I have a few questions.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:40:40.760 --> 00:40:41.840
<v Speaker 1>Questions for you.

NOTE CONF {"raw":[100,100,100]}

00:40:46.880 --> 00:40:51.000
<v Speaker 1>So what would you say is the other main criticisms

NOTE CONF {"raw":[100,100,100,100,100,100,100,61,100,100]}

00:40:51.200 --> 00:40:52.920
<v Speaker 1>of the Rumelhart and McClelland model?

NOTE CONF {"raw":[100,100,100,100,97,100]}

00:40:52.920 --> 00:40:56.240
<v Speaker 1>We've discussed some some aspects, but for example, that it

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:40:56.240 --> 00:40:59.800
<v Speaker 1>doesn't learn irregular verbs, that training takes too much time,

NOTE CONF {"raw":[100,100,100,100,49,100,100,100,100,100]}

00:41:00.640 --> 00:41:04.760
<v Speaker 1>that the U-shaped learning is actually not convincing.

NOTE CONF {"raw":[100,100,91,100,100,100,100,100]}

00:41:04.760 --> 00:41:07.080
<v Speaker 1>Maybe it's just an artefact of the training regime that

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:41:07.080 --> 00:41:07.920
<v Speaker 1>they've devised.

NOTE CONF {"raw":[100,100]}

00:41:09.040 --> 00:41:13.160
<v Speaker 1>Uh, it's not robust against changes in the training regime.

NOTE CONF {"raw":[80,100,100,100,100,100,100,100,100,100]}

00:41:13.560 --> 00:41:15.250
<v Speaker 1>Doesn't capture that.

NOTE CONF {"raw":[100,100,100]}

00:41:15.530 --> 00:41:20.010
<v Speaker 1>The children learn common verbs first, or the representation is

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:41:20.010 --> 00:41:20.970
<v Speaker 1>oversimplified.

NOTE CONF {"raw":[100]}

00:41:22.010 --> 00:41:26.770
<v Speaker 1>So what do you think are valid criticisms of the

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,99]}

00:41:26.770 --> 00:41:27.250
<v Speaker 1>model?

NOTE CONF {"raw":[100]}

00:42:02.090 --> 00:42:02.930
<v Speaker 0>Let's see.

NOTE CONF {"raw":[47,55]}

00:42:10.370 --> 00:42:10.890
<v Speaker 0>How?

NOTE CONF {"raw":[23]}

00:42:15.540 --> 00:42:15.780
<v Speaker 0>So.

NOTE CONF {"raw":[91]}

00:42:26.380 --> 00:42:26.780
<v Speaker 1>Okay.

NOTE CONF {"raw":[87]}

00:42:26.820 --> 00:42:28.340
<v Speaker 1>So let's have a look at each of these.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:42:30.340 --> 00:42:32.980
<v Speaker 1>Number one it doesn't learn irregular verbs correctly.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:42:32.980 --> 00:42:37.980
<v Speaker 1>So 29% thought that this is, uh, a valid criticism.

NOTE CONF {"raw":[100,100,100,100,100,100,81,100,100,100]}

00:42:38.980 --> 00:42:42.700
<v Speaker 1>Um, yes.

NOTE CONF {"raw":[88,100]}

00:42:42.980 --> 00:42:44.820
<v Speaker 1>I think it's a valid criticism.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:42:45.740 --> 00:42:47.980
<v Speaker 1>Um.

NOTE CONF {"raw":[93]}

00:42:50.020 --> 00:42:52.580
<v Speaker 1>In particular, for unseen irregulars.

NOTE CONF {"raw":[100,100,100,100,100]}

00:42:52.820 --> 00:42:56.700
<v Speaker 1>You wouldn't expect perfect, uh, performance.

NOTE CONF {"raw":[100,100,100,100,98,100]}

00:42:56.740 --> 00:42:56.980
<v Speaker 1>Right?

NOTE CONF {"raw":[100]}

00:42:57.020 --> 00:42:59.260
<v Speaker 1>Humans, if they see a new verb and they don't

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:42:59.260 --> 00:43:01.140
<v Speaker 1>know it's irregular, they might get it wrong.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:43:01.540 --> 00:43:04.940
<v Speaker 1>But in particular, at training time, you would expect it

NOTE CONF {"raw":[100,100,100,93,100,100,100,100,100,100]}

00:43:04.940 --> 00:43:07.020
<v Speaker 1>to pick up all the irregulars correctly.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:43:08.460 --> 00:43:12.300
<v Speaker 1>The next one, 21% training takes too much time.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:43:12.760 --> 00:43:17.600
<v Speaker 1>Um, no, that's not a valid criticism because we actually

NOTE CONF {"raw":[80,100,100,100,100,100,100,100,100,100]}

00:43:17.600 --> 00:43:19.200
<v Speaker 1>don't look at the training time, right?

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:43:19.240 --> 00:43:23.080
<v Speaker 1>We just look at the overall shape of the learning

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:43:23.080 --> 00:43:23.520
<v Speaker 1>curve.

NOTE CONF {"raw":[100]}

00:43:24.000 --> 00:43:26.360
<v Speaker 1>In a model, the training time will depend on, you

NOTE CONF {"raw":[100,91,100,100,100,100,100,100,100,100]}

00:43:26.360 --> 00:43:29.320
<v Speaker 1>know, how good your implementation is, how fast your hardware

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:43:29.320 --> 00:43:29.480
<v Speaker 1>is.

NOTE CONF {"raw":[100]}

00:43:29.520 --> 00:43:33.800
<v Speaker 1>And these things are not interesting from a cognitive point

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:43:33.800 --> 00:43:34.080
<v Speaker 1>of view.

NOTE CONF {"raw":[100,100]}

00:43:34.120 --> 00:43:36.680
<v Speaker 1>Then the third one, a modelling of the U-shaped learning,

NOTE CONF {"raw":[100,100,100,100,89,99,100,100,79,100]}

00:43:36.680 --> 00:43:39.040
<v Speaker 1>is an artefact of the unusual training regime.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:43:39.640 --> 00:43:40.160
<v Speaker 1>Yes.

NOTE CONF {"raw":[100]}

00:43:40.160 --> 00:43:43.720
<v Speaker 1>So people were actually not able to replicate this.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:43:44.160 --> 00:43:47.840
<v Speaker 1>And we'll see another model in a moment that has

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:43:47.840 --> 00:43:48.640
<v Speaker 1>the same problem.

NOTE CONF {"raw":[100,100,100]}

00:43:48.640 --> 00:43:53.960
<v Speaker 1>You don't get the U-shaped training training curve, and it's

NOTE CONF {"raw":[100,100,100,100,95,100,100,100,100,100]}

00:43:53.960 --> 00:43:55.360
<v Speaker 1>an artefact of the training regime.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:43:55.360 --> 00:43:56.640
<v Speaker 1>So the model is not robust.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:43:56.640 --> 00:43:58.080
<v Speaker 1>If you change the training regime.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:43:58.640 --> 00:44:04.120
<v Speaker 1>Then the next one is, um, doesn't capture the fact

NOTE CONF {"raw":[100,100,100,100,100,98,100,100,100,100]}

00:44:04.120 --> 00:44:06.160
<v Speaker 1>that children learn common verbs first.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:44:07.160 --> 00:44:10.360
<v Speaker 1>Um, that is probably true.

NOTE CONF {"raw":[100,100,100,100,100]}

00:44:10.360 --> 00:44:14.730
<v Speaker 1>But, they have just basically engineered the training regime.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100]}

00:44:14.850 --> 00:44:17.770
<v Speaker 1>So the model doesn't really say anything about that because

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:44:17.770 --> 00:44:21.370
<v Speaker 1>they assume that the learning happens with the common verbs

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:44:21.370 --> 00:44:25.410
<v Speaker 1>first, and the representation with the weaker features is oversimplified.

NOTE CONF {"raw":[100,100,100,100,100,100,46,100,100,100]}

00:44:25.410 --> 00:44:26.530
<v Speaker 1>We've already said this.

NOTE CONF {"raw":[100,100,100,100]}

00:44:26.570 --> 00:44:29.250
<v Speaker 1>For example, it cannot represent the order of the different

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:44:29.290 --> 00:44:29.850
<v Speaker 1>triples.

NOTE CONF {"raw":[100]}

00:44:30.130 --> 00:44:32.610
<v Speaker 1>It can only represent whether the triple is there or

NOTE CONF {"raw":[84,100,100,100,100,100,100,100,100,100]}

00:44:32.610 --> 00:44:32.970
<v Speaker 1>not.

NOTE CONF {"raw":[100]}

00:44:38.970 --> 00:44:39.570
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:44:39.570 --> 00:44:44.890
<v Speaker 1>So the next one and I think we're sort of

NOTE CONF {"raw":[100,100,100,100,100,100,100,97,100,100]}

00:44:44.930 --> 00:44:45.730
<v Speaker 1>out of time.

NOTE CONF {"raw":[100,100,100]}

00:44:45.730 --> 00:44:48.570
<v Speaker 1>So I'll skip the the next one.

NOTE CONF {"raw":[100,100,100,60,100,100,100]}

00:44:48.570 --> 00:44:49.050
<v Speaker 1>Sorry.

NOTE CONF {"raw":[100]}

00:44:50.770 --> 00:44:55.090
<v Speaker 1>So that we can quickly talk about the next model.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:44:55.610 --> 00:44:59.330
<v Speaker 1>So this is an extreme version of a model in

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:44:59.330 --> 00:45:00.810
<v Speaker 1>the sense that there's no rules.

NOTE CONF {"raw":[100,100,100,61,100,100]}

00:45:01.530 --> 00:45:02.850
<v Speaker 1>Everything is memorised.

NOTE CONF {"raw":[100,100,100]}

00:45:02.890 --> 00:45:06.130
<v Speaker 1>It's it's just learning from data, generalising from data.

NOTE CONF {"raw":[82,100,100,100,100,100,100,100,100]}

00:45:06.570 --> 00:45:08.410
<v Speaker 1>But it makes testable predictions.

NOTE CONF {"raw":[100,100,100,100,100]}

00:45:09.170 --> 00:45:15.580
<v Speaker 1>Um, then people have done experiments and try to validate

NOTE CONF {"raw":[94,100,100,100,100,100,100,100,100,100]}

00:45:15.580 --> 00:45:16.580
<v Speaker 1>those predictions.

NOTE CONF {"raw":[100,100]}

00:45:16.980 --> 00:45:22.340
<v Speaker 1>And they have also proposed other models that, um, for

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:45:22.340 --> 00:45:25.020
<v Speaker 1>example, use different input representations.

NOTE CONF {"raw":[100,100,100,100,100]}

00:45:26.020 --> 00:45:27.460
<v Speaker 1>Uh, they have a hidden layer.

NOTE CONF {"raw":[89,100,100,100,100,100]}

00:45:27.460 --> 00:45:29.980
<v Speaker 1>They have a rule like mechanism, for example, to model

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:45:30.020 --> 00:45:31.900
<v Speaker 1>the regular verbs and so on.

NOTE CONF {"raw":[100,95,100,100,100,100]}

00:45:32.180 --> 00:45:35.580
<v Speaker 1>And we'll briefly look at one such model, the Kirov

NOTE CONF {"raw":[100,92,100,100,100,100,100,100,100,100]}

00:45:35.580 --> 00:45:40.380
<v Speaker 1>and Cotterell model, um, which uses a different neural network

NOTE CONF {"raw":[100,74,100,79,100,100,100,100,100,100]}

00:45:40.380 --> 00:45:44.220
<v Speaker 1>architecture called the recurrent network, which has the advantage that

NOTE CONF {"raw":[100,100,82,100,100,100,100,100,100,100]}

00:45:44.220 --> 00:45:47.540
<v Speaker 1>it can represent an input of an arbitrary length, which,

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:45:47.580 --> 00:45:50.580
<v Speaker 1>as we said, is a useful thing for any language

NOTE CONF {"raw":[100,100,100,100,97,100,100,100,100,100]}

00:45:50.580 --> 00:45:53.940
<v Speaker 1>data, really, because we don't know how many phones our

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,99]}

00:45:53.940 --> 00:45:54.860
<v Speaker 1>word is going to have.

NOTE CONF {"raw":[65,100,100,100,100]}

00:45:55.060 --> 00:45:59.380
<v Speaker 1>We don't know how many, uh, how many words in

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:45:59.380 --> 00:46:00.620
<v Speaker 1>a sentence and so on.

NOTE CONF {"raw":[100,100,100,100,100]}

00:46:00.780 --> 00:46:03.700
<v Speaker 1>So we want to have a flexible representation like this.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:46:04.660 --> 00:46:07.700
<v Speaker 1>Um, it doesn't use wiggle features.

NOTE CONF {"raw":[98,100,100,100,57,100]}

00:46:07.900 --> 00:46:11.360
<v Speaker 1>It just represents the, uh, the phonemes directly.

NOTE CONF {"raw":[100,100,100,100,42,100,100,100]}

00:46:12.320 --> 00:46:16.200
<v Speaker 1>Um, it uses what's called an encoder decoder architecture.

NOTE CONF {"raw":[71,100,100,100,100,100,100,100,100]}

00:46:16.520 --> 00:46:20.560
<v Speaker 1>And this is basically one network that encodes the input

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:46:20.560 --> 00:46:23.480
<v Speaker 1>and another network that decodes the output.

NOTE CONF {"raw":[92,100,100,100,100,100,100]}

00:46:23.640 --> 00:46:27.800
<v Speaker 1>And this is quite a common architecture in language processing.

NOTE CONF {"raw":[100,100,100,100,100,100,100,84,100,100]}

00:46:28.200 --> 00:46:31.240
<v Speaker 1>It uses an attention mechanism, which is basically a way

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:46:31.240 --> 00:46:33.400
<v Speaker 1>of saying which parts of the input and the output

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:46:33.400 --> 00:46:34.160
<v Speaker 1>are important.

NOTE CONF {"raw":[100,100]}

00:46:34.480 --> 00:46:38.560
<v Speaker 1>And then it also addresses this criticism that the model

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:46:38.560 --> 00:46:40.400
<v Speaker 1>can only do one thing, and you would have to

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:46:40.440 --> 00:46:45.240
<v Speaker 1>train another model, for example, to recognise past tense verbs.

NOTE CONF {"raw":[100,100,100,100,100,100,93,100,100,100]}

00:46:45.480 --> 00:46:49.040
<v Speaker 1>So it uses something called multi-task learning, which allows it

NOTE CONF {"raw":[100,100,100,100,100,55,100,100,100,100]}

00:46:49.040 --> 00:46:51.040
<v Speaker 1>to learn multiple tasks at the same time.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:46:51.160 --> 00:46:53.280
<v Speaker 1>So past tense is only one of them.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100]}

00:46:54.400 --> 00:46:57.640
<v Speaker 1>Okay, so this is the encoder decoder architecture where we

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:46:57.640 --> 00:46:59.160
<v Speaker 1>have a recurrent network here.

NOTE CONF {"raw":[100,100,100,100,100]}

00:46:59.160 --> 00:47:03.960
<v Speaker 1>And note these connections here between the units in the

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:47:03.960 --> 00:47:05.200
<v Speaker 1>same in the same layer.

NOTE CONF {"raw":[100,100,100,100,100]}

00:47:05.200 --> 00:47:06.880
<v Speaker 1>These are called recurrent connections.

NOTE CONF {"raw":[100,100,100,100,100]}

00:47:07.520 --> 00:47:11.450
<v Speaker 1>And then the output vector is the input for the

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:47:11.490 --> 00:47:12.090
<v Speaker 1>decoder.

NOTE CONF {"raw":[100]}

00:47:12.090 --> 00:47:12.890
<v Speaker 1>So we have two.

NOTE CONF {"raw":[100,100,100,100]}

00:47:13.890 --> 00:47:17.810
<v Speaker 1>This is actually an example from translation English to Korean.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:47:18.970 --> 00:47:21.730
<v Speaker 1>So we have an encoder that encodes the English and

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:47:21.730 --> 00:47:24.210
<v Speaker 1>a decoder that decodes the Korean.

NOTE CONF {"raw":[52,100,100,100,100,100]}

00:47:24.530 --> 00:47:27.730
<v Speaker 1>This is actually commonly used in machine translation models as

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:47:27.730 --> 00:47:28.050
<v Speaker 1>well.

NOTE CONF {"raw":[100]}

00:47:29.290 --> 00:47:30.650
<v Speaker 1>And this is what they find.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:47:30.650 --> 00:47:33.330
<v Speaker 1>So they compare to a rule based model, the minimal

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:47:33.330 --> 00:47:37.410
<v Speaker 1>generalisation learner which is a state of the art rule

NOTE CONF {"raw":[97,100,100,100,95,100,100,100,100,100]}

00:47:37.450 --> 00:47:38.210
<v Speaker 1>based model.

NOTE CONF {"raw":[100,100]}

00:47:39.050 --> 00:47:40.810
<v Speaker 1>And then they have a single task learner and a

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:47:40.810 --> 00:47:41.770
<v Speaker 1>multi-task learner.

NOTE CONF {"raw":[56,100]}

00:47:42.050 --> 00:47:44.010
<v Speaker 1>And this is the accuracy on the training set.

NOTE CONF {"raw":[100,100,100,100,100,99,100,100,100]}

00:47:44.050 --> 00:47:46.970
<v Speaker 1>So they get very high accuracies on the training set

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:47:46.970 --> 00:47:49.330
<v Speaker 1>which perhaps is is not surprising.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:47:49.370 --> 00:47:49.570
<v Speaker 1>Right.

NOTE CONF {"raw":[91]}

00:47:49.610 --> 00:47:53.810
<v Speaker 1>On the training set and both for the sorry for

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:47:53.810 --> 00:47:57.530
<v Speaker 1>the regular verbs and the irregular verbs, and on the

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:47:57.530 --> 00:47:59.250
<v Speaker 1>development set and on the test set.

NOTE CONF {"raw":[100,100,100,100,100,100,89]}

00:47:59.250 --> 00:48:01.370
<v Speaker 1>So development set is a type of test set that

NOTE CONF {"raw":[81,100,100,100,91,100,100,100,100,100]}

00:48:01.370 --> 00:48:04.730
<v Speaker 1>gets slightly lower accuracies, but still in the high 90s.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:48:05.970 --> 00:48:06.490
<v Speaker 1>And.

NOTE CONF {"raw":[100]}

00:48:07.860 --> 00:48:10.540
<v Speaker 1>the the rule based model here.

NOTE CONF {"raw":[94,100,100,100,100,99]}

00:48:10.780 --> 00:48:13.140
<v Speaker 1>They achieve similar performance.

NOTE CONF {"raw":[100,100,100,100]}

00:48:13.540 --> 00:48:15.260
<v Speaker 1>But what about the irregular ones?

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:48:15.380 --> 00:48:17.020
<v Speaker 1>So on the training set no problem.

NOTE CONF {"raw":[100,84,100,100,100,100,100]}

00:48:17.020 --> 00:48:20.980
<v Speaker 1>But on the development set and test set, uh, we

NOTE CONF {"raw":[100,100,100,100,94,93,100,99,74,100]}

00:48:20.980 --> 00:48:22.940
<v Speaker 1>have significantly worse performance.

NOTE CONF {"raw":[100,100,100,100]}

00:48:23.300 --> 00:48:27.220
<v Speaker 1>So, um, and this is maybe it's a plausible behaviour,

NOTE CONF {"raw":[100,87,100,100,100,100,79,100,100,83]}

00:48:27.260 --> 00:48:27.500
<v Speaker 1>right?

NOTE CONF {"raw":[98]}

00:48:27.540 --> 00:48:31.580
<v Speaker 1>Because then if you give a human learner an irregular

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:48:31.580 --> 00:48:34.380
<v Speaker 1>verb that I haven't seen before, initially they would probably,

NOTE CONF {"raw":[100,100,86,100,100,100,100,100,48,100]}

00:48:35.100 --> 00:48:38.860
<v Speaker 1>uh, not correct correctly find the past tense form.

NOTE CONF {"raw":[94,100,96,100,100,100,100,100,100]}

00:48:39.420 --> 00:48:41.020
<v Speaker 1>And this is their learning curve.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:48:41.540 --> 00:48:47.220
<v Speaker 1>So you have, uh, the regulars here, uh, the irregulars

NOTE CONF {"raw":[100,100,100,96,100,100,100,58,100,100]}

00:48:47.420 --> 00:48:50.340
<v Speaker 1>in the multitask or the single task setting.

NOTE CONF {"raw":[100,100,52,100,100,100,100,100]}

00:48:50.860 --> 00:48:54.860
<v Speaker 1>And as you can see, the performance eventually goes up

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:48:55.900 --> 00:48:57.060
<v Speaker 1>to nearly 100.

NOTE CONF {"raw":[100,100,74]}

00:48:57.260 --> 00:48:59.340
<v Speaker 1>Here on the on the training set.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:49:00.140 --> 00:49:02.460
<v Speaker 1>But it takes much longer to get there for the

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:49:02.460 --> 00:49:03.180
<v Speaker 1>irregular ones.

NOTE CONF {"raw":[100,100]}

00:49:03.220 --> 00:49:03.380
<v Speaker 1>Right.

NOTE CONF {"raw":[98]}

00:49:03.420 --> 00:49:05.060
<v Speaker 1>On the x axis you have the number of training

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:49:05.100 --> 00:49:06.120
<v Speaker 1>iterations again.

NOTE CONF {"raw":[97,98]}

00:49:06.720 --> 00:49:08.120
<v Speaker 1>And crucially.

NOTE CONF {"raw":[100,97]}

00:49:08.160 --> 00:49:10.000
<v Speaker 1>What again do we not see?

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:49:11.000 --> 00:49:13.680
<v Speaker 1>There is no U-shape right of any kind.

NOTE CONF {"raw":[86,86,100,100,100,100,100,100]}

00:49:14.280 --> 00:49:18.160
<v Speaker 1>There's a bit of a zigzag here for the irregulars.

NOTE CONF {"raw":[100,100,100,100,64,83,100,100,100,100]}

00:49:18.680 --> 00:49:20.000
<v Speaker 1>For the regulars, definitely.

NOTE CONF {"raw":[100,100,100,100]}

00:49:20.000 --> 00:49:21.480
<v Speaker 1>They go straight to 100.

NOTE CONF {"raw":[100,100,100,100,100]}

00:49:22.200 --> 00:49:23.440
<v Speaker 1>But the irregulars are.

NOTE CONF {"raw":[100,100,100,91]}

00:49:23.480 --> 00:49:24.840
<v Speaker 1>A bit more difficult to learn.

NOTE CONF {"raw":[100,100,100,100,100,100]}

00:49:24.880 --> 00:49:27.320
<v Speaker 1>Take a bit longer, but we don't see a U-shaped

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:49:27.320 --> 00:49:27.720
<v Speaker 1>curve.

NOTE CONF {"raw":[100]}

00:49:28.720 --> 00:49:30.640
<v Speaker 1>So at some level.

NOTE CONF {"raw":[100,100,100,100]}

00:49:30.880 --> 00:49:33.600
<v Speaker 1>And this is just one of the the models that

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:49:33.600 --> 00:49:34.360
<v Speaker 1>people have published.

NOTE CONF {"raw":[100,100,100]}

00:49:34.400 --> 00:49:34.600
<v Speaker 1>Right.

NOTE CONF {"raw":[98]}

00:49:34.640 --> 00:49:41.320
<v Speaker 1>But at some level, the neural network exercise was a

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:49:41.320 --> 00:49:44.960
<v Speaker 1>success in terms of learning the verbs and generalising or

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:49:44.960 --> 00:49:47.680
<v Speaker 1>over generalising for the irregulars as well.

NOTE CONF {"raw":[100,100,100,100,100,100,100]}

00:49:48.120 --> 00:49:50.600
<v Speaker 1>But it was a failure in the terms in terms

NOTE CONF {"raw":[100,100,100,100,100,100,44,71,100,100]}

00:49:50.600 --> 00:49:53.640
<v Speaker 1>of modelling particular aspects of human behaviour.

NOTE CONF {"raw":[100,100,100,100,100,100,88]}

00:49:54.040 --> 00:49:58.480
<v Speaker 1>For example, U-shaped learning, where the error initially is low,

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:49:58.680 --> 00:50:00.440
<v Speaker 1>then high, then low again.

NOTE CONF {"raw":[100,100,100,100,100]}

00:50:01.320 --> 00:50:03.760
<v Speaker 1>That is not something that we see in in neural

NOTE CONF {"raw":[100,100,100,100,100,100,100,85,100,100]}

00:50:03.760 --> 00:50:04.360
<v Speaker 1>networks.

NOTE CONF {"raw":[100]}

00:50:06.610 --> 00:50:07.130
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:50:07.170 --> 00:50:08.090
<v Speaker 1>With that in mind.

NOTE CONF {"raw":[100,100,100,100]}

00:50:08.570 --> 00:50:09.010
<v Speaker 1>Oh yeah.

NOTE CONF {"raw":[100,100]}

00:50:09.050 --> 00:50:10.090
<v Speaker 1>Some some examples.

NOTE CONF {"raw":[100,100,100]}

00:50:10.090 --> 00:50:12.690
<v Speaker 1>So it makes interesting overgeneralisation.

NOTE CONF {"raw":[100,100,100,100,56]}

00:50:12.690 --> 00:50:14.530
<v Speaker 1>And it oscillates.

NOTE CONF {"raw":[100,100,100]}

00:50:14.610 --> 00:50:21.930
<v Speaker 1>So for example between uh clang clang and clink clink.

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:50:22.850 --> 00:50:26.050
<v Speaker 1>Uh, so these are the phonetic representations here.

NOTE CONF {"raw":[66,100,100,100,100,100,100,100]}

00:50:27.130 --> 00:50:33.010
<v Speaker 1>Uh, but ultimately, uh, we don't see the U-shaped curve

NOTE CONF {"raw":[82,100,100,87,100,100,100,100,100,100]}

00:50:33.610 --> 00:50:38.450
<v Speaker 1>and the U-shaped curve that we see in the, uh,

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,91]}

00:50:38.490 --> 00:50:42.250
<v Speaker 1>in the McClelland and Rumelhart model is probably an artefact

NOTE CONF {"raw":[100,100,70,100,100,100,99,100,100,100]}

00:50:42.250 --> 00:50:44.930
<v Speaker 1>of their training, of their training regime.

NOTE CONF {"raw":[100,100,100,100,93,100,100]}

00:50:47.170 --> 00:50:47.650
<v Speaker 1>Okay.

NOTE CONF {"raw":[100]}

00:50:48.090 --> 00:50:53.610
<v Speaker 1>Uh, we will look at other aspects of, uh, language

NOTE CONF {"raw":[98,100,100,100,100,100,100,100,100,100]}

00:50:53.610 --> 00:50:54.130
<v Speaker 1>tomorrow.

NOTE CONF {"raw":[94]}

00:50:54.170 --> 00:50:58.890
<v Speaker 1>We'll look at speech segmentation and we'll look at some

NOTE CONF {"raw":[100,100,100,100,100,100,100,100,100,100]}

00:50:58.890 --> 00:51:00.130
<v Speaker 1>modelling paradigms there.

NOTE CONF {"raw":[98,100,100]}

00:51:01.050 --> 00:51:01.650
<v Speaker 1>Thank you.

NOTE CONF {"raw":[77,77]}
